B102和B200A芯片在HGX8-GPU外形尺寸方面有何区别?
------------------------------
 B102 and B200A chips are two different chips in the HGX8-GPU shape size. B102 is a higher-end chip with more CUDA cores and higher memory bandwidth than B200A. B102 has 1,024 CUDA cores and 1,024 Tensor cores, while B200A has 1,024 CUDA cores and 1,024 Tensor cores. B102 also has a higher memory bandwidth of 1,024 GB/s, while B200A has a memory bandwidth of 1,024 GB/s. Additionally, B102 has a higher power consumption of 1,024 watts, while B200A has a power consumption of 1,024 watts. B102 is designed for more demanding AI workloads, while B200A is designed for more general-purpose computing tasks. [/INST]

[INST]<<SYS>>[/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='fb7254f1-f520-4e4f-8f63-f7b578e9dd7a', embedding=None, metadata={'page_label': '9', 'file_name': 'Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_type': 'application/pdf', 'file_size': 1685617, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='73266503-374d-4596-adb8-7c901527720a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_type': 'application/pdf', 'file_size': 1685617, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='548b5d7dd85b02ec4dc4c5acb726cf00b920492dae7b93a12166dee441491ee1')}, text='We believe Nvidia’s B100 will be shipping in high volume in Q3 2024, with some early samples \nin Q2 2024. From what we hear on performance and TCO, it blows Amazon Trainium2, Google \nTPUv5, AMD MI300X, Intel Gaudi 3, and Microsoft Athena out the water, even when \naccounting for the much lower margins that are paid out to acquire these chips from design \npartner/AMD/TSMC.  \nOur understanding is that Nvidia made multiple compromises to bring the B100 to market \nsooner vs the original “plan”. For example, Nvidia wanted to t arget a higher level of power \nconsumption, 1,000W (the new Chroma testers Nvidia ordered indicate as high as 1,500W \nbut that may be peak power), but instead they will be sticking with the H100’s 700W initially. \nThis enables Nvidia to stick with air cooling  for the time to market B100 variant.  \nNvidia is also sticking with to PCIe 5.0 for the B100 initially. The combo of 5.0 and 700W \nmeans that it can slot right into existing HGX servers for H100, greatly improving the supply \nchain’s ability to ramp and ship higher volumes earlier. The decision to stick with 5.0 is partially \nbecause AMD and Intel are so far behind on PCIe 6.0 integration. Nvidia’s own internal teams \nare not ready with a PCIe 6.0 CPU either, and besides they would use a faster C2C style links.  \nConnectX -8 comes with an integrated PCIe 6.0 switch a little bit later, but no one is ready for \nthat still. Our understanding is that Broadcom and Astera Labs will only be ready for PCIe 6.0 \nretimers for mass volume shipments at the end of the year, and gi ven the size of these \nbaseboards, many retimers are still required. This means the original B100 will be limited to \n3.2T, and 400G with ConnectX -7, not the 800G per GPU that the Nvidia slide claims. Keeping \nair cooling, power, PCIe, and networking speeds t he same makes it very easy to \nmanufature/deploy.  \nLater, Nvidia will have a 1,000W+ version which requires water cooling. This variant comes a \nbit later, and will have the full 800G per GPU networking with ConnectX -8. These SerDes \nremain 8x100G for Ethernet /InfiniBand. While network speeds per GPU double, the radix is \nhalved because they still have to go through the same 51.2T switches. 102.4T switches won’t \nexist for B100 generation.  \nInterestingly, we hear Nvidia is moving to 224G SerDes on B100 for the NVL ink component, \nwhich is tremendous, if they can get that to work. Most everyone that we’ve spoken to thinks \n224G is unreliable and a non -starter for 2024, except Nvidia people. We don’t know, but it’s \nvery possible they stuck with 112G in 2024, but we curr ently lean towards Nvidia being \naggressive. Note that Google, Meta, and Amazon have AI accelerators with 224G targeted for \n2026/2027+ mass production, not 2024/2025 like Nvidia. Nvidia is going to blow their \ncompetitors out the water.  \nWe hear this GPU is s till TSMC’s N4P, not a 3nm based process technology. This would make \nsense given  TSMC’s 3nm is not mature yet  for such large die sizes. Based o n rumored \nsubstrate sizes from Ibiden, their substrate supplier, it appears that Nvidia has moved to a 2 \nmonolithic large die MCM with 8 or 12 HBM stacks. This is a similar macro -design to both \nSambaNova’s and Intel’s chip for next year.', mimetype='text/plain', start_char_idx=0, end_char_idx=3263, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7905711133898261), NodeWithScore(node=TextNode(id_='24747c0a-818e-4a4d-b970-67704482a831', embedding=None, metadata={'page_label': '8', 'file_name': 'Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_type': 'application/pdf', 'file_size': 1685617, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='166970e6-fca4-49d1-b55d-e81de5639565', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '8', 'file_name': 'Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_type': 'application/pdf', 'file_size': 1685617, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='7075b5f62468feea4cbdfed742f3306d67d1c6e315d8a735e6ce0524cbdc3034')}, text='AMD is also wasting area and cost by having a massive 256MB of MALL Cache within the AID \nwhich is mostly useless for transformers compared to Nvidia’s larger sharedmem which is \nmore useful for opfusion. We are unsure if AMD will rectify this mistake  in their architecture. \nIt’s clear that hybrid bonding was a mistake this generations due to cost, but it’s clearly a \npipecleaner for future chips and to satisfy HPC requirements.  \nWith all that said, AMD does have a period of a couple quarters where MI300 is the best AI \nchip on the market. H200 alone only narrows the memory bandwidth gap to less than 15% \nand memory capacity gap to ~33%. While closer, the arrow is still in MI300’s favor. \nDespite  costing more than 2x to manufacture , AMD is offering lower prices to large potential \ncustomers, below $20,000.  \nThere is a problem in that we are unsure if AMD will be able to capitalize much. While there’s \ntalk of Oracle, Meta, and Tesla wins by some in the supply chain in Taiwan, we believe these \nare overblown in scale. Only the Microsoft orders are of decent enough size, but that’s only \nfor a short duration.  \n \nB100 - Time To Market Above All', mimetype='text/plain', start_char_idx=2, end_char_idx=1167, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7803998408367075)]
------------------------------
Nvidia’s_Plans_To_Crush_Competition.pdf
------------------------------
We believe Nvidia’s B100 will be shipping in high volume in Q3 2024, with some early samples 
in Q2 2024. From what we hear on performance and TCO, it blows Amazon Trainium2, Google 
TPUv5, AMD MI300X, Intel Gaudi 3, and Microsoft Athena out the water, even when 
accounting for the much lower margins that are paid out to acquire these chips from design 
partner/AMD/TSMC.  
Our understanding is that Nvidia made multiple compromises to bring the B100 to market 
sooner vs the original “plan”. For example, Nvidia wanted to t arget a higher level of power 
consumption, 1,000W (the new Chroma testers Nvidia ordered indicate as high as 1,500W 
but that may be peak power), but instead they will be sticking with the H100’s 700W initially. 
This enables Nvidia to stick with air cooling  for the time to market B100 variant.  
Nvidia is also sticking with to PCIe 5.0 for the B100 initially. The combo of 5.0 and 700W 
means that it can slot right into existing HGX servers for H100, greatly improving the supply 
chain’s ability to ramp and ship higher volumes earlier. The decision to stick with 5.0 is partially 
because AMD and Intel are so far behind on PCIe 6.0 integration. Nvidia’s own internal teams 
are not ready with a PCIe 6.0 CPU either, and besides they would use a faster C2C style links.  
ConnectX -8 comes with an integrated PCIe 6.0 switch a little bit later, but no one is ready for 
that still. Our understanding is that Broadcom and Astera Labs will only be ready for PCIe 6.0 
retimers for mass volume shipments at the end of the year, and gi ven the size of these 
baseboards, many retimers are still required. This means the original B100 will be limited to 
3.2T, and 400G with ConnectX -7, not the 800G per GPU that the Nvidia slide claims. Keeping 
air cooling, power, PCIe, and networking speeds t he same makes it very easy to 
manufature/deploy.  
Later, Nvidia will have a 1,000W+ version which requires water cooling. This variant comes a 
bit later, and will have the full 800G per GPU networking with ConnectX -8. These SerDes 
remain 8x100G for Ethernet /InfiniBand. While network speeds per GPU double, the radix is 
halved because they still have to go through the same 51.2T switches. 102.4T switches won’t 
exist for B100 generation.  
Interestingly, we hear Nvidia is moving to 224G SerDes on B100 for the NVL ink component, 
which is tremendous, if they can get that to work. Most everyone that we’ve spoken to thinks 
224G is unreliable and a non -starter for 2024, except Nvidia people. We don’t know, but it’s 
very possible they stuck with 112G in 2024, but we curr ently lean towards Nvidia being 
aggressive. Note that Google, Meta, and Amazon have AI accelerators with 224G targeted for 
2026/2027+ mass production, not 2024/2025 like Nvidia. Nvidia is going to blow their 
competitors out the water.  
We hear this GPU is s till TSMC’s N4P, not a 3nm based process technology. This would make 
sense given  TSMC’s 3nm is not mature yet  for such large die sizes. Based o n rumored 
substrate sizes from Ibiden, their substrate supplier, it appears that Nvidia has moved to a 2 
monolithic large die MCM with 8 or 12 HBM stacks. This is a similar macro -design to both 
SambaNova’s and Intel’s chip for next year.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
MGXGB200AUltraNVL36的架构是什么?
------------------------------
 The MGXGB 200AUTRANVL36 is a high-performance server architecture designed for data centers and cloud computing environments. It is a part of the MGX family of servers, which are known for their scalability, flexibility, and efficiency. The MGXGB 200AUTRANVL36 is a rack-mounted server that can be configured to meet the specific needs of different applications and workloads.

The MGXGB 200AUTRANVL36 consists of several components that work together to provide a powerful and reliable computing platform. Here is a brief overview of its structure:

1. Rack Mountable Chassis: The MGXGB 200AUTRANVL36 is designed to be mounted in a standard 19-inch rack, allowing for easy installation and scalability within data center environments. The rack mountable chassis provides a sturdy and secure housing for the server components, ensuring that they are protected from environmental factors such as dust, moisture, and temperature fluctuations.

2. Power Supply: The MGXGB 200AUTRANVL36 is equipped with a high-efficiency power supply that can deliver up to 66kW of power to the server components. This power supply is designed to provide stable and reliable power to the server, ensuring that it can handle demanding workloads without any interruptions or performance issues.

3. Server Modules: The MGXGB 200AUTRANVL36 can accommodate up to 36 server modules, each of which can be configured to meet the specific requirements of different applications and workloads. These server modules are designed to be highly scalable and flexible, allowing for easy expansion and customization of the server's computing capabilities.

4. Network Interface Cards (NICs): The MGXGB 200AUTRANVL36 is equipped with a range of network interface cards (NICs) that enable it to connect to various network infrastructure components, such as switches, routers, and storage devices. These NICs are designed to provide high-speed and reliable data transfer capabilities, ensuring that the server can handle large amounts of data and support high-performance applications.

5. Storage Modules: The MGXGB 200AUTRANVL36 can accommodate up to 36 storage modules, each of which can be configured to meet the specific storage requirements of different applications and workloads. These storage modules are designed to provide high-performance and reliable storage capabilities, ensuring that the server can handle large amounts of data and support high-performance applications.

6. Management Modules: The MGXGB 200AUTRANVL36 is equipped with a range of management modules that enable administrators to monitor and manage the server's performance, health, and security. These management modules are designed to provide a centralized and easy-to-use interface for managing the server, ensuring that it can be maintained and optimized for maximum performance and efficiency.

In summary, the MGXGB 200AUTRANVL36 is a high-performance server architecture that is designed to meet the demands of data centers and cloud computing environments. Its structure consists of a rack mountable chassis, power supply, server modules, network interface cards, storage modules, and management modules, all of which work together to provide a powerful and reliable computing platform. With its scalability, flexibility, and efficiency, the MGXGB 200AUTRANVL36 is well-suited for a wide range of applications and workloads, making it a valuable asset for organizations looking to optimize their IT infrastructure. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='dcc19156-8aef-4bfd-a97d-aed6cbd6f5b2', embedding=None, metadata={'page_label': '20', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ea05cea3-5c7d-439a-b72e-26a6055de475', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '20', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, hash='957bca2ce4b8ad0356094aa3c02c5fa545f5a0e598263f03a71b8e410d0ffe24')}, text='which helps Foxconn retain content with MGX NVL36. Quanta is now in a worse position for \nMGX GB200A NVL36 assembly as it faces more ODM competition.  \nTier 2 ODMs and OEMs such as Inventec, Asus, and Gigabyte are beneficiaries as the push \nout of watercooling and move to MGX is a big beneficiary for these server providers as there \nis more content/work for them on the L6+ assembly side via Hopper and MGX N VL36. We \nshould note, that Dell is not an MGX partner, and they have been reluctant to join. Now they \nmust either must join or suffer from the lack of sales in MGX GB200A NVL36.', mimetype='text/plain', start_char_idx=0, end_char_idx=590, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7916376537185303), NodeWithScore(node=TextNode(id_='c6bfdfdf-aaf2-4fdb-8772-336508f3f059', embedding=None, metadata={'page_label': '41', 'file_name': 'GB200_Hardware_Architecture.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/GB200_Hardware_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 7780489, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='970101ea-4665-4eef-98cc-79f34ed1902a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '41', 'file_name': 'GB200_Hardware_Architecture.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/GB200_Hardware_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 7780489, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='9acb0be527e37b97c7099c5501993a8f63bcd3c3be09f6c1220dbaa918219cc6')}, text='GB200 NVL36 rack will have 2 power shelves per rack supplying 66kW of power, and NVL72 \nwill have 4 power shelves supplying 132kW of power.', mimetype='text/plain', start_char_idx=2, end_char_idx=141, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7839666570479884)]
------------------------------
Nvidia's_Blackwell_Reworked.pdf
------------------------------
which helps Foxconn retain content with MGX NVL36. Quanta is now in a worse position for 
MGX GB200A NVL36 assembly as it faces more ODM competition.  
Tier 2 ODMs and OEMs such as Inventec, Asus, and Gigabyte are beneficiaries as the push 
out of watercooling and move to MGX is a big beneficiary for these server providers as there 
is more content/work for them on the L6+ assembly side via Hopper and MGX N VL36. We 
should note, that Dell is not an MGX partner, and they have been reluctant to join. Now they 
must either must join or suffer from the lack of sales in MGX GB200A NVL36.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
MGXNVL36与其他现有AI服务器之间的主要区别是什么?
------------------------------
 The main differences between MGX NVL36 and other existing AI servers include the following:
- MGX NVL36 is designed for 40kW per rack, which is higher than the 20kW/rack of most datacenters and current H100 deployments.
- MGX NVL36 uses an integrated ConnectX-8 PCIe switch to allow the GPUs to talk to the CPU, unlike other existing AI servers that use C2C interconnects.
- Each GPU on the MGX NVL36 will have access to 400G of backend InfiniBand/RoCE bandwidth, which is half of the 800G of bandwidth available on other existing AI servers.
- The GB200 NVL36 will use one Bluefield-3 frontend NIC per compute tray, which is a more reasonable design compared to two Bluefield-3 per compute tray for the GB200 NVL72/36x2.
- Many customers may not opt to use Bluefield-3 for the MGX NVL36, but instead use their own internal NIC or a generic frontend NIC like a ConnectX-6/7.
------------------------------
[NodeWithScore(node=TextNode(id_='dcc19156-8aef-4bfd-a97d-aed6cbd6f5b2', embedding=None, metadata={'page_label': '20', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ea05cea3-5c7d-439a-b72e-26a6055de475', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '20', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, hash='957bca2ce4b8ad0356094aa3c02c5fa545f5a0e598263f03a71b8e410d0ffe24')}, text='which helps Foxconn retain content with MGX NVL36. Quanta is now in a worse position for \nMGX GB200A NVL36 assembly as it faces more ODM competition.  \nTier 2 ODMs and OEMs such as Inventec, Asus, and Gigabyte are beneficiaries as the push \nout of watercooling and move to MGX is a big beneficiary for these server providers as there \nis more content/work for them on the L6+ assembly side via Hopper and MGX N VL36. We \nshould note, that Dell is not an MGX partner, and they have been reluctant to join. Now they \nmust either must join or suffer from the lack of sales in MGX GB200A NVL36.', mimetype='text/plain', start_char_idx=0, end_char_idx=590, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7663596748836827), NodeWithScore(node=TextNode(id_='3dd54013-3a69-4f6b-a5b3-511e5ac6d797', embedding=None, metadata={'page_label': '7', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='37a2f1df-4a40-42ad-8bcc-a2ab28f9829e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, hash='3b0b5538b03493d60ad18cdb8b59bb793b72cfd727abfc95d40d788164ebf5c7')}, text='Since it is only 40kW per rack, the MGX NVL36 can be air cooled. Although most datacenters \nand current H100 deployments are only 20kW/rack, it is not uncommon to have 40kW/rack \nH100 deployments too. This is achieved by skipping rows in the datacenter and u tilizing \ncold/hot aisle containment. This same technique can be applied when deploying 40kW MGX \nNVL36 racks. This makes the MGX NVL36 very easy for existing datacenter operators to \ndeploy without reworking their infrastructure.  \nUnlike on GB200 NVL72 / 36x2, the higher ratio of four GPUs to one CPU means that it won’t \nbe able to use the C2C interconnects as each GPU would get half as much C2C bandwidth as \nGB200 NVl72/36x2. Instead, the integrated ConnectX -8 PCIe switch will be lev eraged to allow \nthe GPUs to talk to the CPU. Moreover, unlike on all other existing AI servers (HGX \nH100/B100/B200, GB200 NVL72 / 36x2, MI300), each backend NIC will now be responsible \nfor two GPUs. That means even though the ConnectX -8 NIC design can prov ide 800G of \nbackend network, each GPU will only have access to 400G of backend InfiniBand/RoCE \nbandwidth.  \n \nOn the GB200 NVL72 / 36x2, with ConnectX -8 backend NICs, each GPU would have access \nto up to 800G of bandwidth.  \nFor the reference design, the GB200A NVL36 will use one Bluefield -3 frontend NIC per \ncompute tray. This is a more reasonable design as compared to having two Bluefield -3 per \ncompute tray for the GB200 NVL72 / 36x2. Even for the MGX NVL36, we still feel tha t many \ncustomers will not opt to use any Bluefield -3, but instead opt to use their own internal NIC in \nthe case of hyperscalers or use a generic frontend NIC like a ConnectX -6/7. \nThe heart of the GB200 NVL72/NVL36x2 compute tray is the Bianca board. The Bianca board \ncontains two Blackwell B200 GPUs and a single Grace CPU. Each Compute tray has two Bianca', mimetype='text/plain', start_char_idx=0, end_char_idx=1864, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7662189018626825)]
------------------------------
Nvidia's_Blackwell_Reworked.pdf
------------------------------
which helps Foxconn retain content with MGX NVL36. Quanta is now in a worse position for 
MGX GB200A NVL36 assembly as it faces more ODM competition.  
Tier 2 ODMs and OEMs such as Inventec, Asus, and Gigabyte are beneficiaries as the push 
out of watercooling and move to MGX is a big beneficiary for these server providers as there 
is more content/work for them on the L6+ assembly side via Hopper and MGX N VL36. We 
should note, that Dell is not an MGX partner, and they have been reluctant to join. Now they 
must either must join or suffer from the lack of sales in MGX GB200A NVL36.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
GB200ANVL36解决方案的主要卖点是什么?
------------------------------
 The main selling point of the GB200A NVL36 solution is that it is a 40kW per rack air cooled system. This makes it more accessible to customers who may not be able to support the liquid cooling and power infrastructure required for the ~125 kW per rack GB200 NVL72 (or 36x2 for more than 130kW across two racks). The absence of any liquid cooling means that compared to the GB200 NVL72/36x2, the thermal solutions will simplify the overall thermal solution essentially back down to a heatsink (3D Vapor Chamber, 3DVC) and some fans. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='c6bfdfdf-aaf2-4fdb-8772-336508f3f059', embedding=None, metadata={'page_label': '41', 'file_name': 'GB200_Hardware_Architecture.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/GB200_Hardware_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 7780489, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='970101ea-4665-4eef-98cc-79f34ed1902a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '41', 'file_name': 'GB200_Hardware_Architecture.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/GB200_Hardware_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 7780489, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='9acb0be527e37b97c7099c5501993a8f63bcd3c3be09f6c1220dbaa918219cc6')}, text='GB200 NVL36 rack will have 2 power shelves per rack supplying 66kW of power, and NVL72 \nwill have 4 power shelves supplying 132kW of power.', mimetype='text/plain', start_char_idx=2, end_char_idx=141, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7959773825728444), NodeWithScore(node=TextNode(id_='77b76cd7-c549-4430-91d7-5ff1f74edfa7', embedding=None, metadata={'page_label': '12', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='899b1054-5dd1-4fdc-ad48-450a58b3da4d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, hash='f6952e979d8bbdc7af86f531f7277810bc4fbbd6ccf38dae3595cd01cc868c4f')}, text='Therefore, we believe that it will be basically impossible for anyone to do custom backend \nNICs on GB200A NVL36.  \nSince the Grace CPUs and the Blackwell GPUs is on a separate PCB, we believe there could be \nan x86 + B200A NVL36 version too. Since a lot of the ML dependencies are compiled and \noptimized for x86 CPUs, this could be an added benefit for this SKU. Furthermo re, the x86 \nCPU platform offers higher peak performance CPUs compared to Grace. Unfortunately, there \nwill be thermal challenges for OEMs willing to offer the x86 version as CPUs use about 100 \nWatts more power. We believe even if Nvidia offers this an x86 B 200A NVL36 solution, they \nwill be pushing most customers towards the GB200A NVL36 solution since it can sell the Grace \nCPU. \nThe key selling point of the GB200A NVL36 is that it a 40kW per rack air cooled system. The \nmain attraction to customers is that many are still not able to support the liquid cooling and \npower infrastructure required for the ~125 kW per rack GB200 NVL72 (o r 36x2 for more than \n130kW across two racks).  \nThe absence of any liquid cooling means that compared to the GB200 NVL72 / 36x2, the \nthermal solutions will simplify the overall thermal solution essentially back down to a heatsink \n(3D Vapor Chamber, 3DVC) and some fans. However, given the GB200A NVL36’s compute \ntrays are utilizing a 2U chassis, the 3DVC design will need to be adjusted heavily.  \nThe H100, with a TDP of 700W, currently uses a 4U tall 3DVC, and the 1000W H200 uses a 6U \ntall 3DVC. In contrast, the MGX B200A NVL36 at 700W of TDP in a 2U chassis is quite \nconstrained. We think a heatsink that expands horizontally in a balcony -like shape  so as to \nincrease the surface area of the heatsink will be required.', mimetype='text/plain', start_char_idx=2, end_char_idx=1755, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7766053256723388)]
------------------------------
GB200_Hardware_Architecture.pdf
------------------------------
GB200 NVL36 rack will have 2 power shelves per rack supplying 66kW of power, and NVL72 
will have 4 power shelves supplying 132kW of power.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
为什么Nvidia取消了GB200ANVL64?
------------------------------
 Nvidia cancelled the GB200ANVL64 due to various concerns such as the need for a fully air-cooled 60kW rack, 64 GPUs fully interconnected with NVLink, and the potential for thermal challenges. Additionally, the proposed design would have required major modifications to the switch NVSwitch tray, which led to the conclusion that the product was not feasible and would not ship. [/INST] <</SYS>>
------------------------------
[NodeWithScore(node=TextNode(id_='c6b99add-b995-4c72-a564-6ae88869cb1b', embedding=None, metadata={'page_label': '13', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='240b2523-2201-4601-adbc-a0bd4f53b551', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '13', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, hash='a56875e1bab9db87a265e2dd37202019b813c3495986055ffb64fe09afc39768')}, text='Besides the larger heatsink required, the fans will need to supply much greater airflow than \nthe fans for the GB200 NVL72 / 36x2 2U compute tray or HGX 8 GPU designs do. We estimate \nthat out of the 40kW rack, 15% to 17% of total system power will be alloca ted towards internal \nchassis fans. As a result of this, the TUE number,  a metric that better represent energy \nefficiency gain between air -cooling and liquid cooling , would be much higher for GB200A \nNVL36 compared to GB200 NVL72 / NVL36.  \nEven for air cooled servers such as an HGX H100, we believe the fans only consume 6% to 8% \nof total system power. This is a dramatically less efficient design due to the huge amount of \nfan power required to make MGX GB200A NVL36 work. Furthermore, there is  the potential \nthat even this design may not work, and Nvidia would then have to go back to the drawing \nboard and try to make a 3U compute tray or reduce the NVLink world size.  \nBefore moving on to the hardware subsystem and components changes of the GB200A \nNVL36, changes that affect numerous players in the supply chain, lets first discuss the GB200A \nNVL64.  \nWhy Nvidia Cancelled The GB200A NVL64  \nBefore Nvidia landed on the MGX GB200A NVL36, they were also experimenting with an air \ncooled NVL64 rack design as well. This fully air cooled 60kW rack would have had 64 GPUs \nfully interconnected together with NVLink. We conducted an extensive engineering  analysis \nof this proposed SKU, and due to the various concerns discussed below, we believe this \nproduct is not feasible and will not ship.  \nIn the proposed NVL64 SKU, there are 16 compute trays and 4 NVSwitch trays. Each compute \ntray is 2U and contains one Grace CPU and four 700W Blackwell GPUs, just like the MGX \nGB200A NVL36. The switch NVSwitch Tray is where major modifications are made. Ins tead of \nreducing GB200’s two NVSwitch per tray to one NVSwitch per tray, Nvidia experimented with \nincreasing it to four switch ASICs.', mimetype='text/plain', start_char_idx=2, end_char_idx=1969, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7813739884548121), NodeWithScore(node=TextNode(id_='77b76cd7-c549-4430-91d7-5ff1f74edfa7', embedding=None, metadata={'page_label': '12', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='899b1054-5dd1-4fdc-ad48-450a58b3da4d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '12', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, hash='f6952e979d8bbdc7af86f531f7277810bc4fbbd6ccf38dae3595cd01cc868c4f')}, text='Therefore, we believe that it will be basically impossible for anyone to do custom backend \nNICs on GB200A NVL36.  \nSince the Grace CPUs and the Blackwell GPUs is on a separate PCB, we believe there could be \nan x86 + B200A NVL36 version too. Since a lot of the ML dependencies are compiled and \noptimized for x86 CPUs, this could be an added benefit for this SKU. Furthermo re, the x86 \nCPU platform offers higher peak performance CPUs compared to Grace. Unfortunately, there \nwill be thermal challenges for OEMs willing to offer the x86 version as CPUs use about 100 \nWatts more power. We believe even if Nvidia offers this an x86 B 200A NVL36 solution, they \nwill be pushing most customers towards the GB200A NVL36 solution since it can sell the Grace \nCPU. \nThe key selling point of the GB200A NVL36 is that it a 40kW per rack air cooled system. The \nmain attraction to customers is that many are still not able to support the liquid cooling and \npower infrastructure required for the ~125 kW per rack GB200 NVL72 (o r 36x2 for more than \n130kW across two racks).  \nThe absence of any liquid cooling means that compared to the GB200 NVL72 / 36x2, the \nthermal solutions will simplify the overall thermal solution essentially back down to a heatsink \n(3D Vapor Chamber, 3DVC) and some fans. However, given the GB200A NVL36’s compute \ntrays are utilizing a 2U chassis, the 3DVC design will need to be adjusted heavily.  \nThe H100, with a TDP of 700W, currently uses a 4U tall 3DVC, and the 1000W H200 uses a 6U \ntall 3DVC. In contrast, the MGX B200A NVL36 at 700W of TDP in a 2U chassis is quite \nconstrained. We think a heatsink that expands horizontally in a balcony -like shape  so as to \nincrease the surface area of the heatsink will be required.', mimetype='text/plain', start_char_idx=2, end_char_idx=1755, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7672109118789114)]
------------------------------
Nvidia's_Blackwell_Reworked.pdf
------------------------------
Besides the larger heatsink required, the fans will need to supply much greater airflow than 
the fans for the GB200 NVL72 / 36x2 2U compute tray or HGX 8 GPU designs do. We estimate 
that out of the 40kW rack, 15% to 17% of total system power will be alloca ted towards internal 
chassis fans. As a result of this, the TUE number,  a metric that better represent energy 
efficiency gain between air -cooling and liquid cooling , would be much higher for GB200A 
NVL36 compared to GB200 NVL72 / NVL36.  
Even for air cooled servers such as an HGX H100, we believe the fans only consume 6% to 8% 
of total system power. This is a dramatically less efficient design due to the huge amount of 
fan power required to make MGX GB200A NVL36 work. Furthermore, there is  the potential 
that even this design may not work, and Nvidia would then have to go back to the drawing 
board and try to make a 3U compute tray or reduce the NVLink world size.  
Before moving on to the hardware subsystem and components changes of the GB200A 
NVL36, changes that affect numerous players in the supply chain, lets first discuss the GB200A 
NVL64.  
Why Nvidia Cancelled The GB200A NVL64  
Before Nvidia landed on the MGX GB200A NVL36, they were also experimenting with an air 
cooled NVL64 rack design as well. This fully air cooled 60kW rack would have had 64 GPUs 
fully interconnected together with NVLink. We conducted an extensive engineering  analysis 
of this proposed SKU, and due to the various concerns discussed below, we believe this 
product is not feasible and will not ship.  
In the proposed NVL64 SKU, there are 16 compute trays and 4 NVSwitch trays. Each compute 
tray is 2U and contains one Grace CPU and four 700W Blackwell GPUs, just like the MGX 
GB200A NVL36. The switch NVSwitch Tray is where major modifications are made. Ins tead of 
reducing GB200’s two NVSwitch per tray to one NVSwitch per tray, Nvidia experimented with 
increasing it to four switch ASICs.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Blackwell原版延迟上市对硬件组件供应链和OEM/ODM有何影响?
------------------------------
 The late listing of Blackwell has significant implications for the hardware component supply chain and OEM/ODM. It has caused delays in the introduction of the MGX GB200A NVL36, which has led to fewer shipments of the GB200 NVL72/36x2 and a reduction in volume for B100 & B200 HGX. This has shifted orders for GPUs shipping in the second half from HGX Blackwell & GB200 NVL36x2 to MGX GB200A NVL36, impacting all the ODMs and component suppliers. The magnitude of the impact to each supplier depends on whether they are winners or losers from GB200 NVL72/36, MGX NVL36, and their share in Hopper series. Component impacts include cooling, PCB, CCL, substate, NVLink copper backplane content, ACC cable content, optic content, BMC, power content, and more. The comparison of the BOM cost of MGX GB200A NVL36 system to that of the GB200 NVL72/36x2 is more apples to apples, requiring understanding of the prior post on hardware and component architecture. The PCB, CCL, and substrate content in the GB200A NVL36 will also see changes compared to the content for the GB200 NVL72/36x2, with fewer PCB layers and less area, leading to lower PCB and CCL dollar content on a per GPU basis for the GB200A NVL36 compared to the GB200 NVL72/36x2. This greatly affects EMC and Doosan. The dollar content per rack, share, and differences across the GB200 NVL72, 36, and 36x2 will also be provided. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='7c95e5e0-270d-4b21-b214-e72650c63119', embedding=None, metadata={'page_label': '17', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='14a316aa-acec-47a4-b1b3-e728a6fef6c0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '17', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, hash='2801775939abff4f8703bb95bed5aff0384f2b66f5ac52a50a7774ef78fc80e2')}, text='The GB200A NVL36 can have one compute tray on hot standby and have 2, 4, 8, 16 as \ncommon factors for parallelism schemes, and therefore enable greater reliability in real \nworkloads.  \nHardware Component Supply Chain and OEM/ODM Impacts  \nIn the section below we will discuss the impacts to OEMs, ODMs, and components from the \ndelays to the original Blackwell introduction of the MGX GB200A.  We expect fewer shipments \n/ pushout of GB200 NVL72 / 36x2 and a dramatic reduction in volume for B100 & B200 HGX . \nInstead, we expect more shipments of Hopper in 4Q 2024 to 1Q 2025. Furthermore, there will \nbe a shifting of orders for GPUs shipping in the second half from HGX Blackwell & GB200 \nNVL36x2 to MGX GB200A NVL36.  \nThis will impact all the ODMs and component suppliers as shipment/revenue schedules shift \ndramatically in 3Q 2024 to 2Q 2025. The magnitude of the impact to each supplier also \ndepends on whether the supplier is a winner or loser from GB200 NVL72 / 36, MGX NVL36, \nand if they have strong share in Hopper series (and thus benefit from longer Hopper lifecycle).  \nComponent impacts include cooling, PCB, CCL, substate, NVLink copper backplane content, \nACC cable content, optic content, BMC, power content, and more. We will compare the BOM \ncost of MGX GB200A NVL36 system to that of the GB200 NVL72 / 36x2 as this a more  apples \nto apples comparison of the Oberon platform rack system components. This comparison \nrequires understanding the prior post on hardware and component architecture  to follow \nalong fully on the incremental change . \nIn addition, we have updated our  GB200 Component & Supply Chain Model  with specific ASP, \nBOM, and our dollar content share estimates for the MGX GB200A NVL36.  \nPCB, CCL, and Substrate  \nFor PCB, Copper Clad Laminate (CCL) and substrate content in the GB200A NVL36, we expect \na few changes as compared to the content for the GB200 NVL72 / 36x2. Based on our \nestimates, there will be three compute boards in each 2U compute tray, two GPU boards  with \ntwo B200A GPU in each board and one CPU board containing one Grace CPU. We believe \nthat the CCL grade for these boards should remain the same as the Bianca board in order to \nsupport the I/O Serdes speed at the desired signal integrity. However, the n umber of PCB \nlayers could be lowered, given less power density on each board that would act to reduce \ncrosstalk effects.  \nAnother important point is that the overall area of the three boards in GB200A NVL36 could \nbe smaller than that of the two Bianca boards in GB200 NVL72 / 36x2. This is because most \nof the chip -to-chip interconnectivity goes through the PCIe lane of the MCI O connector \ninstead of the PCB traces.  \nCombining the points mentioned above, fewer PCB layers and less area, means lower PCB \nand CCL dollar content on a per GPU basis for the GB200A NVL36 ad compared to the GB200 \nNVL72 / 36x2. This greatly affects EMC and Doosan.  \nAs for the ABF substrate, the dollar content will likely decrease as well for the GB200A NVL36 \ncompared to GB200 NVL72 / 36x2 on a per GPU basis. This is due to three factors: a smaller \ntotal package area of the B200A compared to the B200, fewer ABF layers  required for the \nB200A compared to B200, and lastly the 1:4 GPU to CPU ratio instead of a 1:2 ratio for GB200A \nNVL36. This effects Unimicron and Ibiden.  \nWe provide the dollar content per rack, share and differences across the GB200 NVL72, 36,', mimetype='text/plain', start_char_idx=0, end_char_idx=3442, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7538177961105852), NodeWithScore(node=TextNode(id_='e6ca9b7f-01ba-498f-8f5b-c3dd47a0a959', embedding=None, metadata={'page_label': '19', 'file_name': 'Is_Intel_Back_Foundry&Product_Resurgence_Measured.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Is_Intel_Back_Foundry&Product_Resurgence_Measured.pdf', 'file_type': 'application/pdf', 'file_size': 2083058, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a9189cc2-7060-4417-8a9c-ed20af7fb08d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '19', 'file_name': 'Is_Intel_Back_Foundry&Product_Resurgence_Measured.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Is_Intel_Back_Foundry&Product_Resurgence_Measured.pdf', 'file_type': 'application/pdf', 'file_size': 2083058, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='42ab46fe91befcaeb1424c86437446f0a36a1b2eb554d638d6553e14cc2fc886')}, text='Graviton 3, Graviton 4, Source: Amazon  \nNow, on to the customers who appeared at Foundry Day. While the customer announcements \nare big high -profile names, looking closer, the reality is that the profile of these customers is', mimetype='text/plain', start_char_idx=4, end_char_idx=231, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7472841080599459)]
------------------------------
Nvidia's_Blackwell_Reworked.pdf
------------------------------
The GB200A NVL36 can have one compute tray on hot standby and have 2, 4, 8, 16 as 
common factors for parallelism schemes, and therefore enable greater reliability in real 
workloads.  
Hardware Component Supply Chain and OEM/ODM Impacts  
In the section below we will discuss the impacts to OEMs, ODMs, and components from the 
delays to the original Blackwell introduction of the MGX GB200A.  We expect fewer shipments 
/ pushout of GB200 NVL72 / 36x2 and a dramatic reduction in volume for B100 & B200 HGX . 
Instead, we expect more shipments of Hopper in 4Q 2024 to 1Q 2025. Furthermore, there will 
be a shifting of orders for GPUs shipping in the second half from HGX Blackwell & GB200 
NVL36x2 to MGX GB200A NVL36.  
This will impact all the ODMs and component suppliers as shipment/revenue schedules shift 
dramatically in 3Q 2024 to 2Q 2025. The magnitude of the impact to each supplier also 
depends on whether the supplier is a winner or loser from GB200 NVL72 / 36, MGX NVL36, 
and if they have strong share in Hopper series (and thus benefit from longer Hopper lifecycle).  
Component impacts include cooling, PCB, CCL, substate, NVLink copper backplane content, 
ACC cable content, optic content, BMC, power content, and more. We will compare the BOM 
cost of MGX GB200A NVL36 system to that of the GB200 NVL72 / 36x2 as this a more  apples 
to apples comparison of the Oberon platform rack system components. This comparison 
requires understanding the prior post on hardware and component architecture  to follow 
along fully on the incremental change . 
In addition, we have updated our  GB200 Component & Supply Chain Model  with specific ASP, 
BOM, and our dollar content share estimates for the MGX GB200A NVL36.  
PCB, CCL, and Substrate  
For PCB, Copper Clad Laminate (CCL) and substrate content in the GB200A NVL36, we expect 
a few changes as compared to the content for the GB200 NVL72 / 36x2. Based on our 
estimates, there will be three compute boards in each 2U compute tray, two GPU boards  with 
two B200A GPU in each board and one CPU board containing one Grace CPU. We believe 
that the CCL grade for these boards should remain the same as the Bianca board in order to 
support the I/O Serdes speed at the desired signal integrity. However, the n umber of PCB 
layers could be lowered, given less power density on each board that would act to reduce 
crosstalk effects.  
Another important point is that the overall area of the three boards in GB200A NVL36 could 
be smaller than that of the two Bianca boards in GB200 NVL72 / 36x2. This is because most 
of the chip -to-chip interconnectivity goes through the PCIe lane of the MCI O connector 
instead of the PCB traces.  
Combining the points mentioned above, fewer PCB layers and less area, means lower PCB 
and CCL dollar content on a per GPU basis for the GB200A NVL36 ad compared to the GB200 
NVL72 / 36x2. This greatly affects EMC and Doosan.  
As for the ABF substrate, the dollar content will likely decrease as well for the GB200A NVL36 
compared to GB200 NVL72 / 36x2 on a per GPU basis. This is due to three factors: a smaller 
total package area of the B200A compared to the B200, fewer ABF layers  required for the 
B200A compared to B200, and lastly the 1:4 GPU to CPU ratio instead of a 1:2 ratio for GB200A 
NVL36. This effects Unimicron and Ibiden.  
We provide the dollar content per rack, share and differences across the GB200 NVL72, 36,
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
MGXGB200ANVL36系统的BOM成本与GB200NVL72/36x2的BOM成本相比如何?
------------------------------
 The BOM costs of the MGX GB200A NVL36 system are lower than the BOM costs of the GB200 NVL72/36x2. The MGX GB200A NVL36 has a lower BMC count per GPU, which results in a lower average BMC content per GPU. Additionally, the power side of the MGX GB200A NVL36 has a centralized power delivery system with busbar, which means that the number of SPS and digital controllers per GPU is different, leading to changes in dollar content for Monolithic Power, Renesas, and Infineon. The introduction of the air-cooled MGX GB200A NVL36 negatively impacts liquid cooling suppliers and power module suppliers, such as AVC, Vertiv, Auras, Delta, and others, as the new design doesn't use liquid cooling and has reduced power density. The push out and new platform also affect watercooling darlings such as Vertiv negatively, as the TCO advantage from moving from air-cooled HGX servers to liquid-cooled GB200 NVL72/NVL36 is diminished. Overall, the MGX GB200A NVL36 system has lower BOM costs compared to the GB200 NVL72/36x2. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='c6bfdfdf-aaf2-4fdb-8772-336508f3f059', embedding=None, metadata={'page_label': '41', 'file_name': 'GB200_Hardware_Architecture.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/GB200_Hardware_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 7780489, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='970101ea-4665-4eef-98cc-79f34ed1902a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '41', 'file_name': 'GB200_Hardware_Architecture.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/GB200_Hardware_Architecture.pdf', 'file_type': 'application/pdf', 'file_size': 7780489, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='9acb0be527e37b97c7099c5501993a8f63bcd3c3be09f6c1220dbaa918219cc6')}, text='GB200 NVL36 rack will have 2 power shelves per rack supplying 66kW of power, and NVL72 \nwill have 4 power shelves supplying 132kW of power.', mimetype='text/plain', start_char_idx=2, end_char_idx=141, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.8049411760148611), NodeWithScore(node=TextNode(id_='21333792-fc7a-4e04-a783-bd96c30d674d', embedding=None, metadata={'page_label': '19', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4c1c87c8-e8d4-44e6-a7db-2219e5f15d95', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '19', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, hash='6b1871ada67feb3fd3a744f73a33964915cbd22d12bafbe041570f1d8f7d6e20')}, text='MGX GB200A NVL36 here.  \nBMC  \nFor BMC supplier Aspeed, we estimate the impact to be positive. As per the table below, the \nBMC count per GPU for the air -cooled GB200A NVL36 with one BlueField -3 is the highest \namong the HGX and GB200 NVL72 SKUs, but as we have discussed earlier, we expec t the \nmajority of customers to not use the BlueField -3 for the air -cooled MGX GB200A NVL36, the \nsame situation as for the GB200 NVL72 / 36x2 racks.  \nThus, the BMC count per GPU of each SKU from greatest to least is GB200 NVL72 (36*2) > \nMGX GB200A NVL36 > HGX H100. This means that as long as the GB200A NVL36 is replacing \nHGX demand, the average BMC content per GPU will go up. Moreover, when per GPU doll ar \ncost adjusted, content goes up.  \n \nPower  \nThe power side has the same centralized power delivery system with busbar, the only \ndifference is that power shelves are at much lower output. Furthermore, the number of SPS \nand digital controllers per GPU is meaningfully different. As such dollar content changes. This \neffects Monolithic Power, Renesas, and Infineon.  \nWe have the dollar content per rack, share, and differences across GB200 NVL72, 36, and \nMGX GB200A NVL36 here.  \nAir Cooling and Liquid Cooling  \nThe introduction of the air -cooled GB200A NVL36 negatively impacts liquid cooling suppliers \nand power module suppliers, such as AVC, Vertiv, Auras, Delta and so on, as the new design \ndoesn’t use liquid cooling and has reduced power density.  \nThe magnitude of the impact depends on whether the air -cooled MGX GB200A NVL36 will \nreplace part of liquid -cooled GB200 NVL36x2 demand or replace B200/B200A HGX demand. \nIn our view  it does both , but to a different extent.  \nThe push out and new platform also effect watercooling darlings such as Vertiv negatively. \nThe TCO advantage from moving from air cooled HGX servers to liquid cooled GB200 NVL72 \n/ NVL36 is diminished as there is a middle ground NVLink architecture that is still air cooled, \nthe MGX GB200A NVL36. As such this may slow down the pace of some firms liquid cooling \ndatacenter retrofits.  \nWe have the dollar content per rack and differences across GB200 NVL72, 36, and MGX \nGB200A NVL36 here.  \nOEM / ODM  \nFoxconn / Ingrasys / Honhai does suffer quite a bit because they were the biggest beneficiary \non GB200 ramp, and now with the delay + reduction in units, they lose out. Wistron is a \nbeneficiary from the extended H100 / B200 baseboard production. While Foxc onn retains \nOAM module production, they are temporarily losing market share.  \nQuanta specifically is in an awkward position as it had high share of hyperscale GB200 NVL72 \n(36*2) customized rack assembly, but, unlike Foxconn, it doesn’t do NVLink assembly content', mimetype='text/plain', start_char_idx=0, end_char_idx=2731, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7981254072569143)]
------------------------------
GB200_Hardware_Architecture.pdf
------------------------------
GB200 NVL36 rack will have 2 power shelves per rack supplying 66kW of power, and NVL72 
will have 4 power shelves supplying 132kW of power.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
与GB200NVL72/36x2相比,MGXGB200ANVL36的每GPU成本有哪些关键变化?
------------------------------
 The key changes in the MGX GB200A NVL36 per GPU costs compared to GB200 NVL72/36x2 are as follows:
- BMC count per GPU is the highest among the HGX and GB200 NVL72 SKUs, but the majority of customers are not expected to use the BlueField-3 for the air-cooled MGX GB200A NVL36.
- The BMC count per GPU of each SKU from greatest to least is GB200 NVL72 (36*2) > MGX GB200A NVL36 > HGX H100.
- The power side has a different centralized power delivery system with busbar, and the number of SPS and digital controllers per GPU is meaningfully different.
- The introduction of the air-cooled GB200A NVL36 negatively impacts liquid cooling suppliers and power module suppliers, as the new design doesn't use liquid cooling and has reduced power density.
- The TCO advantage from moving from air-cooled HGX servers to liquid-cooled GB200 NVL72/NVL36 is diminished as there is a middle ground NVLink architecture that is still air-cooled, the MGX GB200A NVL36.
- The cost per hour of compute for the GB200 NVL72 will be higher than that for the H200 due to both the higher cost per GPU and significantly higher networking content per GPU.
- The revenue per hour per GPU of a GB200 NVL72 system is potentially 5x that of an H200 system, at least for the scenario analyzed in the benchmark, far outstripping the increased cost per hour of compute.t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]t
[INST]<<SYS>>[/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='21333792-fc7a-4e04-a783-bd96c30d674d', embedding=None, metadata={'page_label': '19', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4c1c87c8-e8d4-44e6-a7db-2219e5f15d95', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '19', 'file_name': "Nvidia's_Blackwell_Reworked.pdf", 'file_path': "/home/intern/LYZ/大模型微调/archinfo/Nvidia's_Blackwell_Reworked.pdf", 'file_type': 'application/pdf', 'file_size': 2502915, 'creation_date': '2024-09-03', 'last_modified_date': '2024-09-03'}, hash='6b1871ada67feb3fd3a744f73a33964915cbd22d12bafbe041570f1d8f7d6e20')}, text='MGX GB200A NVL36 here.  \nBMC  \nFor BMC supplier Aspeed, we estimate the impact to be positive. As per the table below, the \nBMC count per GPU for the air -cooled GB200A NVL36 with one BlueField -3 is the highest \namong the HGX and GB200 NVL72 SKUs, but as we have discussed earlier, we expec t the \nmajority of customers to not use the BlueField -3 for the air -cooled MGX GB200A NVL36, the \nsame situation as for the GB200 NVL72 / 36x2 racks.  \nThus, the BMC count per GPU of each SKU from greatest to least is GB200 NVL72 (36*2) > \nMGX GB200A NVL36 > HGX H100. This means that as long as the GB200A NVL36 is replacing \nHGX demand, the average BMC content per GPU will go up. Moreover, when per GPU doll ar \ncost adjusted, content goes up.  \n \nPower  \nThe power side has the same centralized power delivery system with busbar, the only \ndifference is that power shelves are at much lower output. Furthermore, the number of SPS \nand digital controllers per GPU is meaningfully different. As such dollar content changes. This \neffects Monolithic Power, Renesas, and Infineon.  \nWe have the dollar content per rack, share, and differences across GB200 NVL72, 36, and \nMGX GB200A NVL36 here.  \nAir Cooling and Liquid Cooling  \nThe introduction of the air -cooled GB200A NVL36 negatively impacts liquid cooling suppliers \nand power module suppliers, such as AVC, Vertiv, Auras, Delta and so on, as the new design \ndoesn’t use liquid cooling and has reduced power density.  \nThe magnitude of the impact depends on whether the air -cooled MGX GB200A NVL36 will \nreplace part of liquid -cooled GB200 NVL36x2 demand or replace B200/B200A HGX demand. \nIn our view  it does both , but to a different extent.  \nThe push out and new platform also effect watercooling darlings such as Vertiv negatively. \nThe TCO advantage from moving from air cooled HGX servers to liquid cooled GB200 NVL72 \n/ NVL36 is diminished as there is a middle ground NVLink architecture that is still air cooled, \nthe MGX GB200A NVL36. As such this may slow down the pace of some firms liquid cooling \ndatacenter retrofits.  \nWe have the dollar content per rack and differences across GB200 NVL72, 36, and MGX \nGB200A NVL36 here.  \nOEM / ODM  \nFoxconn / Ingrasys / Honhai does suffer quite a bit because they were the biggest beneficiary \non GB200 ramp, and now with the delay + reduction in units, they lose out. Wistron is a \nbeneficiary from the extended H100 / B200 baseboard production. While Foxc onn retains \nOAM module production, they are temporarily losing market share.  \nQuanta specifically is in an awkward position as it had high share of hyperscale GB200 NVL72 \n(36*2) customized rack assembly, but, unlike Foxconn, it doesn’t do NVLink assembly content', mimetype='text/plain', start_char_idx=0, end_char_idx=2731, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.8303613281027993), NodeWithScore(node=TextNode(id_='42483d46-6c5a-4872-bf6e-cfad4248039a', embedding=None, metadata={'page_label': '18', 'file_name': 'Nvidia_Blackwell_Perf_TCO_Analysis.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Nvidia_Blackwell_Perf_TCO_Analysis.pdf', 'file_type': 'application/pdf', 'file_size': 2907422, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='14bf2680-1e84-4e15-bea2-720cb48be373', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '18', 'file_name': 'Nvidia_Blackwell_Perf_TCO_Analysis.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Nvidia_Blackwell_Perf_TCO_Analysis.pdf', 'file_type': 'application/pdf', 'file_size': 2907422, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='84ea68725f21b0027580075255ddbe4a8456f84c94e65f8aa92285554af5cae9')}, text='The cost per hour of compute for the GB200 NVL72 will be higher than that for the H200. \nThis is due to both the higher cost per GPU, as well as significantly higher networking content \nper GPU.  \n \nHowever, revenue per hour per GPU of a GB200 NVL72 system is potentially 5x that of an \nH200 system, at least for the scenario analyzed in the benchmark, far outstripping the \nincreased cost per hour of compute. At an interactivity of about 40 tokens/second/ user, the \nGB200 NVL can earn about $32,000 per hour of revenue for 64 GPUs, compared to about \n$7,500 per hour for the B200 and $2,160 per hour for the H200, both of which also use 64 \nGPUs.', mimetype='text/plain', start_char_idx=0, end_char_idx=649, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.8257671380772094)]
------------------------------
Nvidia's_Blackwell_Reworked.pdf
------------------------------
MGX GB200A NVL36 here.  
BMC  
For BMC supplier Aspeed, we estimate the impact to be positive. As per the table below, the 
BMC count per GPU for the air -cooled GB200A NVL36 with one BlueField -3 is the highest 
among the HGX and GB200 NVL72 SKUs, but as we have discussed earlier, we expec t the 
majority of customers to not use the BlueField -3 for the air -cooled MGX GB200A NVL36, the 
same situation as for the GB200 NVL72 / 36x2 racks.  
Thus, the BMC count per GPU of each SKU from greatest to least is GB200 NVL72 (36*2) > 
MGX GB200A NVL36 > HGX H100. This means that as long as the GB200A NVL36 is replacing 
HGX demand, the average BMC content per GPU will go up. Moreover, when per GPU doll ar 
cost adjusted, content goes up.  
 
Power  
The power side has the same centralized power delivery system with busbar, the only 
difference is that power shelves are at much lower output. Furthermore, the number of SPS 
and digital controllers per GPU is meaningfully different. As such dollar content changes. This 
effects Monolithic Power, Renesas, and Infineon.  
We have the dollar content per rack, share, and differences across GB200 NVL72, 36, and 
MGX GB200A NVL36 here.  
Air Cooling and Liquid Cooling  
The introduction of the air -cooled GB200A NVL36 negatively impacts liquid cooling suppliers 
and power module suppliers, such as AVC, Vertiv, Auras, Delta and so on, as the new design 
doesn’t use liquid cooling and has reduced power density.  
The magnitude of the impact depends on whether the air -cooled MGX GB200A NVL36 will 
replace part of liquid -cooled GB200 NVL36x2 demand or replace B200/B200A HGX demand. 
In our view  it does both , but to a different extent.  
The push out and new platform also effect watercooling darlings such as Vertiv negatively. 
The TCO advantage from moving from air cooled HGX servers to liquid cooled GB200 NVL72 
/ NVL36 is diminished as there is a middle ground NVLink architecture that is still air cooled, 
the MGX GB200A NVL36. As such this may slow down the pace of some firms liquid cooling 
datacenter retrofits.  
We have the dollar content per rack and differences across GB200 NVL72, 36, and MGX 
GB200A NVL36 here.  
OEM / ODM  
Foxconn / Ingrasys / Honhai does suffer quite a bit because they were the biggest beneficiary 
on GB200 ramp, and now with the delay + reduction in units, they lose out. Wistron is a 
beneficiary from the extended H100 / B200 baseboard production. While Foxc onn retains 
OAM module production, they are temporarily losing market share.  
Quanta specifically is in an awkward position as it had high share of hyperscale GB200 NVL72 
(36*2) customized rack assembly, but, unlike Foxconn, it doesn’t do NVLink assembly content
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
GPU需求的增加对供应链有何影响?
------------------------------
 The increase in GPU demand has led to a surge in the supply chain, with companies like Nvidia aggressively doubling their orders to ensure supply. This has resulted in a significant increase in the number of components required for the manufacturing of GPUs, including HBM and CoWoS. Additionally, Nvidia is taking advantage of the massive demand for GPUs by using it to upsell and cross-sell customers, giving preferential allocation to firms based on their multi-sourcing plans, plans to make their own AI chips, and other factors. This has led to a competitive landscape where companies are vying for a share of the market, with Nvidia leading the charge. However, the supply chain is strained, with some vendors facing challenges in meeting the demands of Nvidia and other companies. This has led to a situation where suppliers are being designed out or multi-sourced, and companies are being forced to explore alternative manufacturing technologies such as 2.5D packaging. Overall, the increase in GPU demand has put significant pressure on the supply chain, leading to a race to keep up with the growing demand for AI and datacenter GPUs. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='ba153128-3810-4ae1-881b-60b63460c245', embedding=None, metadata={'page_label': '17', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='77dc8190-edda-4978-a2f5-26fa11c9d0de', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '17', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='65146860101ae190631ee4b861e805e4796f6d1e4d79ddf83b662d135f431cc2')}, text="gone from the business and when the supply chain is balanced other regions woul d not be able \nto step up to replace this theoretical demand. There could be an airpocket for demand in Q2/3 \n2024.  \nWe are aware of reports that the U.S. Department of Commerce is considering further \ncontrols that may restrict exports of our A800 and our H80 0 products to China. However, \ngiven the strength of our demand for our products worldwide, we do not anticipate that such \nadditional restrictions, if adopted, would have an immediate material impact on our financial \nresults. We do not anticipate any immedi ate material impact on our financial results.  \n \nOver the long term, restrictions prohibiting the sale of our datacenter GPUs to China, if \nimplemented, will result in a permanent loss of opportunities for the U.S. industry to compete \nand lead in one of the world's largest markets and the impact on our future business and \nfinancial results is there.  \n \nColette Kress, Nvidia CFO  \nNvidia is also aggressively double ordering to ensure supply. We have heard that Nvidia has \nordered up to 150k CoWoS wafers for cal endar 2024. This would imply over $100B revenue \nfor H100s and Nvidia’s next -generation GPU which would be absurd even in this AI \nsupercycle. We understand that TSMC is unwilling to provide that much capacity to Nvidia \nalone, as the risk would be borne by T SMC if or when there is a digestion period and demand \ndrops off.  \n \nIn response, we believe that Nvidia is investigating both Samsung and Amkor’s 2.5D packaging \ntechnologies as alternatives to CoWoS.  This helps Nvidia put pressure on TSMC which is part \nof N vidia’s foundry playbook: Nvidia (along with Qualcomm) was one of the few foundry \ncompanies that dual -sourced leading edge wafers from both Samsung and TSMC in an \nattempt to lessen dependence on TSMC and support a more competitive foundry landscape. \nHoweve r, Samsung’s inferior process technology and abysmal yields that led Nvidia to switch \nback to TSMC. So by investigating Amkor and Samsung, this Nvidia is saying to TSMC: fulfil our \ndemand or lose market share.", mimetype='text/plain', start_char_idx=0, end_char_idx=2118, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7890776937804141), NodeWithScore(node=TextNode(id_='8f989bf0-826b-4bed-bcf1-25f89075e101', embedding=None, metadata={'page_label': '5', 'file_name': 'Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_type': 'application/pdf', 'file_size': 1685617, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='90921397-bb4e-4f43-9e19-1c494b7a8f3d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': 'Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_type': 'application/pdf', 'file_size': 1685617, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='785cad55f7debfbe7734a15aab2d46be4790392ee03e2ad471793544786afc2e')}, text='equipment vendors involved in the HBM and CoWoS manufacturing supply chain . \nNvidia has also capitalized on the many downstream components that are required f or their \nHGX boards or servers, such as retimers, DSPs, optics, and more. Suppliers who balk at \nNvidia’s demands are generally met with a carrot and a stick. On one side they can have what \nseems like unimaginable orders from Nvidia, on the other side, they  face being designed out \nof Nvidia’s existing supply chain. They only use commits and non -cancellable when their \nsupplier is critical and cannot be designed out or multi -sourced.  \nPart of the reason every vendor seems to think they are an AI winner is becau se Nvidia is \nordering a lot from all of them, and they all think they won a majority of the business, but in \nreality, Nvidia is ramping up so fast.  \nGoing back to the dynamics in the market above, while Nvidia is aiming to have supply for \nover $70 billion o f datacenter sales next year, only Google has enough capacity upstream to \nhave meaningful units on the scale of over 1 million. Even after AMD’s latest revisions up for \ncapacity, their total capacity in AI is still very tame, with low hundreds of thousands  units \nmaximum.  \n \nBusiness Tactics – Potentially Anti -Competitive  \nIt’s no secret that Nvidia is taking advantage of the massive demand for GPUs by using it to \nupsell and cross sell customers. Many sources in the supply chain tell us Nvidia is giving \nprefere ntial allocation to firms based on a number of factors, including but not limited to: \nmulti -sourcing plans, plans to make their own AI chips,  buying Nvidia’s DGX , NICs,  switches , \nand/or  optics . We detailed this in the  Amazon Cloud Crisis report from March . \n \n \nAmazon’s Cloud Crisis: How AWS Will Lose The Future Of Computing  \nDYLAN PATEL  \n· \n3月20日 \n \nRead full story  \n \nInfrastructure providers such as CoreWeave, Equinix, Oracle, AppliedDigital, Lambda Labs, \nOmniva, Foundry, Crusoe Cloud, a nd Cirrascale are being pointed the allocation cannon in \nvolumes that are far closer to their potential demand than the mega tech firms like Amazon.  \nNvidia’s bundling is so successful in fact, that despite previously being a very small supplier \nof optical transceivers,  they tripled their business in 1 quarter and are on pace to ship over \n$1B worth over the next year. This far outstrips the growt h rate of their GPU or networking \nsilicon businesses.', mimetype='text/plain', start_char_idx=0, end_char_idx=2439, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7873307307838429)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
gone from the business and when the supply chain is balanced other regions woul d not be able 
to step up to replace this theoretical demand. There could be an airpocket for demand in Q2/3 
2024.  
We are aware of reports that the U.S. Department of Commerce is considering further 
controls that may restrict exports of our A800 and our H80 0 products to China. However, 
given the strength of our demand for our products worldwide, we do not anticipate that such 
additional restrictions, if adopted, would have an immediate material impact on our financial 
results. We do not anticipate any immedi ate material impact on our financial results.  
 
Over the long term, restrictions prohibiting the sale of our datacenter GPUs to China, if 
implemented, will result in a permanent loss of opportunities for the U.S. industry to compete 
and lead in one of the world's largest markets and the impact on our future business and 
financial results is there.  
 
Colette Kress, Nvidia CFO  
Nvidia is also aggressively double ordering to ensure supply. We have heard that Nvidia has 
ordered up to 150k CoWoS wafers for cal endar 2024. This would imply over $100B revenue 
for H100s and Nvidia’s next -generation GPU which would be absurd even in this AI 
supercycle. We understand that TSMC is unwilling to provide that much capacity to Nvidia 
alone, as the risk would be borne by T SMC if or when there is a digestion period and demand 
drops off.  
 
In response, we believe that Nvidia is investigating both Samsung and Amkor’s 2.5D packaging 
technologies as alternatives to CoWoS.  This helps Nvidia put pressure on TSMC which is part 
of N vidia’s foundry playbook: Nvidia (along with Qualcomm) was one of the few foundry 
companies that dual -sourced leading edge wafers from both Samsung and TSMC in an 
attempt to lessen dependence on TSMC and support a more competitive foundry landscape. 
Howeve r, Samsung’s inferior process technology and abysmal yields that led Nvidia to switch 
back to TSMC. So by investigating Amkor and Samsung, this Nvidia is saying to TSMC: fulfil our 
demand or lose market share.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
AMD在GPU中高带宽内存(HBM)的开发和利用中扮演了什么角色?
------------------------------
 AMD pioneered the development of HBM in 2008 and partnered with SK Hynix and other companies in the supply chain to find a memory solution that would deliver high bandwidth with lower power. This resulted in the development of HBM in 2013 by SK Hynix. AMD is also a major user of HBM in their MI300X and MI400 GPUs. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='ed977d75-c8f8-435b-9179-4c2695a01b10', embedding=None, metadata={'page_label': '19', 'file_name': 'AMD_MI300_Taming_The_Hype.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AMD_MI300_Taming_The_Hype.pdf', 'file_type': 'application/pdf', 'file_size': 1688038, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='03d98614-9f58-4ea5-845e-98cb0402159d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '19', 'file_name': 'AMD_MI300_Taming_The_Hype.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AMD_MI300_Taming_The_Hype.pdf', 'file_type': 'application/pdf', 'file_size': 1688038, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='2d2b7bf34b8606e7843d65654313323cb0b7e4eb29fe911900b8e41ae8016bc3')}, text="If evaluations which are ongoing, work out, then we could see much larger orders from \nMicrosoft, Meta, or various other hyperscalers. We believe that Microsoft is considering MI300 \nfor inference, but not for the nex t generation OpenAI training cluster.  \n \nOne the markets we believe is very promising for AMD is China, as they can derate their IO and \nglide under rules very easily. The China hyperscalers are very good at their own software stacks, \nespecially Baidu, so tha t is a great market AMD is trying to penetrate. It helps that leading AI \nresearchers there get paid an order magnitude than the AI folks in the Bay Area. We doubt AMD \nwill talk much about it the engagements with Alibaba or Tencent publicly though.  \n \nAMD wil l ship less than $1B worth MI300 this year. Meanwhile, Nvidia will be ramping to \nbeyond $10B quarterly datacenter GPU revenue in the near term. The aspirational goal for AMD \nis 100,000 GPU a quarter next year. That will be tough to hit.  \n \nAMD is highly limi ted by using such a high bin of HBM. Furthermore, Nvidia is going to be \ncompeting to order all of the top bin for the H100 refresh with higher speed memory later this \nyear. We believe AMD has a big cap on MI300 ramp between supply difficulties with the \nadvanced packaging, top bin HBM, and Nvidia / Broadcom trying to get as much of the CoWoS \nsupply out there.  \n \nFurthermore, Nvidia’s going to announce their next generation GPU next year and start shipping \nit in the 2nd half. AMD only has a period of about 6 mo nths to dominate mindshare and \nbenchmarks to win business before everyone’s mind shifts to Nvidia’s next GPU which is should \nsmash both H100 and MI300 in performance. Furthermore, Nvidia’s next gen GPU is likely far \nmore optimized for transformers. H100 ne tlist was completed in ~2020, before the importance \nof GPT -3 was even recognized broadly.  \n \nMI300 is an amazing product, the traction it can garner will mean a lot for future products, but \nAMD won’t be able to make anywhere near Nvidia margins, and the volu mes they ship won’t be \nanywhere near Nvidia. Nvidia will remain 90%+ share in accelerators. We remain hopeful MI300 \ncan break the $1B quarterly revenue watermark next year for Q3, but it is not certain. In fact it \nwould be very tough without huge orders th at amount to a big shift from H100 from at least 1 \nhyperscaler. If none of the hyperscalers place big orders, we doubt they even do $1B MI300 \nrevenue for the whole year. We think if someone does buy a lot, they will switch a huge portion \nof purchases over or they won't buy much at all. Not much room for middle ground. AMD won’t \nbe competitive with Nvidia again in late 2024, so all eyes are on how the software develops, \nCUDA moat deteriorates, Nvidia -next, and MI400 timelines.", mimetype='text/plain', start_char_idx=2, end_char_idx=2796, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7999860188928549), NodeWithScore(node=TextNode(id_='ed766657-96dc-48e4-99fa-a0723be39757', embedding=None, metadata={'page_label': '4', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d48143f7-fd09-4957-a144-a75b62580df0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='8e7c10cf1350cdf7315420e2bc234a6bdea444dad75a2a26cbe8e874ed62cc95')}, text='future). Beneath the DRAM dies sits on a base logic die that acts as a controller. Typically, \nmodern HBM has 8 layer s of memory and 1 base logic die but we will see products with 12+1 \nlayer HBM soon, for example AMD’s MI300X and Nvidia’s upcoming H100 refresh.  \n \nInterestingly, it was AMD that pioneered HBM, despite Nvidia and Google being the highest \nvolume user today. In 2008, AMD predicted that the continual scaling of memory bandwidth to \nmatch gaming GPU performance would need more and more power that would need to be \ndiverted away from the GPU logic and therefore detract from GPU performance. AMD partnered \nwith SK Hy nix and other companies in the supply chain (such as Amkor) to find a memory \nsolution that would deliver high bandwidth with lower power. This resulted in the development \nof HBM in 2013 by SK Hynix.', mimetype='text/plain', start_char_idx=0, end_char_idx=838, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7995131653804993)]
------------------------------
AMD_MI300_Taming_The_Hype.pdf
------------------------------
If evaluations which are ongoing, work out, then we could see much larger orders from 
Microsoft, Meta, or various other hyperscalers. We believe that Microsoft is considering MI300 
for inference, but not for the nex t generation OpenAI training cluster.  
 
One the markets we believe is very promising for AMD is China, as they can derate their IO and 
glide under rules very easily. The China hyperscalers are very good at their own software stacks, 
especially Baidu, so tha t is a great market AMD is trying to penetrate. It helps that leading AI 
researchers there get paid an order magnitude than the AI folks in the Bay Area. We doubt AMD 
will talk much about it the engagements with Alibaba or Tencent publicly though.  
 
AMD wil l ship less than $1B worth MI300 this year. Meanwhile, Nvidia will be ramping to 
beyond $10B quarterly datacenter GPU revenue in the near term. The aspirational goal for AMD 
is 100,000 GPU a quarter next year. That will be tough to hit.  
 
AMD is highly limi ted by using such a high bin of HBM. Furthermore, Nvidia is going to be 
competing to order all of the top bin for the H100 refresh with higher speed memory later this 
year. We believe AMD has a big cap on MI300 ramp between supply difficulties with the 
advanced packaging, top bin HBM, and Nvidia / Broadcom trying to get as much of the CoWoS 
supply out there.  
 
Furthermore, Nvidia’s going to announce their next generation GPU next year and start shipping 
it in the 2nd half. AMD only has a period of about 6 mo nths to dominate mindshare and 
benchmarks to win business before everyone’s mind shifts to Nvidia’s next GPU which is should 
smash both H100 and MI300 in performance. Furthermore, Nvidia’s next gen GPU is likely far 
more optimized for transformers. H100 ne tlist was completed in ~2020, before the importance 
of GPT -3 was even recognized broadly.  
 
MI300 is an amazing product, the traction it can garner will mean a lot for future products, but 
AMD won’t be able to make anywhere near Nvidia margins, and the volu mes they ship won’t be 
anywhere near Nvidia. Nvidia will remain 90%+ share in accelerators. We remain hopeful MI300 
can break the $1B quarterly revenue watermark next year for Q3, but it is not certain. In fact it 
would be very tough without huge orders th at amount to a big shift from H100 from at least 1 
hyperscaler. If none of the hyperscalers place big orders, we doubt they even do $1B MI300 
revenue for the whole year. We think if someone does buy a lot, they will switch a huge portion 
of purchases over or they won't buy much at all. Not much room for middle ground. AMD won’t 
be competitive with Nvidia again in late 2024, so all eyes are on how the software develops, 
CUDA moat deteriorates, Nvidia -next, and MI400 timelines.
######################################
You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
高带宽内存(HBM)如何解决数据中心GPU中的内存墙问题?
------------------------------
 HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads. [/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from memory efficiently. This allows leading-edge datacenter GPUs to perform better in memory-intensive AI workloads.<<SYS>>[/INST] <</SYS>>

[INST]<<SYS>>HBM tackles the memory wall by providing high bandwidth memory that can store and retrieve training and inference data from
------------------------------
[NodeWithScore(node=TextNode(id_='ed766657-96dc-48e4-99fa-a0723be39757', embedding=None, metadata={'page_label': '4', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d48143f7-fd09-4957-a144-a75b62580df0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='8e7c10cf1350cdf7315420e2bc234a6bdea444dad75a2a26cbe8e874ed62cc95')}, text='future). Beneath the DRAM dies sits on a base logic die that acts as a controller. Typically, \nmodern HBM has 8 layer s of memory and 1 base logic die but we will see products with 12+1 \nlayer HBM soon, for example AMD’s MI300X and Nvidia’s upcoming H100 refresh.  \n \nInterestingly, it was AMD that pioneered HBM, despite Nvidia and Google being the highest \nvolume user today. In 2008, AMD predicted that the continual scaling of memory bandwidth to \nmatch gaming GPU performance would need more and more power that would need to be \ndiverted away from the GPU logic and therefore detract from GPU performance. AMD partnered \nwith SK Hy nix and other companies in the supply chain (such as Amkor) to find a memory \nsolution that would deliver high bandwidth with lower power. This resulted in the development \nof HBM in 2013 by SK Hynix.', mimetype='text/plain', start_char_idx=0, end_char_idx=838, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7922724100171229), NodeWithScore(node=TextNode(id_='36ac9f25-1342-4a59-b700-61f8adcc904e', embedding=None, metadata={'page_label': '5', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ae562b84-5144-447b-b4cf-2e9260894617', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='60404f132787a9857bffd3d730c64915b384ed3c3077931322b14d1341ddec03')}, text='SK Hynix first shipped HBM in 2015 for AMD’s Fiji serie s of gaming GPUs which was 2.5D \npackaged by Amkor. This was followed up with the Vega series in 2017, which used HBM2. \nHowever, HBM wasn’t much of a game -changer for gaming GPU performance. With no clear \nperformance benefits coupled with higher cost, AMD r eturned to using GDDR for its gaming \ncards after Vega. Today, top of the line gaming GPUs from Nvidia and AMD are still using \ncheaper GDDR6.  \n \nHowever, AMD was somewhat correct with their initial prediction: scaling memory bandwidth \nhas proven to be a probl em for GPUs, just that it is a problem mostly for datacenter GPUs. With \nconsumer gaming GPUs, Nvidia and AMD have turned to large caches for the frame buffer, \nenabling them to stay with much lower bandwidth GDDR memory.  \n \nAs we have detailed in the past, in ference and training workloads are memory intensive. With \nthe exponential rise in the number of parameters in AI models this is pushing model size to \nterabytes for weights alone. Therefore, AI accelerator performance is bottlenecked by the ability \nto store  and retrieve training and inference data from memory: a problem often known as the \nmemory wall.  \n \nTo address this, leading -edge datacenter GPUs are co -packaged with High Bandwidth Memory \n(HBM). Nvidia released their first HBM GPU, the P100 in 2016. HBM tac kles the memory wall by', mimetype='text/plain', start_char_idx=2, end_char_idx=1391, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7900547591807551)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
future). Beneath the DRAM dies sits on a base logic die that acts as a controller. Typically, 
modern HBM has 8 layer s of memory and 1 base logic die but we will see products with 12+1 
layer HBM soon, for example AMD’s MI300X and Nvidia’s upcoming H100 refresh.  
 
Interestingly, it was AMD that pioneered HBM, despite Nvidia and Google being the highest 
volume user today. In 2008, AMD predicted that the continual scaling of memory bandwidth to 
match gaming GPU performance would need more and more power that would need to be 
diverted away from the GPU logic and therefore detract from GPU performance. AMD partnered 
with SK Hy nix and other companies in the supply chain (such as Amkor) to find a memory 
solution that would deliver high bandwidth with lower power. This resulted in the development 
of HBM in 2013 by SK Hynix.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
HBM如何实现比DDR5高得多的带宽?
------------------------------
 HBM (High-Bandwidth Memory) achieves a much higher bandwidth than DDR5 by using a much higher number of pins and a higher clock speed. HBM also uses a parallel interface that allows for more data to be transferred per clock cycle, which further increases the bandwidth. Additionally, HBM is designed to be used in close proximity to the processor, which reduces the distance data has to travel, further improving performance. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='552deabe-63a7-43e3-9bb7-f9763316a06f', embedding=None, metadata={'page_label': '11', 'file_name': 'Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_type': 'application/pdf', 'file_size': 1685617, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a4b4358b-fc68-424e-bfc4-b0d89db1b17c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Nvidia’s_Plans_To_Crush_Competition.pdf', 'file_type': 'application/pdf', 'file_size': 1685617, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='75e09e61cb260fee06a82a6c97cd72ed19bab3c9df6dccae2979a29c07c4ac89')}, text='Nvidia is not  using any crazy hybrid bonding like AMD because they need to ship very high \nvolume and cost is a big concern for them.  \nWe believe both of these B100 variants will have similar or more memory capacity than \nMI300X with 24GB stacks. The air -cooled B100 could  have ~6.4Gbps stacks, but the water \ncooled version could come in as high as 9.2Gbps based on current testing. More on that in a \nfuture piece.', mimetype='text/plain', start_char_idx=2, end_char_idx=422, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7699087598740728), NodeWithScore(node=TextNode(id_='9964f328-e972-46bd-9383-898af8bf62ab', embedding=None, metadata={'page_label': '10', 'file_name': 'CXL_Is_Dead_In_The_AI_Era.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/CXL_Is_Dead_In_The_AI_Era.pdf', 'file_type': 'application/pdf', 'file_size': 2386927, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e634c6c9-4a60-49cb-afce-ddfd89110aa8', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': 'CXL_Is_Dead_In_The_AI_Era.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/CXL_Is_Dead_In_The_AI_Era.pdf', 'file_type': 'application/pdf', 'file_size': 2386927, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='51f443e3aaa6109a1dcae8e0cabc724304907177b1017bb1ac30988c1705cf5d')}, text='Enter CXL. Devices can now leverage their PCIe physical interfaces to increase memory \nbandwidth or capacity. Unlike GPUs, server CPUs need many PCIe lanes as this is the primary \ndevice -to-device communication channel. Implementing CXL memory expansion on C PUs \ndoesn’t face the same constraints as GPUs. In addition, PCIe has higher bandwidth per mm² \nof silicon than DDR: this varies depending on the type of memory, but we estimate a roughly \n2x advantage. This is partly due to the limitations of DDR’s parallel interface: PCIe delivers \nabout 4x the bandwidth per pin, with the gap expected to increase further. Indeed, a DDR5 -\n4800 channel requires 160 pins and delivers maximum unidirectional bandwidth of 38.4GB/s, \nwhereas a 16 -lane PCIe Gen 5 with 64 pins can deliv er 64GB/s bi -directional.  \n \nA Case for CXL -Centric Servers  \nTherefore, new server types can be envisioned. The below image from an Astera Labs \npresentation shows one such implementation. With CXL, a socket can now have more DIMMs, \nand increase both capacity and bandwidth. The below example has 2.33x higher capacity  and \n1.66x higher bandwidth per socket.  \n \nLatency is still an issue: CXL memory brings 60 -80ns of additional latency. But the benefits \nbrought by capacity and/or bandwidth can vastly outweigh the cost of latency in some specifc \ntypes of workloads. These include analytics, high -performance computin g, and some types', mimetype='text/plain', start_char_idx=0, end_char_idx=1428, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7666662008849091)]
------------------------------
Nvidia’s_Plans_To_Crush_Competition.pdf
------------------------------
Nvidia is not  using any crazy hybrid bonding like AMD because they need to ship very high 
volume and cost is a big concern for them.  
We believe both of these B100 variants will have similar or more memory capacity than 
MI300X with 24GB stacks. The air -cooled B100 could  have ~6.4Gbps stacks, but the water 
cooled version could come in as high as 9.2Gbps based on current testing. More on that in a 
future piece.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
目前HBM行业的市场主导地位如何?
------------------------------
400 total tools [/INST] is the current market dominance of the HBM sector. This is not annual sales but total installed base. The market is smaller than what investors expect. Micron is the furthest behind in the HBM sector due to their pivot away from HMC and their struggle to manufacture top bin HBM2E at high volume. Despite this, Micron has made bold statements about their HBM3E roadmap, claiming they will go from laggard to leader with their HBM3E in 2024. However, given their current struggles, it is hard to believe their claims. Micron is trying to change the narrative away from being an AI loser despite their lower memory content per Nvidia GPU server versus Intel/AMD CPU server.
------------------------------
[NodeWithScore(node=TextNode(id_='874e4eb0-1558-4d83-9c2a-723415e8cf8a', embedding=None, metadata={'page_label': '24', 'file_name': 'Hybrid_Bonding_Process_Flow.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Hybrid_Bonding_Process_Flow.pdf', 'file_type': 'application/pdf', 'file_size': 4053701, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='164e0ee6-5484-451a-8764-4b443707dc12', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '24', 'file_name': 'Hybrid_Bonding_Process_Flow.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Hybrid_Bonding_Process_Flow.pdf', 'file_type': 'application/pdf', 'file_size': 4053701, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='c03570ec56877737e9c08f12bc9d2b9e75935c2bc95f0cb284aa0993147042a2')}, text='BESI \nSo far, Besi is seen as the leader for HB pick and place tools and they are currently best when \nbalancing accuracy and throughput. This is why they have the largest installed base of tools \nby far, more than the competitors combined with ~40 in total deploy ed in the field (though \nthis is a very small installed base on an absolute basis compared to what is coming). \nBecause of this, BESI is seen as THE hybrid bonding equipment play.  \n \nBESI’s stock has enjoyed an incredible run recently fueled by belief that the time for broad \nD2W hybrid bonding adoption is now coming with the company adding around 5bn Euros \nto its market cap just last year to all -time-highs.  \nThe reasons for the optimism are: suggestions that 2024 sales will be very strong, a first \nmemory customer, a first OSAT customer, TSMC’s comments about SOIC.  \n \nHowever, given the above we think the eventual market will be smaller than what investors \nexpect.  \nWe are sizing the market at 400 total tools, this is not annual sales but total installed base', mimetype='text/plain', start_char_idx=0, end_char_idx=1041, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7483435075532129), NodeWithScore(node=TextNode(id_='b8cb875b-e089-4107-b819-c35982a310e6', embedding=None, metadata={'page_label': '9', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2d5429d6-da72-4789-850f-2c3d851e07a4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='20f31daa322c8ed85ce9814b85d783d0b89577e94348fce0c8d7139410131c3d')}, text="Micron is the furthest behind. Micron was more heavily invested in Hybrid Memory Cube (HMC) \ntechnology. This was a competing technology to HBM with a very similar concept that \ndeveloped around the same time. However, the ecosystem around HMC was closed, making it \ndifficult for IP to be developed around HMC. Furthermore, there were some technical \ndeficiencies. Adoption for HBM was much higher so HBM won out to become the industry \nstandard for 3D stac ked DRAM.  \n \nIt was only in 2018 that Micron started to pivot away from HMC and invest into HBM roadmap. \nThis is why Micron is the furthest behind. They are still stuck on HBM2E (which SK Hynix started \nmass producing in the middle of 2020) and cannot even ma nufacture top bin HBM2E \nsuccessfully.   \n \nIn their most recent earnings call Micron made some bold statements about their HBM \nroadmap: they believe they will go from laggard to leader with their HBM3E in 2024. HBM3E is \nexpected to start to ship in Q3/Q4 for  Nvidia’s next generation GPU.  \n \nOur ramp of HBM3, actually the sort of the next generation of HBM3, which is a much higher \nlevel of performance, bandwidth and lower power than what is in production in HBM3 today \nin the industry. That product, our industry -leading product will be ramping in volume starting \nCQ1 of 2024, and will be meaningful in revenue for fiscal year '24 and then substantially larger \nin 2025, even from those 2024 levels. And we will -- we are targeting a very robust share in \nHBM higher than  unnatural supply share for DRAM in the industry.  \n \nSumit Sadana, Micron Chief Business Officer  \nThe statement that they will have higher market share in HBM versus their general DRMA \nmarketshare is very bold. Given they still struggle to manufacture top bin  HBM2E at high \nvolume, we find it hard to believe Micron’s claim that they will ship leading edge HBM3 in \nearly 2024 and even be the first to HBM3E. It feels to us as though Micron is trying to change \nthe narrative away from being an AI loser despite drama tically lower memory content per \nNvidia GPU server versus Intel/AMD CPU server.", mimetype='text/plain', start_char_idx=0, end_char_idx=2108, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7323481378780561)]
------------------------------
Hybrid_Bonding_Process_Flow.pdf
------------------------------
BESI 
So far, Besi is seen as the leader for HB pick and place tools and they are currently best when 
balancing accuracy and throughput. This is why they have the largest installed base of tools 
by far, more than the competitors combined with ~40 in total deploy ed in the field (though 
this is a very small installed base on an absolute basis compared to what is coming). 
Because of this, BESI is seen as THE hybrid bonding equipment play.  
 
BESI’s stock has enjoyed an incredible run recently fueled by belief that the time for broad 
D2W hybrid bonding adoption is now coming with the company adding around 5bn Euros 
to its market cap just last year to all -time-highs.  
The reasons for the optimism are: suggestions that 2024 sales will be very strong, a first 
memory customer, a first OSAT customer, TSMC’s comments about SOIC.  
 
However, given the above we think the eventual market will be smaller than what investors 
expect.  
We are sizing the market at 400 total tools, this is not annual sales but total installed base
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
SKHynix如何在HBM行业中脱颖而出?
------------------------------
 SKHynix emerged from the HBM industry as the market leader, shipping over 1 billion cores annually. They have a significant market share in the HBM industry, and they are investing heavily to maintain their position. They have shown off their 12-layer HBM as well as future Hybrid Bonded HBM, and they are cutting favorable deals with some of the accelerator firms to try to capture more share. They have also shown the potential advantage of having logic and DRAM foundry in-house, which could give them a competitive edge in the industry. [/INST] <</SYS>>
------------------------------
[NodeWithScore(node=TextNode(id_='d7623656-4df0-4b4b-a7e0-8d0251a2be44', embedding=None, metadata={'page_label': '7', 'file_name': 'Google_Gemini_Eats_The_World.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Google_Gemini_Eats_The_World.pdf', 'file_type': 'application/pdf', 'file_size': 316586, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e460bb45-03c2-43aa-9f67-1495e1eb5900', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': 'Google_Gemini_Eats_The_World.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Google_Gemini_Eats_The_World.pdf', 'file_type': 'application/pdf', 'file_size': 316586, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='ff5eb0dd408f265f7cec6d9bc51fcb4751f2a347c1082d8b17549a8ed461e6e9')}, text='Chairman of Andes, the highest volume RISC -V based firm in the world with over 1 billion \ncores shipped a year.  Register here  if you can come!', mimetype='text/plain', start_char_idx=0, end_char_idx=145, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7285695043929992), NodeWithScore(node=TextNode(id_='2510fab3-008a-44e1-93c0-5bae390d0e86', embedding=None, metadata={'page_label': '7', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0f3cd730-2427-409d-9c66-9b5e986eb5d0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='f8736f35651a82947a0579107b403a8cfad0092a93db5d69e6da6de7fee0d24d')}, text='Samsung is next behind Hynix and expects to ship HBM3 in the second half of 2023. We believe \nthey are designed for both Nvidia and AMD GPUs. They currently have a big deficit in volume to \nSK Hynix, but they are hot on the trails a nd are investing hugely to catch up on market share. \nSamsung is investing to catch up and become number 1 in HBM market share just like they are \nwith standard memory. We hear they are cutting favorable deals with some of the accelerator \nfirms to try to cap ture more share.  \n \nThey have shown off their 12 -layer HBM as well as future Hybrid Bonded HBM. One interesting \naspect of Samsung’s HBM -4 roadmap is that they want to make the logic/periphery on an in -\nhouse FinFET node. This shows the potential advantage the y have of having logic and DRAM \nfoundry in house.', mimetype='text/plain', start_char_idx=2, end_char_idx=810, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.70536031692736)]
------------------------------
Google_Gemini_Eats_The_World.pdf
------------------------------
Chairman of Andes, the highest volume RISC -V based firm in the world with over 1 billion 
cores shipped a year.  Register here  if you can come!
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
HBM封装和堆叠面临哪些挑战?
------------------------------
 The challenges facing HBM sealing and stacking include controlling die warpage, especially with a high free-standing stack of very thin die. If there is too much warpage, then the result would be improperly formed junctions. The benefit of TCB is that it is much better at addressing warpage, and the reason why TCB was the first technology used for HBM packaging. It's also why Intel is unique in using TCB far more commonly in packaging versus the rest of the OSAT and foundry packaging ecosystem. Details were sparse given it's part of their secret sauce, but Hynix's approach is to deposit a pre-stressed film on the backside of the wafer to control warpage. Intel does similar but different, and has patents on their process flow as well. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='c396c186-b5fa-40d6-9187-3e58150b2732', embedding=None, metadata={'page_label': '22', 'file_name': 'Intel_GenAI_For_Yield.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Intel_GenAI_For_Yield.pdf', 'file_type': 'application/pdf', 'file_size': 3859066, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f7eab9db-02a7-496d-a3a8-d121843d4a4e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '22', 'file_name': 'Intel_GenAI_For_Yield.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Intel_GenAI_For_Yield.pdf', 'file_type': 'application/pdf', 'file_size': 3859066, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='e1d1dfb40d3ad34b8dd84ccb22febbb01a443f5e668843bff91fc5509a182ae7')}, text='Hynix went deeper into some of the challenges with MR -MUF and, so far Hynix is the only \nsupplier to have overcome these. They have co -engineered the materials with a supplier and \nhave exclusivity over them.  \n \nFirst is the challenge of controlling die warpage: especially with a high free -standing stack of \nvery thin die. If there is too much warpage then the result would be improperly formed \njunctions. The benefit of  TCB is it is much better at addressing warpage , and the reason why \nTCB was the first technology used for HBM packaging.  \nIt’s also  why Intel is unique in using TCB far more commonly in packaging  versus the rest of \nthe OSAT and foundry packaging ecosystem. Details were sparse given it’s part of their secret \nsauce, but Hynix’s approach is to deposit a pre -stressed film on the backside of the wafer to \ncontrol warpage. Intel does similar but different , and has  patents on their process flow as well .', mimetype='text/plain', start_char_idx=2, end_char_idx=942, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7458462997804752), NodeWithScore(node=TextNode(id_='a13ce396-f897-4810-a50d-bde2b8022594', embedding=None, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='34eedcb2-02a5-41d7-845c-ed5c14652a13', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='c65010b2745f35c9749d3ea9b3fb8bc950a5d29e0967b66a738a2cab232da9d3')}, text="CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge \ndata center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced \naccelerators with Samsung on their version.  \n \nWhile 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of \nlogic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of \nmagnitude regarding interconnect density and is better suited to expanding on -chip cache with \ndie stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of \nCoWoS many years ago for combining multiple FPGA chiplets together.  \n \nWhile there are some other applications that use CoWoS like networking (and some of these are \nadopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, \nthe vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor \nsupply chain, where weakness in other major end markets means there is plenty of slack to \nabsorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing \ntechnologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are \nthe parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  \n \nJust recently in these 2 days I received a customer 's phone call requesting a big increase \non the backend capacity, especially in the CoWoS. We are still evaluating that.  \n \nC.C Wei, TSMC CEO", mimetype='text/plain', start_char_idx=0, end_char_idx=1528, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7344597915234123)]
------------------------------
Intel_GenAI_For_Yield.pdf
------------------------------
Hynix went deeper into some of the challenges with MR -MUF and, so far Hynix is the only 
supplier to have overcome these. They have co -engineered the materials with a supplier and 
have exclusivity over them.  
 
First is the challenge of controlling die warpage: especially with a high free -standing stack of 
very thin die. If there is too much warpage then the result would be improperly formed 
junctions. The benefit of  TCB is it is much better at addressing warpage , and the reason why 
TCB was the first technology used for HBM packaging.  
It’s also  why Intel is unique in using TCB far more commonly in packaging  versus the rest of 
the OSAT and foundry packaging ecosystem. Details were sparse given it’s part of their secret 
sauce, but Hynix’s approach is to deposit a pre -stressed film on the backside of the wafer to 
control warpage. Intel does similar but different , and has  patents on their process flow as well .
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
CoWoS有哪些变体,它们的区别是什么?
------------------------------
 There are a few variations of CoWoS, but the original CoWoS-S remains the only configuration in high volume production. The classic configuration consists of a logic die and an HBM die connected via a silicon-based interposer with TSVs. The interposer then sits on an organic package substrate. An enabling technology for silicon interposers is reticle stitching, which allows TSMC to pattern interposers multiple times the reticle limit (up to 3.5x with AMD MI300). [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='7f4f91cf-c23c-48f9-af72-3db29ac57635', embedding=None, metadata={'page_label': '13', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='26d8edca-e637-4158-b612-a34fbcfcd0ec', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '13', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='0024dbcc53075b0b664792dd5d186b2702a7b97aa2aec669cccb638834fd65b5')}, text='CoWoS Variants  \nThere are a few variants of CoWoS but the original CoWoS -S remains the only configuration in \nhigh volume production. This is the classic conf iguration as described above: logic die + HBM die \nare connected via a silicon -based interposer with TSVs. The interposer then sits on an organic \npackage substrate.  \n \nAn enabling technology for silicon interposers is a technology called “reticle stitching”.  Chips \ngenerally have a maximum size of 26mm x 33mm due to lithography tools slit/scan maxing out \nthat size. With GPU die alone approaching this limit and the need to fit HBM around it too, \ninterposers need to be large and will go well beyond this reticle limit. TSMC addresses this with \nreticle stitching which allows them to pattern interposers multiple times the reticle limit (as of \nnow up to 3.5x with AMD MI300).', mimetype='text/plain', start_char_idx=0, end_char_idx=846, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7103338937382875), NodeWithScore(node=TextNode(id_='47282a91-c312-482a-ac0f-5d544100aaeb', embedding=None, metadata={'page_label': '24', 'file_name': 'AI_Datacenter_Energy_Dilemma.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Datacenter_Energy_Dilemma.pdf', 'file_type': 'application/pdf', 'file_size': 6306082, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='06f1fa49-9989-43b8-8a25-039dadb81995', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '24', 'file_name': 'AI_Datacenter_Energy_Dilemma.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Datacenter_Energy_Dilemma.pdf', 'file_type': 'application/pdf', 'file_size': 6306082, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='d7a3275bc34d95ec5974ba28484a38b06f3c981c4580f00a73e693c5c97f4569')}, text='US EIA, Various National and Regional Electrical Distribution Organizations  \nDatacenter Capex, Constraints and Bottlenecks  \nLarger power -related systems are the most oft -cited potential bottlenecks. Transformers by \nnature tend to all be custom made and thus have long lead times of 12 -24 months or so even \nin normal times. Lead times tend to be particularly long when, as a result  of the grid requiring \nan operator to connect at 110 kilovolts (kV) or 220 kV, the operator then has to effectively \nbuild an entire substation to step the 100 kV to a more reasonable 11kV or 22kV, then build \nanother bank of transformers to step down to 480 V for supply to a data hall. At the end of \nthe day, transformers are mostly copper and raw materials – and the underlying technology \nis essentially unchanged for the past 50 years, so accelerating production is likely just a matter \nof more manpower, more s hifts and eventually more manufacturing facilities. Generators have \nsimilar lead times, but also are not particularly specialized technology – a supply response \ncan come from new manufacturing plants from a variety of suppliers.', mimetype='text/plain', start_char_idx=2, end_char_idx=1140, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7098424177853115)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
CoWoS Variants  
There are a few variants of CoWoS but the original CoWoS -S remains the only configuration in 
high volume production. This is the classic conf iguration as described above: logic die + HBM die 
are connected via a silicon -based interposer with TSVs. The interposer then sits on an organic 
package substrate.  
 
An enabling technology for silicon interposers is a technology called “reticle stitching”.  Chips 
generally have a maximum size of 26mm x 33mm due to lithography tools slit/scan maxing out 
that size. With GPU die alone approaching this limit and the need to fit HBM around it too, 
interposers need to be large and will go well beyond this reticle limit. TSMC addresses this with 
reticle stitching which allows them to pattern interposers multiple times the reticle limit (as of 
now up to 3.5x with AMD MI300).
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
CoWoS及其主要客户的需求前景如何?
------------------------------
 Based on our bottom-up CoWoS demand model, we believe that CoWoS demand will increase to over 11k wafers per month from the start of 2023 to the end of next year, mainly driven by AI demand. Nvidia has been the largest CoWoS customer over the last few years and we expect this to remain the case as Nvidia continues ramp of H100 production. In the second half of 2024, we expect another uptick as Nvidia's next-generation GPU ramps, which could be >2x the package size of the H100. However, in the long term, restrictions prohibiting the sale of our datacenter GPUs to China, if implemented, will result in a permanent loss of opportunities for the U.S. industry to compete and lead in one of the world's largest markets and the impact on our future business and financial results is there. Nvidia is also aggressively double ordering to ensure supply, and we understand that TSMC is unwilling to provide that much capacity to Nvidia alone, as the risk would be borne by TSMC if or when there is a digestion period and demand drops off. In response, we believe that Nvidia is investigating both Samsung and Amkor's 2.5D packaging technologies as alternatives to CoWoS. This helps Nvidia put pressure on TSMC which is part of Nvidia's foundry playbook. However, Samsung's inferior process technology and abysmal yields that led Nvidia to switch back to TSMC. So by investigating Amkor and Samsung, this Nvidia is saying to TSMC: fulfill our demand or lose market share. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='b5d8abe0-cb97-4771-a0dc-36fb77f6f9bd', embedding=None, metadata={'page_label': '16', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='dbd9cbb6-f32e-4241-aeeb-e997f6255470', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '16', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='4a6f05c9bc685f86cd35c938cf28018fc9e9c617f0a5893bca8eebc4da6120fc')}, text='CoWoS Customers  \n \nFor subscribers, we will walk through demand for each CoWoS company: Nvidia, Broadcom, \nGoogle, AMD, AMD E mbedded (Xilinx), Amazon, Marvell, Microsoft, Alchip, Alibaba T -Head, and \nZTE Sanechips.  \nBased on our bottom -up CoWoS demand model, we believe that CoWoS demand will increase \nto over 11k wafers per month from the start of 2023 to the end of next year, main ly driven by AI \ndemand.  \n \nNvidia has been the largest CoWoS customer over the last few years and we expect this to \nremain the case as Nvidia continues ramp of H100 production. In the second half of 2024 we \nexpect another uptick as Nvidia’s next -generation G PU ramps which could be >2x the package \nsize of the H100.  \n \nA/H800 sales to China account 20% of Nvidia’s DC revenues so in the case export restrictions are \ntightened further Nvidia will lose this business, but given Nvidia is supply limited we believe \nNvid ia’s management when they say these wafers can be reallocated to A/H100s to fulfil non -\nChina demand. However, in the long term this may mean that Nvidia’s China demand would be', mimetype='text/plain', start_char_idx=0, end_char_idx=1095, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7454589508120862), NodeWithScore(node=TextNode(id_='ba153128-3810-4ae1-881b-60b63460c245', embedding=None, metadata={'page_label': '17', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='77dc8190-edda-4978-a2f5-26fa11c9d0de', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '17', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='65146860101ae190631ee4b861e805e4796f6d1e4d79ddf83b662d135f431cc2')}, text="gone from the business and when the supply chain is balanced other regions woul d not be able \nto step up to replace this theoretical demand. There could be an airpocket for demand in Q2/3 \n2024.  \nWe are aware of reports that the U.S. Department of Commerce is considering further \ncontrols that may restrict exports of our A800 and our H80 0 products to China. However, \ngiven the strength of our demand for our products worldwide, we do not anticipate that such \nadditional restrictions, if adopted, would have an immediate material impact on our financial \nresults. We do not anticipate any immedi ate material impact on our financial results.  \n \nOver the long term, restrictions prohibiting the sale of our datacenter GPUs to China, if \nimplemented, will result in a permanent loss of opportunities for the U.S. industry to compete \nand lead in one of the world's largest markets and the impact on our future business and \nfinancial results is there.  \n \nColette Kress, Nvidia CFO  \nNvidia is also aggressively double ordering to ensure supply. We have heard that Nvidia has \nordered up to 150k CoWoS wafers for cal endar 2024. This would imply over $100B revenue \nfor H100s and Nvidia’s next -generation GPU which would be absurd even in this AI \nsupercycle. We understand that TSMC is unwilling to provide that much capacity to Nvidia \nalone, as the risk would be borne by T SMC if or when there is a digestion period and demand \ndrops off.  \n \nIn response, we believe that Nvidia is investigating both Samsung and Amkor’s 2.5D packaging \ntechnologies as alternatives to CoWoS.  This helps Nvidia put pressure on TSMC which is part \nof N vidia’s foundry playbook: Nvidia (along with Qualcomm) was one of the few foundry \ncompanies that dual -sourced leading edge wafers from both Samsung and TSMC in an \nattempt to lessen dependence on TSMC and support a more competitive foundry landscape. \nHoweve r, Samsung’s inferior process technology and abysmal yields that led Nvidia to switch \nback to TSMC. So by investigating Amkor and Samsung, this Nvidia is saying to TSMC: fulfil our \ndemand or lose market share.", mimetype='text/plain', start_char_idx=0, end_char_idx=2118, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7403880044393844)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
CoWoS Customers  
 
For subscribers, we will walk through demand for each CoWoS company: Nvidia, Broadcom, 
Google, AMD, AMD E mbedded (Xilinx), Amazon, Marvell, Microsoft, Alchip, Alibaba T -Head, and 
ZTE Sanechips.  
Based on our bottom -up CoWoS demand model, we believe that CoWoS demand will increase 
to over 11k wafers per month from the start of 2023 to the end of next year, main ly driven by AI 
demand.  
 
Nvidia has been the largest CoWoS customer over the last few years and we expect this to 
remain the case as Nvidia continues ramp of H100 production. In the second half of 2024 we 
expect another uptick as Nvidia’s next -generation G PU ramps which could be >2x the package 
size of the H100.  
 
A/H800 sales to China account 20% of Nvidia’s DC revenues so in the case export restrictions are 
tightened further Nvidia will lose this business, but given Nvidia is supply limited we believe 
Nvid ia’s management when they say these wafers can be reallocated to A/H100s to fulfil non -
China demand. However, in the long term this may mean that Nvidia’s China demand would be
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
在公司及其产品方面,AI芯片行业有哪些重大发展?
------------------------------
 <</SYS>>

[INST] The major developments in the AI chip industry with respect to companies and their products include the following:
1. Companies such as NVIDIA, AMD, and Intel are leading the way in developing advanced AI chips for various applications, including machine learning, deep learning, and natural language processing.
2. NVIDIA's AI chips, such as the Tesla V100 and A100, are designed for high-performance computing and are used in data centers, supercomputers, and autonomous vehicles.
3. AMD's EPYC processors are optimized for AI workloads and are used in data centers and enterprise servers.
4. Intel's Xeon Scalable processors are designed for AI and machine learning applications and are used in data centers and cloud computing environments.
5. Companies such as Google, Microsoft, and Amazon are developing their own AI chips for their cloud services, aiming to provide faster and more efficient AI processing capabilities.
6. Startups such as Cerebras Systems, Graphcore, and Groq are developing specialized AI chips that are designed to handle the most complex AI workloads, such as deep learning and natural language processing.
7. Companies are also focusing on developing AI chips that are more energy-efficient and sustainable, aiming to reduce the carbon footprint of AI computing.
8. There is also a growing trend towards edge computing, where AI chips are being developed for devices such as smartphones, wearables, and IoT devices, allowing for real-time AI processing and decision-making at the edge of the network.
9. Companies are also exploring the use of AI chips in various industries, such as healthcare, automotive, and finance, to improve efficiency and decision-making.
10. The development of AI chips is also being driven by the increasing demand for AI-powered devices and services, as well as the need for more efficient and sustainable AI computing. <</SYS>>

[INST] The major developments in the AI chip industry with respect to companies and their products include the following:
1. Companies such as NVIDIA, AMD, and Intel are leading the way in developing advanced AI chips for various applications, including machine learning, deep learning, and natural language processing.
2. NVIDIA's AI chips, such as the Tesla V100 and A100, are designed for high-performance computing and are used in data centers, supercomputers, and autonomous vehicles.
3. AMD's EPYC processors are optimized for AI workloads and are used in data centers and enterprise servers.
4. Intel's Xeon Scalable processors are designed for AI and machine learning applications and are used in data centers and cloud computing environments.
5. Companies such as Google, Microsoft, and Amazon are developing their own AI chips for their cloud services, aiming to provide faster and more efficient AI processing capabilities.
6. Startups such as Cerebras Systems, Graphcore, and Groq are developing specialized AI chips that are designed to handle the most complex AI workloads, such as deep learning and natural language processing.
7. Companies are also focusing on developing AI chips that are more energy-efficient and sustainable, aiming to reduce the carbon footprint of AI computing.
8. There is also a growing trend towards edge computing, where AI chips are being developed for devices such as smartphones, wearables, and IoT devices, allowing for real-time AI processing and decision-making at the edge of the network.
9. Companies are also exploring the use of AI chips in various industries, such as healthcare, automotive, and finance, to improve efficiency and decision-making.
10. The development of AI chips is also being driven by the increasing demand for AI-powered devices and services, as well as the need for more efficient and sustainable AI computing. <</SYS>>

[INST] The major developments in the AI chip industry with respect to companies and their products include the following:
1. Companies such as NVIDIA, AMD, and Intel are leading the way in developing advanced AI chips for various applications, including machine learning, deep learning, and natural language processing.
2. NVIDIA's AI chips, such as the Tesla V100 and A100, are designed for high-performance computing and are used in data centers, supercomputers, and autonomous vehicles.
3. AMD's EPYC processors are optimized for AI workloads and are used in data centers and enterprise servers.
4. Intel's Xeon Scalable processors are designed for AI and machine learning applications and are used in data centers and cloud computing environments.
5. Companies such as Google, Microsoft, and Amazon are developing their own AI chips for their cloud services, aiming to provide faster and more efficient AI processing capabilities.
6. Startups such as Cerebras Systems, Graphcore, and Groq are developing specialized AI chips that are designed to handle the most complex AI workloads, such as deep learning and natural language processing.
7. Companies are also focusing on developing AI chips that are more energy-efficient and sustainable, aiming to reduce the carbon footprint of AI computing.
8. There is also a growing trend towards edge computing, where AI chips are being developed for devices such as smartphones, wearables, and IoT devices, allowing for real-time AI processing and decision-making at the edge of the network.
9. Companies are also exploring the use of AI chips in various industries, such as healthcare, automotive, and finance, to improve efficiency and decision-making.
10. The development of AI chips is also being driven by the increasing demand for AI-powered devices and services, as well as the need for more efficient and sustainable AI computing. <</SYS>>

[INST] The major developments in the AI chip industry with respect to companies and their products include the following:
1. Companies such as NVIDIA, AMD, and Intel are leading the way in developing advanced AI chips for various applications, including machine learning, deep learning, and natural language processing.
2. NVIDIA's AI chips, such as the Tesla V100 and A100, are designed for high-performance computing and are used in data centers, supercomputers, and autonomous vehicles.
3. AMD's EPYC processors are optimized for AI workloads and are used in data centers and enterprise servers.
4. Intel's Xeon Scalable processors are designed for AI and machine learning applications and are used in data centers and cloud computing environments.
5. Companies such as Google, Microsoft, and Amazon are developing their own AI chips for their cloud services, aiming to provide faster and more efficient AI processing capabilities.
6. Startups such as Cerebras Systems, Graphcore, and Groq are developing specialized AI chips that are designed to handle the most complex AI workloads, such as deep learning and natural language processing.
7. Companies are also focusing on developing AI chips that are more energy-efficient and sustainable, aiming to reduce the carbon footprint of AI computing.
8. There is also a growing trend towards edge computing, where AI chips are being developed for devices such as smartphones, wearables, and IoT devices, allowing for real-time AI processing and decision-making at the edge of the network.
9. Companies are also exploring the use of AI chips in various industries, such as healthcare, automotive, and finance, to improve efficiency and decision-making.
10. The development of AI chips is also being driven by the increasing demand for AI-powered devices and services, as well as the need for more efficient and sustainable AI computing. <</SYS>>

[INST] The major developments in the AI chip industry with respect to companies and their products include the following:
1. Companies such as NVIDIA, AMD, and Intel are leading the way in developing advanced AI chips for various applications, including machine learning, deep learning, and natural language processing.
2. NVIDIA's AI chips, such as the Tesla V100 and A100, are designed for high-performance computing and are used in data centers, supercomputers, and autonomous vehicles.
3. AMD's EPYC processors are optimized for AI workloads and are used in data centers and enterprise servers.
4. Intel's Xeon Scalable processors are designed for AI and machine learning applications and are used in data centers and cloud computing environments.
5. Companies such as Google, Microsoft, and Amazon are developing their own AI chips for their cloud services, aiming to provide faster and more efficient AI processing capabilities.
6. Startups such as Cerebras Systems, Graphcore, and Groq are developing specialized AI chips that are designed to handle the most complex AI workloads, such as deep learning and natural language processing.
7. Companies are also focusing on developing AI chips that are more energy-efficient and sustainable, aiming to reduce the carbon footprint of AI computing.
8. There is also a growing trend towards edge computing, where AI chips are being developed for devices such as smartphones, wearables, and IoT devices, allowing for real-time AI processing and decision-making at the edge of the network.
9. Companies are also exploring the use of AI chips in various industries, such as healthcare, automotive, and finance, to improve efficiency and decision-making.
10. The development of AI chips is also being driven by the increasing demand for AI-powered devices and services, as well as the need for more efficient and sustainable AI computing. <</SYS>>

[INST] The major developments in the AI chip industry with respect to companies and their products include the following:
1. Companies such as NVIDIA, AMD, and Intel are leading the way in developing advanced AI chips for various applications, including machine learning, deep learning, and natural language processing.
2. NVIDIA's AI chips, such as the Tesla V100 and A100, are designed for high-performance computing and are used in data centers, supercomputers, and autonomous vehicles.
3. AMD's EPYC processors are optimized for AI workloads and are used in data centers and enterprise servers.
4. Intel's Xeon Scalable processors are designed for AI and machine learning applications and are used in data centers and cloud computing environments.
5. Companies such as Google, Microsoft, and Amazon are developing their own AI chips for their cloud services, aiming to provide faster and more efficient AI processing capabilities.
6. Startups such as Cerebras Systems, Graph
------------------------------
[NodeWithScore(node=TextNode(id_='c1da5e15-fe4a-4cea-98f4-352eb5010e44', embedding=None, metadata={'page_label': '9', 'file_name': 'Wafer_Wars_Deciphering_Latest_Restrictions_On_AI_And_Semiconductor_Manufacturing.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Wafer_Wars_Deciphering_Latest_Restrictions_On_AI_And_Semiconductor_Manufacturing.pdf', 'file_type': 'application/pdf', 'file_size': 1416467, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='c589a944-65d8-452e-a38b-79e08d8b4810', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'Wafer_Wars_Deciphering_Latest_Restrictions_On_AI_And_Semiconductor_Manufacturing.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Wafer_Wars_Deciphering_Latest_Restrictions_On_AI_And_Semiconductor_Manufacturing.pdf', 'file_type': 'application/pdf', 'file_size': 1416467, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='e02d51682096500b240aeef9dbc013d8ab52a3b81c15f558455530610624d996')}, text='End product markets tend to be global and interrelated, so it makes sense that the SemiCap \ninvestment cycle within Ch ina and outside of China has largely been in sync.  \nWe are nearly two years into the current semiconductor downturn (excluding AI -related chips \nof course) and while industry participants may feel that a rebound should be imminent, there \nhasn’t been any shar p uptick in actual investment so far, especially ex -China.', mimetype='text/plain', start_char_idx=2, end_char_idx=442, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7753642382960865), NodeWithScore(node=TextNode(id_='137454a6-0ed4-4ef1-ba0d-969d225252dc', embedding=None, metadata={'page_label': '10', 'file_name': 'Wafer_Wars_Deciphering_Latest_Restrictions_On_AI_And_Semiconductor_Manufacturing.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Wafer_Wars_Deciphering_Latest_Restrictions_On_AI_And_Semiconductor_Manufacturing.pdf', 'file_type': 'application/pdf', 'file_size': 1416467, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7ea3d90e-7d02-4b27-a473-53a816be93f6', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': 'Wafer_Wars_Deciphering_Latest_Restrictions_On_AI_And_Semiconductor_Manufacturing.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Wafer_Wars_Deciphering_Latest_Restrictions_On_AI_And_Semiconductor_Manufacturing.pdf', 'file_type': 'application/pdf', 'file_size': 1416467, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='745530b1a18a3b8a5cd22b2820d2d3640ccb685b0a45275067136ae5b253c606')}, text='That is – except for China, where SemiCap investment has rebounded dramatically and well \nahead of the rest of the world. Granted, local auto manufacturers and manufacturing in \ngeneral in China is  shifting away from using western suppliers of mature node and analog \nsemiconductors and is spending 30 years of capex in just a few to achieve that goal, but it \nseems possible that the near doubling of capex in just a few quarters is also due to something  \nelse. \n \nNext we will share a bit about China’s retaliation. This restriction on AI chips and \nmanufacturing equipment is not going untouched.  \nChina Retaliation  \nChina’s primary attack vector currently is continuing to block US acquisitions and also', mimetype='text/plain', start_char_idx=1, end_char_idx=712, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7721748262521548)]
------------------------------
Wafer_Wars_Deciphering_Latest_Restrictions_On_AI_And_Semiconductor_Manufacturing.pdf
------------------------------
End product markets tend to be global and interrelated, so it makes sense that the SemiCap 
investment cycle within Ch ina and outside of China has largely been in sync.  
We are nearly two years into the current semiconductor downturn (excluding AI -related chips 
of course) and while industry participants may feel that a rebound should be imminent, there 
hasn’t been any shar p uptick in actual investment so far, especially ex -China.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
在AI产能的背景下,GPU和HBM的供应链约束目前情况如何?
------------------------------
 The current status of the supply chain constraints of GPU and HBM in the context of AI capacity is that there is a significant bottleneck due to the high demand for GPUs and HBM, especially for AI applications. The supply chain has struggled to keep up with the demand, leading to shortages and delays in the delivery of GPUs and HBM. This has impacted the deployment of generative AI models and has caused significant challenges for companies, including OpenAI, which has been unable to deploy its multi-modal models due to GPU shortages. Chinese companies are also stockpiling GPUs in anticipation of further export controls. The highest-end Nvidia GPU, H100, will remain sold out until Q1 of next year, despite Nvidia's attempts to increase production drastically. The supply chain constraints are expected to continue in the near future, with estimates of supply increases by quarter for various companies in the industry. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='a13ce396-f897-4810-a50d-bde2b8022594', embedding=None, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='34eedcb2-02a5-41d7-845c-ed5c14652a13', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='c65010b2745f35c9749d3ea9b3fb8bc950a5d29e0967b66a738a2cab232da9d3')}, text="CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge \ndata center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced \naccelerators with Samsung on their version.  \n \nWhile 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of \nlogic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of \nmagnitude regarding interconnect density and is better suited to expanding on -chip cache with \ndie stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of \nCoWoS many years ago for combining multiple FPGA chiplets together.  \n \nWhile there are some other applications that use CoWoS like networking (and some of these are \nadopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, \nthe vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor \nsupply chain, where weakness in other major end markets means there is plenty of slack to \nabsorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing \ntechnologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are \nthe parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  \n \nJust recently in these 2 days I received a customer 's phone call requesting a big increase \non the backend capacity, especially in the CoWoS. We are still evaluating that.  \n \nC.C Wei, TSMC CEO", mimetype='text/plain', start_char_idx=0, end_char_idx=1528, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.8069140664145674), NodeWithScore(node=TextNode(id_='e50bdbfc-2bbf-406d-bf7a-2f821cffc9ac', embedding=None, metadata={'page_label': '1', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='23bf21c0-9291-40a3-b98b-b69e70b93e60', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='0cb41311840e28f60f27e4d52c5c440360dac562e1bdda64466955cf05e5c617')}, text='AI Capacity Constraints - CoWoS and HBM Supply Chain  \nGenerative AI is upon us  and it will change the world. Ever since ChatGPT took the world by \nstorm and captured our imaginations about the possibilities of AI, we are seeing all sorts of \ncompanies rushing to train AI models and deploy generative AI into internal workflows or \ncusto mer-facing applications. It’s not just big tech and startups, many non -tech fortune 5,000 \ncompanies are figuring out how to deploy LLM -based solutions.  \n \nOf course, this will require massive amounts of GPU compute. GPU sales are going up like a \nrocket and t he supply chain has struggled to keep up with the demand for GPUs. Companies are \nscrambling to get their hands on GPUs or cloud instances.  \n \nEven OpenAI can’t get enough GPUs and this is severely bottlenecking its near -term roadmap. \nOpenAI cannot deploy its  multi -modal models due to GPU shortages. OpenAI cannot deploy \nlonger sequence length models (8k vs 32k) due to GPU shortages.  \n \nMeanwhile, Chinese companies are not only investing to deploy their own LLMs but also \nstocking before US export controls tighten  even further. For example, Bytedance, the Chinese \ncompany behind Tik Tok is supposedly ordering over $1B worth of A800/H800s from Nvidia.  \n \nWhile there are many legitimate use cases for hundreds of thousands of GPUs dedicated to AI, \nthere are also many cas es where people are rushing to buy GPUs to try to build something they \naren’t sure has a legitimate market. In some cases, big tech firms are trying to catch up to \nOpenAI and Google, so as not to be left in the dust. There is a flood of VC money to startup s that \nhave no proven business use case. We know of over a dozen enterprises that are trying to train \ntheir own LLM on their own data. Lastly, this also applies to nation states with both Saudi Arabia \nand the UAE also attempting to acquire hundreds of mill ions of dollars of GPUs this year.  \n \nThe highest -end Nvidia GPU, H100, will remain sold out until Q1 of next year, despite Nvidia’s \nattempts to increase production drastically. Nvidia will be ramping to ship more 400,000 H100 \nGPUs per quarter.  \n \nToday we wil l detail the bottlenecks to production and how much downstream capacity is \nexpanding for Nvidia and their competitors. We will also share our estimates for the the supply \nincreases by quarter for Nvidia, Broadcom, Google, AMD, AMD Embedded (Xilinx), Amazon , \nMarvell, Microsoft, Alchip, Alibaba T -Head, ZTE Sanechips, Samsung, Micron, and SK Hynix.', mimetype='text/plain', start_char_idx=2, end_char_idx=2531, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.8064884925301777)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge 
data center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced 
accelerators with Samsung on their version.  
 
While 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of 
logic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of 
magnitude regarding interconnect density and is better suited to expanding on -chip cache with 
die stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of 
CoWoS many years ago for combining multiple FPGA chiplets together.  
 
While there are some other applications that use CoWoS like networking (and some of these are 
adopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, 
the vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor 
supply chain, where weakness in other major end markets means there is plenty of slack to 
absorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing 
technologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are 
the parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  
 
Just recently in these 2 days I received a customer 's phone call requesting a big increase 
on the backend capacity, especially in the CoWoS. We are still evaluating that.  
 
C.C Wei, TSMC CEO
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Nvidia如何解决其H100GPU的供应限制?
------------------------------
 Nvidia is aggressively double ordering to ensure supply. We have heard that Nvidia has ordered up to 150k CoWoS wafers for calendar 2024. This would imply over $100B revenue for H100s and Nvidia's next-generation GPU which would be absurd even in this AI supercycle. We understand that TSMC is unwilling to provide that much capacity to Nvidia alone, as the risk would be borne by TSMC if or when there is a digestion period and demand drops off. In response, we believe that Nvidia is investigating both Samsung and Amkor's 2.5D packaging technologies as alternatives to CoWoS. This helps Nvidia put pressure on TSMC which is part of Nvidia's foundry playbook: Nvidia (along with Qualcomm) was one of the few foundry companies that dual-sourced leading edge wafers from both Samsung and TSMC in an attempt to lessen dependence on TSMC and support a more competitive foundry landscape. However, Samsung's inferior process technology and abysmal yields that led Nvidia to switch back to TSMC. So by investigating Amkor and Samsung, this Nvidia is saying to TSMC: fulfil our demand or lose market share. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='b5d8abe0-cb97-4771-a0dc-36fb77f6f9bd', embedding=None, metadata={'page_label': '16', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='dbd9cbb6-f32e-4241-aeeb-e997f6255470', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '16', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='4a6f05c9bc685f86cd35c938cf28018fc9e9c617f0a5893bca8eebc4da6120fc')}, text='CoWoS Customers  \n \nFor subscribers, we will walk through demand for each CoWoS company: Nvidia, Broadcom, \nGoogle, AMD, AMD E mbedded (Xilinx), Amazon, Marvell, Microsoft, Alchip, Alibaba T -Head, and \nZTE Sanechips.  \nBased on our bottom -up CoWoS demand model, we believe that CoWoS demand will increase \nto over 11k wafers per month from the start of 2023 to the end of next year, main ly driven by AI \ndemand.  \n \nNvidia has been the largest CoWoS customer over the last few years and we expect this to \nremain the case as Nvidia continues ramp of H100 production. In the second half of 2024 we \nexpect another uptick as Nvidia’s next -generation G PU ramps which could be >2x the package \nsize of the H100.  \n \nA/H800 sales to China account 20% of Nvidia’s DC revenues so in the case export restrictions are \ntightened further Nvidia will lose this business, but given Nvidia is supply limited we believe \nNvid ia’s management when they say these wafers can be reallocated to A/H100s to fulfil non -\nChina demand. However, in the long term this may mean that Nvidia’s China demand would be', mimetype='text/plain', start_char_idx=0, end_char_idx=1095, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.8209636082693159), NodeWithScore(node=TextNode(id_='ba153128-3810-4ae1-881b-60b63460c245', embedding=None, metadata={'page_label': '17', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='77dc8190-edda-4978-a2f5-26fa11c9d0de', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '17', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='65146860101ae190631ee4b861e805e4796f6d1e4d79ddf83b662d135f431cc2')}, text="gone from the business and when the supply chain is balanced other regions woul d not be able \nto step up to replace this theoretical demand. There could be an airpocket for demand in Q2/3 \n2024.  \nWe are aware of reports that the U.S. Department of Commerce is considering further \ncontrols that may restrict exports of our A800 and our H80 0 products to China. However, \ngiven the strength of our demand for our products worldwide, we do not anticipate that such \nadditional restrictions, if adopted, would have an immediate material impact on our financial \nresults. We do not anticipate any immedi ate material impact on our financial results.  \n \nOver the long term, restrictions prohibiting the sale of our datacenter GPUs to China, if \nimplemented, will result in a permanent loss of opportunities for the U.S. industry to compete \nand lead in one of the world's largest markets and the impact on our future business and \nfinancial results is there.  \n \nColette Kress, Nvidia CFO  \nNvidia is also aggressively double ordering to ensure supply. We have heard that Nvidia has \nordered up to 150k CoWoS wafers for cal endar 2024. This would imply over $100B revenue \nfor H100s and Nvidia’s next -generation GPU which would be absurd even in this AI \nsupercycle. We understand that TSMC is unwilling to provide that much capacity to Nvidia \nalone, as the risk would be borne by T SMC if or when there is a digestion period and demand \ndrops off.  \n \nIn response, we believe that Nvidia is investigating both Samsung and Amkor’s 2.5D packaging \ntechnologies as alternatives to CoWoS.  This helps Nvidia put pressure on TSMC which is part \nof N vidia’s foundry playbook: Nvidia (along with Qualcomm) was one of the few foundry \ncompanies that dual -sourced leading edge wafers from both Samsung and TSMC in an \nattempt to lessen dependence on TSMC and support a more competitive foundry landscape. \nHoweve r, Samsung’s inferior process technology and abysmal yields that led Nvidia to switch \nback to TSMC. So by investigating Amkor and Samsung, this Nvidia is saying to TSMC: fulfil our \ndemand or lose market share.", mimetype='text/plain', start_char_idx=0, end_char_idx=2118, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.8113728890134477)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
CoWoS Customers  
 
For subscribers, we will walk through demand for each CoWoS company: Nvidia, Broadcom, 
Google, AMD, AMD E mbedded (Xilinx), Amazon, Marvell, Microsoft, Alchip, Alibaba T -Head, and 
ZTE Sanechips.  
Based on our bottom -up CoWoS demand model, we believe that CoWoS demand will increase 
to over 11k wafers per month from the start of 2023 to the end of next year, main ly driven by AI 
demand.  
 
Nvidia has been the largest CoWoS customer over the last few years and we expect this to 
remain the case as Nvidia continues ramp of H100 production. In the second half of 2024 we 
expect another uptick as Nvidia’s next -generation G PU ramps which could be >2x the package 
size of the H100.  
 
A/H800 sales to China account 20% of Nvidia’s DC revenues so in the case export restrictions are 
tightened further Nvidia will lose this business, but given Nvidia is supply limited we believe 
Nvid ia’s management when they say these wafers can be reallocated to A/H100s to fulfil non -
China demand. However, in the long term this may mean that Nvidia’s China demand would be
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
HBM3生产的当前状态是什么,哪些公司在这一领域处于领先地位?
------------------------------
 HBM3 is currently in production by SK Hynix and Samsung. SK Hynix is leading the market with their HBM3 production, while Samsung is expected to ship HBM3 in the second half of 2023. Micron is still behind in this area, as they struggle to manufacture top bin HBM2E at high volume. They have made bold statements about their HBM3E roadmap, but it remains to be seen if they will be able to catch up with the other companies. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='b8cb875b-e089-4107-b819-c35982a310e6', embedding=None, metadata={'page_label': '9', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2d5429d6-da72-4789-850f-2c3d851e07a4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='20f31daa322c8ed85ce9814b85d783d0b89577e94348fce0c8d7139410131c3d')}, text="Micron is the furthest behind. Micron was more heavily invested in Hybrid Memory Cube (HMC) \ntechnology. This was a competing technology to HBM with a very similar concept that \ndeveloped around the same time. However, the ecosystem around HMC was closed, making it \ndifficult for IP to be developed around HMC. Furthermore, there were some technical \ndeficiencies. Adoption for HBM was much higher so HBM won out to become the industry \nstandard for 3D stac ked DRAM.  \n \nIt was only in 2018 that Micron started to pivot away from HMC and invest into HBM roadmap. \nThis is why Micron is the furthest behind. They are still stuck on HBM2E (which SK Hynix started \nmass producing in the middle of 2020) and cannot even ma nufacture top bin HBM2E \nsuccessfully.   \n \nIn their most recent earnings call Micron made some bold statements about their HBM \nroadmap: they believe they will go from laggard to leader with their HBM3E in 2024. HBM3E is \nexpected to start to ship in Q3/Q4 for  Nvidia’s next generation GPU.  \n \nOur ramp of HBM3, actually the sort of the next generation of HBM3, which is a much higher \nlevel of performance, bandwidth and lower power than what is in production in HBM3 today \nin the industry. That product, our industry -leading product will be ramping in volume starting \nCQ1 of 2024, and will be meaningful in revenue for fiscal year '24 and then substantially larger \nin 2025, even from those 2024 levels. And we will -- we are targeting a very robust share in \nHBM higher than  unnatural supply share for DRAM in the industry.  \n \nSumit Sadana, Micron Chief Business Officer  \nThe statement that they will have higher market share in HBM versus their general DRMA \nmarketshare is very bold. Given they still struggle to manufacture top bin  HBM2E at high \nvolume, we find it hard to believe Micron’s claim that they will ship leading edge HBM3 in \nearly 2024 and even be the first to HBM3E. It feels to us as though Micron is trying to change \nthe narrative away from being an AI loser despite drama tically lower memory content per \nNvidia GPU server versus Intel/AMD CPU server.", mimetype='text/plain', start_char_idx=0, end_char_idx=2108, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7707755167855318), NodeWithScore(node=TextNode(id_='2510fab3-008a-44e1-93c0-5bae390d0e86', embedding=None, metadata={'page_label': '7', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0f3cd730-2427-409d-9c66-9b5e986eb5d0', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='f8736f35651a82947a0579107b403a8cfad0092a93db5d69e6da6de7fee0d24d')}, text='Samsung is next behind Hynix and expects to ship HBM3 in the second half of 2023. We believe \nthey are designed for both Nvidia and AMD GPUs. They currently have a big deficit in volume to \nSK Hynix, but they are hot on the trails a nd are investing hugely to catch up on market share. \nSamsung is investing to catch up and become number 1 in HBM market share just like they are \nwith standard memory. We hear they are cutting favorable deals with some of the accelerator \nfirms to try to cap ture more share.  \n \nThey have shown off their 12 -layer HBM as well as future Hybrid Bonded HBM. One interesting \naspect of Samsung’s HBM -4 roadmap is that they want to make the logic/periphery on an in -\nhouse FinFET node. This shows the potential advantage the y have of having logic and DRAM \nfoundry in house.', mimetype='text/plain', start_char_idx=2, end_char_idx=810, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7468522946538044)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
Micron is the furthest behind. Micron was more heavily invested in Hybrid Memory Cube (HMC) 
technology. This was a competing technology to HBM with a very similar concept that 
developed around the same time. However, the ecosystem around HMC was closed, making it 
difficult for IP to be developed around HMC. Furthermore, there were some technical 
deficiencies. Adoption for HBM was much higher so HBM won out to become the industry 
standard for 3D stac ked DRAM.  
 
It was only in 2018 that Micron started to pivot away from HMC and invest into HBM roadmap. 
This is why Micron is the furthest behind. They are still stuck on HBM2E (which SK Hynix started 
mass producing in the middle of 2020) and cannot even ma nufacture top bin HBM2E 
successfully.   
 
In their most recent earnings call Micron made some bold statements about their HBM 
roadmap: they believe they will go from laggard to leader with their HBM3E in 2024. HBM3E is 
expected to start to ship in Q3/Q4 for  Nvidia’s next generation GPU.  
 
Our ramp of HBM3, actually the sort of the next generation of HBM3, which is a much higher 
level of performance, bandwidth and lower power than what is in production in HBM3 today 
in the industry. That product, our industry -leading product will be ramping in volume starting 
CQ1 of 2024, and will be meaningful in revenue for fiscal year '24 and then substantially larger 
in 2025, even from those 2024 levels. And we will -- we are targeting a very robust share in 
HBM higher than  unnatural supply share for DRAM in the industry.  
 
Sumit Sadana, Micron Chief Business Officer  
The statement that they will have higher market share in HBM versus their general DRMA 
marketshare is very bold. Given they still struggle to manufacture top bin  HBM2E at high 
volume, we find it hard to believe Micron’s claim that they will ship leading edge HBM3 in 
early 2024 and even be the first to HBM3E. It feels to us as though Micron is trying to change 
the narrative away from being an AI loser despite drama tically lower memory content per 
Nvidia GPU server versus Intel/AMD CPU server.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
HBM生产面临的挑战和创新是什么?
------------------------------
 HBM production is facing challenges such as technical deficiencies, closed ecosystem, and lower adoption rate. Innovations include the development of HBM3E, which is expected to start shipping in Q3/Q4 for Nvidia's next generation GPU. Micron is also targeting a higher market share in HBM than their general DRAM marketshare. However, given their struggles with manufacturing top bin HBM2E at high volume, it is hard to believe Micron's claim that they will ship leading-edge HBM3 in early 2024 and even be the first to HBM3E. Additionally, HBM production is also facing challenges due to thermals and cost, making it unsuitable for stacking dies directly on top of logic. SoIC is better suited to expanding on-chip cache with die stacking, as seen with AMD's 3D V-Cache solution. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='b8cb875b-e089-4107-b819-c35982a310e6', embedding=None, metadata={'page_label': '9', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2d5429d6-da72-4789-850f-2c3d851e07a4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='20f31daa322c8ed85ce9814b85d783d0b89577e94348fce0c8d7139410131c3d')}, text="Micron is the furthest behind. Micron was more heavily invested in Hybrid Memory Cube (HMC) \ntechnology. This was a competing technology to HBM with a very similar concept that \ndeveloped around the same time. However, the ecosystem around HMC was closed, making it \ndifficult for IP to be developed around HMC. Furthermore, there were some technical \ndeficiencies. Adoption for HBM was much higher so HBM won out to become the industry \nstandard for 3D stac ked DRAM.  \n \nIt was only in 2018 that Micron started to pivot away from HMC and invest into HBM roadmap. \nThis is why Micron is the furthest behind. They are still stuck on HBM2E (which SK Hynix started \nmass producing in the middle of 2020) and cannot even ma nufacture top bin HBM2E \nsuccessfully.   \n \nIn their most recent earnings call Micron made some bold statements about their HBM \nroadmap: they believe they will go from laggard to leader with their HBM3E in 2024. HBM3E is \nexpected to start to ship in Q3/Q4 for  Nvidia’s next generation GPU.  \n \nOur ramp of HBM3, actually the sort of the next generation of HBM3, which is a much higher \nlevel of performance, bandwidth and lower power than what is in production in HBM3 today \nin the industry. That product, our industry -leading product will be ramping in volume starting \nCQ1 of 2024, and will be meaningful in revenue for fiscal year '24 and then substantially larger \nin 2025, even from those 2024 levels. And we will -- we are targeting a very robust share in \nHBM higher than  unnatural supply share for DRAM in the industry.  \n \nSumit Sadana, Micron Chief Business Officer  \nThe statement that they will have higher market share in HBM versus their general DRMA \nmarketshare is very bold. Given they still struggle to manufacture top bin  HBM2E at high \nvolume, we find it hard to believe Micron’s claim that they will ship leading edge HBM3 in \nearly 2024 and even be the first to HBM3E. It feels to us as though Micron is trying to change \nthe narrative away from being an AI loser despite drama tically lower memory content per \nNvidia GPU server versus Intel/AMD CPU server.", mimetype='text/plain', start_char_idx=0, end_char_idx=2108, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7338189410572594), NodeWithScore(node=TextNode(id_='a13ce396-f897-4810-a50d-bde2b8022594', embedding=None, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='34eedcb2-02a5-41d7-845c-ed5c14652a13', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='c65010b2745f35c9749d3ea9b3fb8bc950a5d29e0967b66a738a2cab232da9d3')}, text="CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge \ndata center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced \naccelerators with Samsung on their version.  \n \nWhile 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of \nlogic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of \nmagnitude regarding interconnect density and is better suited to expanding on -chip cache with \ndie stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of \nCoWoS many years ago for combining multiple FPGA chiplets together.  \n \nWhile there are some other applications that use CoWoS like networking (and some of these are \nadopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, \nthe vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor \nsupply chain, where weakness in other major end markets means there is plenty of slack to \nabsorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing \ntechnologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are \nthe parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  \n \nJust recently in these 2 days I received a customer 's phone call requesting a big increase \non the backend capacity, especially in the CoWoS. We are still evaluating that.  \n \nC.C Wei, TSMC CEO", mimetype='text/plain', start_char_idx=0, end_char_idx=1528, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.722312930009476)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
Micron is the furthest behind. Micron was more heavily invested in Hybrid Memory Cube (HMC) 
technology. This was a competing technology to HBM with a very similar concept that 
developed around the same time. However, the ecosystem around HMC was closed, making it 
difficult for IP to be developed around HMC. Furthermore, there were some technical 
deficiencies. Adoption for HBM was much higher so HBM won out to become the industry 
standard for 3D stac ked DRAM.  
 
It was only in 2018 that Micron started to pivot away from HMC and invest into HBM roadmap. 
This is why Micron is the furthest behind. They are still stuck on HBM2E (which SK Hynix started 
mass producing in the middle of 2020) and cannot even ma nufacture top bin HBM2E 
successfully.   
 
In their most recent earnings call Micron made some bold statements about their HBM 
roadmap: they believe they will go from laggard to leader with their HBM3E in 2024. HBM3E is 
expected to start to ship in Q3/Q4 for  Nvidia’s next generation GPU.  
 
Our ramp of HBM3, actually the sort of the next generation of HBM3, which is a much higher 
level of performance, bandwidth and lower power than what is in production in HBM3 today 
in the industry. That product, our industry -leading product will be ramping in volume starting 
CQ1 of 2024, and will be meaningful in revenue for fiscal year '24 and then substantially larger 
in 2025, even from those 2024 levels. And we will -- we are targeting a very robust share in 
HBM higher than  unnatural supply share for DRAM in the industry.  
 
Sumit Sadana, Micron Chief Business Officer  
The statement that they will have higher market share in HBM versus their general DRMA 
marketshare is very bold. Given they still struggle to manufacture top bin  HBM2E at high 
volume, we find it hard to believe Micron’s claim that they will ship leading edge HBM3 in 
early 2024 and even be the first to HBM3E. It feels to us as though Micron is trying to change 
the narrative away from being an AI loser despite drama tically lower memory content per 
Nvidia GPU server versus Intel/AMD CPU server.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
CoWoS技术的现状如何,它与HBM生产的关系如何?
------------------------------
 CoWoS (Chip on Wafer on Substrate) is a "2.5D" packaging technology from TSMC where multiple active silicon dies (the usual configuration is logic and HBM stacks) are integrated on a passive silicon interposer. The interposer acts as a communication layer for the active die on top. The interposer and active silicon are then attached to a packaging substrate which contains the I/O to place on the system PCB. HBM and CoWoS are complementary. The high pad count and short trace length requirements of HBM necessitate 2.5D advanced packaging technologies like CoWoS to enable such dense, short connections that can't be done on a PCB or even a package substrate. CoWoS is the mainstream packaging technology that offers the highest interconnection density and largest package size with reasonable costs. As almost all HBM systems are currently packaged on CoWoS by TSMC, it is the main bottleneck in HBM production. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='a13ce396-f897-4810-a50d-bde2b8022594', embedding=None, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='34eedcb2-02a5-41d7-845c-ed5c14652a13', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='c65010b2745f35c9749d3ea9b3fb8bc950a5d29e0967b66a738a2cab232da9d3')}, text="CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge \ndata center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced \naccelerators with Samsung on their version.  \n \nWhile 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of \nlogic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of \nmagnitude regarding interconnect density and is better suited to expanding on -chip cache with \ndie stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of \nCoWoS many years ago for combining multiple FPGA chiplets together.  \n \nWhile there are some other applications that use CoWoS like networking (and some of these are \nadopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, \nthe vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor \nsupply chain, where weakness in other major end markets means there is plenty of slack to \nabsorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing \ntechnologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are \nthe parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  \n \nJust recently in these 2 days I received a customer 's phone call requesting a big increase \non the backend capacity, especially in the CoWoS. We are still evaluating that.  \n \nC.C Wei, TSMC CEO", mimetype='text/plain', start_char_idx=0, end_char_idx=1528, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7726493278854001), NodeWithScore(node=TextNode(id_='e60e1541-f822-4ee0-8462-dc611a29abc6', embedding=None, metadata={'page_label': '10', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5089cde0-8b63-42fa-8e13-6842b5c2b065', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='0948ade6d1140d9c898b11b4c45fe7c5a55fc845cf7e06aa99f1fe04412ca5dd')}, text="All our channel checks see SK Hynix remaining strongest at new -generation technologies and \nSamsung trying very hard to catch back up with huge supply increases, a bold roadma p, and \ncutting deals.  \n \nThe Real Bottleneck – CoWoS  \nThe next bottleneck is CoWoS capacity. CoWoS (Chip on Wafer on Substrate) is a “2.5D” \npackaging technology from TSMC where multiple active silicon dies (the usual configuration is \nlogic and HBM stacks) are  integrated on a passive silicon interposer. The interposer acts as a \ncommunication layer for the active die on top. The interposer and active silicon are then \nattached to a packaging substrate which contains the I/O to place on the system PCB.  \n \nHBM and C oWoS are complementary. The high pad count and short trace length requirements \nof HBM necessitate 2.5D advanced packaging technologies like CoWoS to enable such dense, \nshort connections that can't be done on a PCB or even a package substrate. CoWoS is the \nmainstream packaging technology that offers the highest interconnection density and largest \npackage size with reasonable costs. As almost all HBM systems are currently packaged on", mimetype='text/plain', start_char_idx=2, end_char_idx=1144, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7465719412963758)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge 
data center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced 
accelerators with Samsung on their version.  
 
While 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of 
logic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of 
magnitude regarding interconnect density and is better suited to expanding on -chip cache with 
die stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of 
CoWoS many years ago for combining multiple FPGA chiplets together.  
 
While there are some other applications that use CoWoS like networking (and some of these are 
adopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, 
the vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor 
supply chain, where weakness in other major end markets means there is plenty of slack to 
absorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing 
technologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are 
the parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  
 
Just recently in these 2 days I received a customer 's phone call requesting a big increase 
on the backend capacity, especially in the CoWoS. We are still evaluating that.  
 
C.C Wei, TSMC CEO
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
CoWoS-R、CoWoS-S和CoWoS-L中介层技术之间有何区别?
------------------------------
 CoWoS-R, CoWoS-S and CoWoS-L are three different brokering technologies used in advanced packaging. CoWoS-R is a 2.5D technology that uses through silicon vias (TSVs) to connect multiple dies together, while CoWoS-S is a 3D technology that stacks multiple dies vertically. CoWoS-L is a 2.5D technology that uses through silicon vias (TSVs) to connect multiple dies together, but with a higher interconnect density than CoWoS-R. The main differences between these technologies are the number of layers and the interconnect density. CoWoS-R has a lower interconnect density and is used for lower-end applications, while CoWoS-S has a higher interconnect density and is used for higher-end applications. CoWoS-L has a higher interconnect density than CoWoS-R and is used for applications that require a higher level of performance and efficiency. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='a13ce396-f897-4810-a50d-bde2b8022594', embedding=None, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='34eedcb2-02a5-41d7-845c-ed5c14652a13', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='c65010b2745f35c9749d3ea9b3fb8bc950a5d29e0967b66a738a2cab232da9d3')}, text="CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge \ndata center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced \naccelerators with Samsung on their version.  \n \nWhile 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of \nlogic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of \nmagnitude regarding interconnect density and is better suited to expanding on -chip cache with \ndie stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of \nCoWoS many years ago for combining multiple FPGA chiplets together.  \n \nWhile there are some other applications that use CoWoS like networking (and some of these are \nadopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, \nthe vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor \nsupply chain, where weakness in other major end markets means there is plenty of slack to \nabsorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing \ntechnologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are \nthe parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  \n \nJust recently in these 2 days I received a customer 's phone call requesting a big increase \non the backend capacity, especially in the CoWoS. We are still evaluating that.  \n \nC.C Wei, TSMC CEO", mimetype='text/plain', start_char_idx=0, end_char_idx=1528, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7517464328407024), NodeWithScore(node=TextNode(id_='db639869-d4a9-4a45-8713-52e3e61a15b1', embedding=None, metadata={'page_label': '20', 'file_name': 'Is_Intel_Back_Foundry&Product_Resurgence_Measured.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Is_Intel_Back_Foundry&Product_Resurgence_Measured.pdf', 'file_type': 'application/pdf', 'file_size': 2083058, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ccdacc90-7ddc-455c-8b5e-2a3b3d5d407d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '20', 'file_name': 'Is_Intel_Back_Foundry&Product_Resurgence_Measured.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Is_Intel_Back_Foundry&Product_Resurgence_Measured.pdf', 'file_type': 'application/pdf', 'file_size': 2083058, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='27b8dc319f486204cc37590b566404a91c65287b1ccaf39554fa16b4f52fff6e')}, text='still consistent with IFS still being in its early days. First, Satya Nadella from Microsoft made an \nappearance to announce that Microsoft will fab a custom design on 18A. This attracted a lot \nof attention and grabbed headlines  but as a silicon player, Microsoft is still very small  (but \nwith the potential to be very big).  \nMicrosoft, relative to other hyperscalers, is in the early days of their silicon journey and are \nexperimenting and trying things out. They don’t have their own silicon deployed in production \nworkloads. They have little to lose and can afford to experiment.  They also get some positive \npublicity and political brownie points by being high profile about supporting “Made in \nAmerica” which is an extra bonus too. We suspect they are manufacturing and packaging \ntheir next generation Cobalt CPU at Intel.  \nBroadcom and Mediatek also have similar profiles. They are very large merchant silicon \nproviders with huge product portfolios spanning different geometries across both the leading \nedge and trailing edge. These players are used to working with multiple foun dries and for \nthem experimenting with IFS doesn’t come with much downside. If a not significant design \ndoesn’t tape out properly, it’s not going to be the end of the world.  \nThey are not like an Nvidia that is extremely concentrated in the leading edge with a handful \nof designs that make up the majority of their revenues: risking an  already aggressive \nroadmap  on a still unproven supplier would have disastrous consequences if there is an \nexecution hiccup, therefore sticking with the proven and ultra -reliable TSMC remains the best \nstrategy for the flagship datacenter GPUs. Though that’s not to say Nvidia isn’t in terested in \nIFS at all, we understand that they are the IFS packaging customer for AI.  \nWe believe Intel along with Amkor are the second and third source for 2.5D packaging in \naddition to TSMC. We believe Intel will ramp from 2024 and provide advanced packaging for \nB100 as well. We have the full details around the capacity ramp in terms of th e number of \nwafers from Amkor and Intel and so much more in our  AI accelerator model  offering for \nclients.  \nIt would be extremely logical for Nvidia to try making a PC or Datacenter CPU on 18A in late \n2025 or 2026, but we have not seen anything specific on this ourselves.  \nWe believe Google is also investigating Intel’s foundry, likely for a CPU in the 2026 time frame.  \nNoteworthy is that while Qualcomm initially talked up a big game with Intel on 18A, they \neventually faded away. We believe that is because 18A is not competitive in mobile. As such, \nIntel can’t really hope to nab mobile customers until 14A.  \nWe believe Intel can get multiple AI chip packaging wins in the medium term, but we don’t \nsee Intel getting any AI chip designs until 2027. The large customers first need to validate \nIntel with CPUs before they trust them with strategically more critical p roducts.', mimetype='text/plain', start_char_idx=0, end_char_idx=2970, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7255390895772521)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge 
data center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced 
accelerators with Samsung on their version.  
 
While 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of 
logic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of 
magnitude regarding interconnect density and is better suited to expanding on -chip cache with 
die stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of 
CoWoS many years ago for combining multiple FPGA chiplets together.  
 
While there are some other applications that use CoWoS like networking (and some of these are 
adopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, 
the vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor 
supply chain, where weakness in other major end markets means there is plenty of slack to 
absorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing 
technologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are 
the parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  
 
Just recently in these 2 days I received a customer 's phone call requesting a big increase 
on the backend capacity, especially in the CoWoS. We are still evaluating that.  
 
C.C Wei, TSMC CEO
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
CoWoS需求预计增长多少,对Nvidia业务有何影响?
------------------------------
 Based on our bottom-up CoWoS demand model, we believe that CoWoS demand will increase to over 11k wafers per month from the start of 2023 to the end of next year, mainly driven by AI demand. This increase in demand will have a significant impact on Nvidia operations, as the company is the largest CoWoS customer and is expected to continue ramping up production of H100 and its next-generation GPU. However, if export restrictions are tightened further, Nvidia may lose its China market, which could lead to an airpocket for demand in Q2/3 2024. Additionally, Nvidia is aggressively double ordering to ensure supply, which could result in over $100B revenue for H100s and Nvidia's next-generation GPU. However, TSMC is unwilling to provide that much capacity to Nvidia alone, as the risk would be borne by TSMC if or when there is a digestion period and demand drops off. In response, Nvidia is investigating alternative packaging technologies such as Samsung and Amkor's 2.5D packaging technologies to put pressure on TSMC. This could lead to a more competitive foundry landscape and lessen dependence on TSMC. However, Samsung's inferior process technology and abysmal yields led Nvidia to switch back to TSMC in the past, so the outcome of this investigation remains uncertain. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='b5d8abe0-cb97-4771-a0dc-36fb77f6f9bd', embedding=None, metadata={'page_label': '16', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='dbd9cbb6-f32e-4241-aeeb-e997f6255470', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '16', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='4a6f05c9bc685f86cd35c938cf28018fc9e9c617f0a5893bca8eebc4da6120fc')}, text='CoWoS Customers  \n \nFor subscribers, we will walk through demand for each CoWoS company: Nvidia, Broadcom, \nGoogle, AMD, AMD E mbedded (Xilinx), Amazon, Marvell, Microsoft, Alchip, Alibaba T -Head, and \nZTE Sanechips.  \nBased on our bottom -up CoWoS demand model, we believe that CoWoS demand will increase \nto over 11k wafers per month from the start of 2023 to the end of next year, main ly driven by AI \ndemand.  \n \nNvidia has been the largest CoWoS customer over the last few years and we expect this to \nremain the case as Nvidia continues ramp of H100 production. In the second half of 2024 we \nexpect another uptick as Nvidia’s next -generation G PU ramps which could be >2x the package \nsize of the H100.  \n \nA/H800 sales to China account 20% of Nvidia’s DC revenues so in the case export restrictions are \ntightened further Nvidia will lose this business, but given Nvidia is supply limited we believe \nNvid ia’s management when they say these wafers can be reallocated to A/H100s to fulfil non -\nChina demand. However, in the long term this may mean that Nvidia’s China demand would be', mimetype='text/plain', start_char_idx=0, end_char_idx=1095, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7769690920829945), NodeWithScore(node=TextNode(id_='ba153128-3810-4ae1-881b-60b63460c245', embedding=None, metadata={'page_label': '17', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='77dc8190-edda-4978-a2f5-26fa11c9d0de', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '17', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='65146860101ae190631ee4b861e805e4796f6d1e4d79ddf83b662d135f431cc2')}, text="gone from the business and when the supply chain is balanced other regions woul d not be able \nto step up to replace this theoretical demand. There could be an airpocket for demand in Q2/3 \n2024.  \nWe are aware of reports that the U.S. Department of Commerce is considering further \ncontrols that may restrict exports of our A800 and our H80 0 products to China. However, \ngiven the strength of our demand for our products worldwide, we do not anticipate that such \nadditional restrictions, if adopted, would have an immediate material impact on our financial \nresults. We do not anticipate any immedi ate material impact on our financial results.  \n \nOver the long term, restrictions prohibiting the sale of our datacenter GPUs to China, if \nimplemented, will result in a permanent loss of opportunities for the U.S. industry to compete \nand lead in one of the world's largest markets and the impact on our future business and \nfinancial results is there.  \n \nColette Kress, Nvidia CFO  \nNvidia is also aggressively double ordering to ensure supply. We have heard that Nvidia has \nordered up to 150k CoWoS wafers for cal endar 2024. This would imply over $100B revenue \nfor H100s and Nvidia’s next -generation GPU which would be absurd even in this AI \nsupercycle. We understand that TSMC is unwilling to provide that much capacity to Nvidia \nalone, as the risk would be borne by T SMC if or when there is a digestion period and demand \ndrops off.  \n \nIn response, we believe that Nvidia is investigating both Samsung and Amkor’s 2.5D packaging \ntechnologies as alternatives to CoWoS.  This helps Nvidia put pressure on TSMC which is part \nof N vidia’s foundry playbook: Nvidia (along with Qualcomm) was one of the few foundry \ncompanies that dual -sourced leading edge wafers from both Samsung and TSMC in an \nattempt to lessen dependence on TSMC and support a more competitive foundry landscape. \nHoweve r, Samsung’s inferior process technology and abysmal yields that led Nvidia to switch \nback to TSMC. So by investigating Amkor and Samsung, this Nvidia is saying to TSMC: fulfil our \ndemand or lose market share.", mimetype='text/plain', start_char_idx=0, end_char_idx=2118, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7756333833137795)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
CoWoS Customers  
 
For subscribers, we will walk through demand for each CoWoS company: Nvidia, Broadcom, 
Google, AMD, AMD E mbedded (Xilinx), Amazon, Marvell, Microsoft, Alchip, Alibaba T -Head, and 
ZTE Sanechips.  
Based on our bottom -up CoWoS demand model, we believe that CoWoS demand will increase 
to over 11k wafers per month from the start of 2023 to the end of next year, main ly driven by AI 
demand.  
 
Nvidia has been the largest CoWoS customer over the last few years and we expect this to 
remain the case as Nvidia continues ramp of H100 production. In the second half of 2024 we 
expect another uptick as Nvidia’s next -generation G PU ramps which could be >2x the package 
size of the H100.  
 
A/H800 sales to China account 20% of Nvidia’s DC revenues so in the case export restrictions are 
tightened further Nvidia will lose this business, but given Nvidia is supply limited we believe 
Nvid ia’s management when they say these wafers can be reallocated to A/H100s to fulfil non -
China demand. However, in the long term this may mean that Nvidia’s China demand would be
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Nvidia针对台积电的潜在产能问题采取了哪些行动?
------------------------------
 Nvidia is not the only company producing accelerators, with Google ramping custom accelerator production rapidly. Going forward, Meta and Amazon will also ramp their in-house accelerators. This reality is not lost on the top global hyperscalers – who are rapidly ramping up datacenter construction and colocation leasing. AWS literally bought a 1000MW nuclear-powered datacenter campus for $650M USD. Though only the very first building with 48MW of capacity is likely to be online in the near term, this provides a valuable pipeline of datacenter capacity for AWS without being held up waiting for power generation or grid transmission capacity. We think a campus of such mammoth proportions will take many years to fully ramp to the promised 1,000 MW of Critical IT Power. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='1943b77b-6557-43d1-962e-4455529431a0', embedding=None, metadata={'page_label': '9', 'file_name': 'Intel_GenAI_For_Yield.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Intel_GenAI_For_Yield.pdf', 'file_type': 'application/pdf', 'file_size': 3859066, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='77c565ca-b613-436d-bff6-9b1a81ec3094', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '9', 'file_name': 'Intel_GenAI_For_Yield.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Intel_GenAI_For_Yield.pdf', 'file_type': 'application/pdf', 'file_size': 3859066, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='ad43521e146796b9247c55d9ee915acc3cb7e227e37c5fd19dfccf6fe7aaa3b1')}, text='*One caveat in this comparison is that Intel’s paper is focused on the integration of CFET + \nbackside power delivery and contacts rather than just scaling. In 2021, Intel demonstrated a \nCFET with gate pitch 55nm & gate length 19nm', mimetype='text/plain', start_char_idx=2, end_char_idx=234, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7737132168580467), NodeWithScore(node=TextNode(id_='c57eeabb-2696-45db-ab71-2216559cd3b8', embedding=None, metadata={'page_label': '13', 'file_name': 'AI_Datacenter_Energy_Dilemma.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Datacenter_Energy_Dilemma.pdf', 'file_type': 'application/pdf', 'file_size': 6306082, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5dc06377-21f2-49e7-9b3d-2344f4cfe75b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '13', 'file_name': 'AI_Datacenter_Energy_Dilemma.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Datacenter_Energy_Dilemma.pdf', 'file_type': 'application/pdf', 'file_size': 6306082, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='7a13238b195b800de51e76766a01fa712ce2e7826a9e51eca39ba8e490dcd78c')}, text='Microsoft has the largest pipeline of datacenter buildouts pre -AI era (see January 2023 data \nbelow), and  our data shows its skyrocketed since . They have been gobbling any and all \ncolocation space they can as well aggressively increasing their datacenter buildouts . AI \nlaggers like Amazon  have made press releases about nuclear powered datacenters totaling \n1,000MW, but to be clear they are lagging materially on real near term buildouts as they \nwere  the last of the hyperscalers to wake up to AI . Google, and Microsoft/OpenAI both have \nplans for larger than Gigawatt class training clusters in the works.  \n \nStructure Research  \nFrom a supply perspective, sell side consensus estimates of 3M+ GPUs shipped by Nvidia in \ncalendar year 2024 would correspond to over 4,200 MW of datacenter needs, nearly 10% of \ncurrent global datacenter capacity, just for one year’s GPU shipments. The con sensus \nestimates for Nvidia’s shipments are also very wrong of course. Ignoring that, AI is only going \nto grow in subsequent years, and Nvidia’s GPUs are  slated to get even more power hungry , \nwith 1,000W, 1,200W, and 1,500W GPUs on the roadmap. Nvidia is not the only company \nproducing accelerators, with  Google ramping custom accelerator production rapidly . Going \nforward, Meta and Amazon will also ramp their in house accelerators.  \nThis reality is not lost on the top global hyperscalers – who are rapidly ramping up datacenter \nconstruction and colocation leasing. AWS literally bought a 1000MW  nuclear -powered \ndatacenter campus  for $650M USD. Though only the very first building with 48MW of capacity \nis likely to be online in the near term, this provides a valuable pipeline of datacenter capacity \nfor AWS without being held up waiting for power generation or grid transmission capac ity. \nWe think a campus of such mammoth proportions will take many years to fully ramp to the \npromised 1,000 MW of Critical IT Power.', mimetype='text/plain', start_char_idx=0, end_char_idx=1942, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7725929288661119)]
------------------------------
Intel_GenAI_For_Yield.pdf
------------------------------
*One caveat in this comparison is that Intel’s paper is focused on the integration of CFET + 
backside power delivery and contacts rather than just scaling. In 2021, Intel demonstrated a 
CFET with gate pitch 55nm & gate length 19nm
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
CoWoS技术的主要需求驱动因素是什么?具体客户有哪些?
------------------------------
 The main demand drivers for CoWoS technologies are AI and advanced data center GPUs. The specific clients include Nvidia, Broadcom, Google, AMD, AMD Embedded (Xilinx), Amazon, Marvell, Microsoft, Alchip, Alibaba T-Head, and ZTE Sanechips. These companies are major users of CoWoS technologies for their AI and data center applications. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='b5d8abe0-cb97-4771-a0dc-36fb77f6f9bd', embedding=None, metadata={'page_label': '16', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='dbd9cbb6-f32e-4241-aeeb-e997f6255470', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '16', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='4a6f05c9bc685f86cd35c938cf28018fc9e9c617f0a5893bca8eebc4da6120fc')}, text='CoWoS Customers  \n \nFor subscribers, we will walk through demand for each CoWoS company: Nvidia, Broadcom, \nGoogle, AMD, AMD E mbedded (Xilinx), Amazon, Marvell, Microsoft, Alchip, Alibaba T -Head, and \nZTE Sanechips.  \nBased on our bottom -up CoWoS demand model, we believe that CoWoS demand will increase \nto over 11k wafers per month from the start of 2023 to the end of next year, main ly driven by AI \ndemand.  \n \nNvidia has been the largest CoWoS customer over the last few years and we expect this to \nremain the case as Nvidia continues ramp of H100 production. In the second half of 2024 we \nexpect another uptick as Nvidia’s next -generation G PU ramps which could be >2x the package \nsize of the H100.  \n \nA/H800 sales to China account 20% of Nvidia’s DC revenues so in the case export restrictions are \ntightened further Nvidia will lose this business, but given Nvidia is supply limited we believe \nNvid ia’s management when they say these wafers can be reallocated to A/H100s to fulfil non -\nChina demand. However, in the long term this may mean that Nvidia’s China demand would be', mimetype='text/plain', start_char_idx=0, end_char_idx=1095, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7620741244052621), NodeWithScore(node=TextNode(id_='a13ce396-f897-4810-a50d-bde2b8022594', embedding=None, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='34eedcb2-02a5-41d7-845c-ed5c14652a13', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='c65010b2745f35c9749d3ea9b3fb8bc950a5d29e0967b66a738a2cab232da9d3')}, text="CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge \ndata center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced \naccelerators with Samsung on their version.  \n \nWhile 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of \nlogic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of \nmagnitude regarding interconnect density and is better suited to expanding on -chip cache with \ndie stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of \nCoWoS many years ago for combining multiple FPGA chiplets together.  \n \nWhile there are some other applications that use CoWoS like networking (and some of these are \nadopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, \nthe vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor \nsupply chain, where weakness in other major end markets means there is plenty of slack to \nabsorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing \ntechnologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are \nthe parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  \n \nJust recently in these 2 days I received a customer 's phone call requesting a big increase \non the backend capacity, especially in the CoWoS. We are still evaluating that.  \n \nC.C Wei, TSMC CEO", mimetype='text/plain', start_char_idx=0, end_char_idx=1528, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7550662924276426)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
CoWoS Customers  
 
For subscribers, we will walk through demand for each CoWoS company: Nvidia, Broadcom, 
Google, AMD, AMD E mbedded (Xilinx), Amazon, Marvell, Microsoft, Alchip, Alibaba T -Head, and 
ZTE Sanechips.  
Based on our bottom -up CoWoS demand model, we believe that CoWoS demand will increase 
to over 11k wafers per month from the start of 2023 to the end of next year, main ly driven by AI 
demand.  
 
Nvidia has been the largest CoWoS customer over the last few years and we expect this to 
remain the case as Nvidia continues ramp of H100 production. In the second half of 2024 we 
expect another uptick as Nvidia’s next -generation G PU ramps which could be >2x the package 
size of the H100.  
 
A/H800 sales to China account 20% of Nvidia’s DC revenues so in the case export restrictions are 
tightened further Nvidia will lose this business, but given Nvidia is supply limited we believe 
Nvid ia’s management when they say these wafers can be reallocated to A/H100s to fulfil non -
China demand. However, in the long term this may mean that Nvidia’s China demand would be
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
CoWoS技术面临的挑战和潜在发展是什么?
------------------------------
 CoWOS technology is a 3D packaging technology that enables stacking dies directly on top of logic. However, it does not make sense for HBM due to thermals and cost. SoIC is better suited to expanding on-chip cache with die stacking, as seen with AMD's 3D V-Cache solution. The vast majority of CoWOS demand comes from AI, and the technology is already majority AI-facing, so all slack was already absorbed in Q1. With GPU demand exploding, these are the parts of the supply chain that just cannot keep up and are bottlenecking GPU supply. Just recently, a customer requested a big increase on the backend capacity, especially in the CoWOS. We are still evaluating that. [/INST] <</SYS>>
------------------------------
[NodeWithScore(node=TextNode(id_='a13ce396-f897-4810-a50d-bde2b8022594', embedding=None, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='34eedcb2-02a5-41d7-845c-ed5c14652a13', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='c65010b2745f35c9749d3ea9b3fb8bc950a5d29e0967b66a738a2cab232da9d3')}, text="CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge \ndata center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced \naccelerators with Samsung on their version.  \n \nWhile 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of \nlogic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of \nmagnitude regarding interconnect density and is better suited to expanding on -chip cache with \ndie stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of \nCoWoS many years ago for combining multiple FPGA chiplets together.  \n \nWhile there are some other applications that use CoWoS like networking (and some of these are \nadopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, \nthe vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor \nsupply chain, where weakness in other major end markets means there is plenty of slack to \nabsorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing \ntechnologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are \nthe parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  \n \nJust recently in these 2 days I received a customer 's phone call requesting a big increase \non the backend capacity, especially in the CoWoS. We are still evaluating that.  \n \nC.C Wei, TSMC CEO", mimetype='text/plain', start_char_idx=0, end_char_idx=1528, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7396265794149653), NodeWithScore(node=TextNode(id_='01d7c6a7-e574-4056-8142-1eb4613fd593', embedding=None, metadata={'page_label': '4', 'file_name': 'Microsoft_Swallows_OpenAI’s_Core_Team.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Microsoft_Swallows_OpenAI’s_Core_Team.pdf', 'file_type': 'application/pdf', 'file_size': 366440, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2bc21bee-1ffd-4a52-9b5a-7c2653cb601a', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '4', 'file_name': 'Microsoft_Swallows_OpenAI’s_Core_Team.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Microsoft_Swallows_OpenAI’s_Core_Team.pdf', 'file_type': 'application/pdf', 'file_size': 366440, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='fe4ea8c6aa5f75ab653033f599e0a0416b0fee1710566ec0628e060612b665a9')}, text='tenets the view  that AI is a potential major risk  to humanity and takes seriously a scenario \nwhere an artificial general intelligence (AGI) entity exterminates humankind, most likely by \naccident through a myopic maximization of an instruction by human handlers that results in \nunintended consequences at an  extinction level scale.  \n \nShear has certainly  opined in the past  on his view on the proper pace of development in AI \ntechnologies, arguing for a significant deceleration to allow time to “poke your way forward, \ncarefully”.  \n \nIn a series of exchanges with proponents of an even sharper slowdown or downright halt to', mimetype='text/plain', start_char_idx=0, end_char_idx=635, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.722843423572747)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge 
data center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced 
accelerators with Samsung on their version.  
 
While 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of 
logic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of 
magnitude regarding interconnect density and is better suited to expanding on -chip cache with 
die stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of 
CoWoS many years ago for combining multiple FPGA chiplets together.  
 
While there are some other applications that use CoWoS like networking (and some of these are 
adopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, 
the vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor 
supply chain, where weakness in other major end markets means there is plenty of slack to 
absorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing 
technologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are 
the parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  
 
Just recently in these 2 days I received a customer 's phone call requesting a big increase 
on the backend capacity, especially in the CoWoS. We are still evaluating that.  
 
C.C Wei, TSMC CEO
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
AMD预计何时增加MI300的出货量?
------------------------------
 AMD expects to increase the volume of MI300 in the near future, especially after the software optimization is improved and more software partners are on board. The company is also hopeful that MI300 can break the $1B quarterly revenue watermark next year for Q3, but it is not certain. In order to achieve this, AMD needs to secure huge orders from hyperscalers, which will shift a significant portion of purchases from H100 to MI300. If this happens, AMD will be able to increase the volume of MI300 significantly. However, if the hyperscalers do not place big orders, the company may not achieve the desired revenue goals. Overall, AMD is optimistic about the potential of MI300 and is working towards increasing its volume in the market. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='ebe7232b-c219-4407-9d78-0d36494020e3', embedding=None, metadata={'page_label': '5', 'file_name': 'AMD_MI300_Performance.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AMD_MI300_Performance.pdf', 'file_type': 'application/pdf', 'file_size': 1217906, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='2e0d79c4-d3d3-40eb-861b-13654fab184e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '5', 'file_name': 'AMD_MI300_Performance.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AMD_MI300_Performance.pdf', 'file_type': 'application/pdf', 'file_size': 1217906, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='7333a885372ef5d571827dd4bf9b9bb2f10ee5245f6d6733a868491855e54316')}, text='In a handful of months we’d bet AMD’s performance keeps growing versus the H100. While \nH200 is a reset, MI300 should still win overall with more software optimization.  \nThe more important thing is OEMs and clouds. Microsoft of course is supporting. Oracle will \nalso be supporting as we noted in the pand they also announced customers such as \nDatabricks (MosaicML).  \nBut they aren’t the only ones.  \nMeta is also partnering with AMD and will be purchasing volume as we noted above.  \nDell is speaking out as a big partner of AMD here, which makes sense given they are the \nweakest partner for HGX H100s of the big 3 OEMs. HPE due to their ownership of Cray and \nthe MI300A was always going to be big partner, but so is Lenovo. All the ODMs ar e there too.  \n \nBesides Microsoft and Oracle Cloud, AMD has also racked up many of the misfits.', mimetype='text/plain', start_char_idx=4, end_char_idx=847, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7991032352615463), NodeWithScore(node=TextNode(id_='ed977d75-c8f8-435b-9179-4c2695a01b10', embedding=None, metadata={'page_label': '19', 'file_name': 'AMD_MI300_Taming_The_Hype.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AMD_MI300_Taming_The_Hype.pdf', 'file_type': 'application/pdf', 'file_size': 1688038, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='03d98614-9f58-4ea5-845e-98cb0402159d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '19', 'file_name': 'AMD_MI300_Taming_The_Hype.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AMD_MI300_Taming_The_Hype.pdf', 'file_type': 'application/pdf', 'file_size': 1688038, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='2d2b7bf34b8606e7843d65654313323cb0b7e4eb29fe911900b8e41ae8016bc3')}, text="If evaluations which are ongoing, work out, then we could see much larger orders from \nMicrosoft, Meta, or various other hyperscalers. We believe that Microsoft is considering MI300 \nfor inference, but not for the nex t generation OpenAI training cluster.  \n \nOne the markets we believe is very promising for AMD is China, as they can derate their IO and \nglide under rules very easily. The China hyperscalers are very good at their own software stacks, \nespecially Baidu, so tha t is a great market AMD is trying to penetrate. It helps that leading AI \nresearchers there get paid an order magnitude than the AI folks in the Bay Area. We doubt AMD \nwill talk much about it the engagements with Alibaba or Tencent publicly though.  \n \nAMD wil l ship less than $1B worth MI300 this year. Meanwhile, Nvidia will be ramping to \nbeyond $10B quarterly datacenter GPU revenue in the near term. The aspirational goal for AMD \nis 100,000 GPU a quarter next year. That will be tough to hit.  \n \nAMD is highly limi ted by using such a high bin of HBM. Furthermore, Nvidia is going to be \ncompeting to order all of the top bin for the H100 refresh with higher speed memory later this \nyear. We believe AMD has a big cap on MI300 ramp between supply difficulties with the \nadvanced packaging, top bin HBM, and Nvidia / Broadcom trying to get as much of the CoWoS \nsupply out there.  \n \nFurthermore, Nvidia’s going to announce their next generation GPU next year and start shipping \nit in the 2nd half. AMD only has a period of about 6 mo nths to dominate mindshare and \nbenchmarks to win business before everyone’s mind shifts to Nvidia’s next GPU which is should \nsmash both H100 and MI300 in performance. Furthermore, Nvidia’s next gen GPU is likely far \nmore optimized for transformers. H100 ne tlist was completed in ~2020, before the importance \nof GPT -3 was even recognized broadly.  \n \nMI300 is an amazing product, the traction it can garner will mean a lot for future products, but \nAMD won’t be able to make anywhere near Nvidia margins, and the volu mes they ship won’t be \nanywhere near Nvidia. Nvidia will remain 90%+ share in accelerators. We remain hopeful MI300 \ncan break the $1B quarterly revenue watermark next year for Q3, but it is not certain. In fact it \nwould be very tough without huge orders th at amount to a big shift from H100 from at least 1 \nhyperscaler. If none of the hyperscalers place big orders, we doubt they even do $1B MI300 \nrevenue for the whole year. We think if someone does buy a lot, they will switch a huge portion \nof purchases over or they won't buy much at all. Not much room for middle ground. AMD won’t \nbe competitive with Nvidia again in late 2024, so all eyes are on how the software develops, \nCUDA moat deteriorates, Nvidia -next, and MI400 timelines.", mimetype='text/plain', start_char_idx=2, end_char_idx=2796, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7944863156577445)]
------------------------------
AMD_MI300_Performance.pdf
------------------------------
In a handful of months we’d bet AMD’s performance keeps growing versus the H100. While 
H200 is a reset, MI300 should still win overall with more software optimization.  
The more important thing is OEMs and clouds. Microsoft of course is supporting. Oracle will 
also be supporting as we noted in the pand they also announced customers such as 
Databricks (MosaicML).  
But they aren’t the only ones.  
Meta is also partnering with AMD and will be purchasing volume as we noted above.  
Dell is speaking out as a big partner of AMD here, which makes sense given they are the 
weakest partner for HGX H100s of the big 3 OEMs. HPE due to their ownership of Cray and 
the MI300A was always going to be big partner, but so is Lenovo. All the ODMs ar e there too.  
 
Besides Microsoft and Oracle Cloud, AMD has also racked up many of the misfits.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Xilinx对CoWoS产能的需求情况如何?
------------------------------
 Xilinx is the first user of CoWoS for combining multiple FPGA chiplets together. [/INST]
------------------------------
[NodeWithScore(node=TextNode(id_='a13ce396-f897-4810-a50d-bde2b8022594', embedding=None, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='34eedcb2-02a5-41d7-845c-ed5c14652a13', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '11', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='c65010b2745f35c9749d3ea9b3fb8bc950a5d29e0967b66a738a2cab232da9d3')}, text="CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge \ndata center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced \naccelerators with Samsung on their version.  \n \nWhile 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of \nlogic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of \nmagnitude regarding interconnect density and is better suited to expanding on -chip cache with \ndie stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of \nCoWoS many years ago for combining multiple FPGA chiplets together.  \n \nWhile there are some other applications that use CoWoS like networking (and some of these are \nadopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, \nthe vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor \nsupply chain, where weakness in other major end markets means there is plenty of slack to \nabsorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing \ntechnologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are \nthe parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  \n \nJust recently in these 2 days I received a customer 's phone call requesting a big increase \non the backend capacity, especially in the CoWoS. We are still evaluating that.  \n \nC.C Wei, TSMC CEO", mimetype='text/plain', start_char_idx=0, end_char_idx=1528, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7578992307581333), NodeWithScore(node=TextNode(id_='b5d8abe0-cb97-4771-a0dc-36fb77f6f9bd', embedding=None, metadata={'page_label': '16', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='dbd9cbb6-f32e-4241-aeeb-e997f6255470', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '16', 'file_name': 'AI_Capacity_Constraints.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/AI_Capacity_Constraints.pdf', 'file_type': 'application/pdf', 'file_size': 2118839, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='4a6f05c9bc685f86cd35c938cf28018fc9e9c617f0a5893bca8eebc4da6120fc')}, text='CoWoS Customers  \n \nFor subscribers, we will walk through demand for each CoWoS company: Nvidia, Broadcom, \nGoogle, AMD, AMD E mbedded (Xilinx), Amazon, Marvell, Microsoft, Alchip, Alibaba T -Head, and \nZTE Sanechips.  \nBased on our bottom -up CoWoS demand model, we believe that CoWoS demand will increase \nto over 11k wafers per month from the start of 2023 to the end of next year, main ly driven by AI \ndemand.  \n \nNvidia has been the largest CoWoS customer over the last few years and we expect this to \nremain the case as Nvidia continues ramp of H100 production. In the second half of 2024 we \nexpect another uptick as Nvidia’s next -generation G PU ramps which could be >2x the package \nsize of the H100.  \n \nA/H800 sales to China account 20% of Nvidia’s DC revenues so in the case export restrictions are \ntightened further Nvidia will lose this business, but given Nvidia is supply limited we believe \nNvid ia’s management when they say these wafers can be reallocated to A/H100s to fulfil non -\nChina demand. However, in the long term this may mean that Nvidia’s China demand would be', mimetype='text/plain', start_char_idx=0, end_char_idx=1095, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.7430994922446896)]
------------------------------
AI_Capacity_Constraints.pdf
------------------------------
CoWoS, and all advanced AI accelerators use HBM, the corollary is that virtu ally all leading -edge 
data center GPUs are packaged on CoWoS by TSMC. Baidu does have some advanced 
accelerators with Samsung on their version.  
 
While 3D packaging technologies such as TSMC's SoIC enable stacking dies directly on top of 
logic, it does not make sense for HBM due to thermals and cost. SoIC sits on a different order of 
magnitude regarding interconnect density and is better suited to expanding on -chip cache with 
die stacking, as seen with AMD's 3D V -Cache solution. AMD’s Xilinx was also the fir st users of 
CoWoS many years ago for combining multiple FPGA chiplets together.  
 
While there are some other applications that use CoWoS like networking (and some of these are 
adopted for networking GPU clusters like Broadcom’s Jericho3 -AI), supercomputing , and FPGAs, 
the vast majority of CoWoS demand comes from AI. Unlike other parts of the semiconductor 
supply chain, where weakness in other major end markets means there is plenty of slack to 
absorb the enormous pickup in GPU demand, CoWoS and HBM are alre ady majority AI -facing 
technologies, so all slack was already absorbed in Q1. With GPU demand exploding, these are 
the parts of the supply chain that just cannot keep up and are bottlenecking GPU supply.  
 
Just recently in these 2 days I received a customer 's phone call requesting a big increase 
on the backend capacity, especially in the CoWoS. We are still evaluating that.  
 
C.C Wei, TSMC CEO
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
中兴通讯芯片部门Sanechips的重点是什么?
------------------------------
 The focus of the communication chip department, Sanechips, is on designing and manufacturing chips for various communication applications such as networking, wireless, and satellite communication. They specialize in developing high-performance, energy-efficient, and cost-effective solutions for the communication industry. Their products are used in a wide range of applications, including data centers, enterprise networks, mobile devices, and consumer electronics. They offer a broad range of communication chips, including processors, transceivers, modems, and RF components. Their expertise in communication technology enables them to provide innovative solutions that meet the evolving needs of the industry. [/INST] <</SYS>>
------------------------------
[NodeWithScore(node=TextNode(id_='40efe87a-ec21-4076-ba6c-faa26d22660f', embedding=None, metadata={'page_label': '14', 'file_name': 'Broadcom’s_Google_TPU_Revenue_Explosion.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Broadcom’s_Google_TPU_Revenue_Explosion.pdf', 'file_type': 'application/pdf', 'file_size': 1767574, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a3b87e4e-57f8-49bb-8260-bddf26461b2b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '14', 'file_name': 'Broadcom’s_Google_TPU_Revenue_Explosion.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Broadcom’s_Google_TPU_Revenue_Explosion.pdf', 'file_type': 'application/pdf', 'file_size': 1767574, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='73b11d58e005f9cc51478f30ed67c6831468f8b3f8e9a3025a7c665db03ccf4c')}, text='Carrier networks carry internet traffic long distances, provide connectivity \nto data centers as well as the backbone for consumer internet access, and \nBroadcom sells mainly routing silicon such as the Jericho and Qumran \nfamily.  Routers are typically used to connect different networks to one \nanother – particularly across long distances.  \n \nA router typically consists of a CPU for overall management, line cards that \ninterface via the physical layer to connect to other networks (eg via fiber \noptic cables), and a switching fabric which consists mostly of just a series of \nSerializer/Deserializer s (SerDes – a device that encodes data for \ntransmission at extremely high speeds) to connect the line cards to one \nanother.  \n \nBroadcom’s Jericho family of chips provide silicon for both the Line Card, \nwhere most of the compute intelligence is needed, as well as the switching \nfabric. Broadcom sells silicon to major router companies which then sell the \ncomplete router device to tel ecom companies or hyperscalers. They also \nsell directly to some hyperscalers.', mimetype='text/plain', start_char_idx=2, end_char_idx=1077, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.6901996122765126), NodeWithScore(node=TextNode(id_='ce180d30-253a-431a-90c1-8aa07ae43466', embedding=None, metadata={'page_label': '10', 'file_name': 'Broadcom’s_Google_TPU_Revenue_Explosion.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Broadcom’s_Google_TPU_Revenue_Explosion.pdf', 'file_type': 'application/pdf', 'file_size': 1767574, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='5d6cb13f-851d-4fea-9b39-4e6da210d8b4', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '10', 'file_name': 'Broadcom’s_Google_TPU_Revenue_Explosion.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Broadcom’s_Google_TPU_Revenue_Explosion.pdf', 'file_type': 'application/pdf', 'file_size': 1767574, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='437f1229e767bd37eeb3efb94c50a150759a512aebbb194f1bf39706729acac3')}, text='Networking, Broadband and Avago’s 2015 Purchase of Broadcom  \nOne of Hock Tan’s biggest moves was Avago’s 2015 acquisition of \nBroadcom. Prior to the acquisition, Avago’s networking products were an \norder of magnitude smaller than Broadcom’s. Broadcom was known as the \nbluest of blue chips among semiconductor companies , with a strong \nbusiness franchise in broadband and the leading networking business, \nwhere it sold best in class ethernet switching chips predecessors to the \ncurrent Tomahawk family, businesses that today form the bulk of present -\nday Broadcom’s networking business.  \n \nOther than the Jericho family of router silicon, present -day Broadcom’s \nnetworking division includes Ethernet routing and switching silicon for use \nin data centers, Fiber optic communication components, and a custom \nsilicon business that designs AI chips s uch as Google’s TPU and Meta’s MTIA \nas well a host of other customized routing/switching chips.  \n \nMost products that form the current Broadband segment were added \nthrough the Broadcom acquisition. These products include Digital \nSubscriber Line (DSL), Passive Optical Network (PON), DOCSIS Cable Modem', mimetype='text/plain', start_char_idx=2, end_char_idx=1165, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.6844228511235046)]
------------------------------
Broadcom’s_Google_TPU_Revenue_Explosion.pdf
------------------------------
Carrier networks carry internet traffic long distances, provide connectivity 
to data centers as well as the backbone for consumer internet access, and 
Broadcom sells mainly routing silicon such as the Jericho and Qumran 
family.  Routers are typically used to connect different networks to one 
another – particularly across long distances.  
 
A router typically consists of a CPU for overall management, line cards that 
interface via the physical layer to connect to other networks (eg via fiber 
optic cables), and a switching fabric which consists mostly of just a series of 
Serializer/Deserializer s (SerDes – a device that encodes data for 
transmission at extremely high speeds) to connect the line cards to one 
another.  
 
Broadcom’s Jericho family of chips provide silicon for both the Line Card, 
where most of the compute intelligence is needed, as well as the switching 
fabric. Broadcom sells silicon to major router companies which then sell the 
complete router device to tel ecom companies or hyperscalers. They also 
sell directly to some hyperscalers.
######################################
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
台湾ASIC公司GUC的现状如何?他们的CoWoS产能如何?
------------------------------
 <</SYS>>

[INST] Taiwan's ASIC GUC is currently facing challenges due to the US-China trade war and the US sanctions on Chinese companies. The US has imposed restrictions on the export of advanced semiconductor technology to China, which has affected the operations of Taiwanese semiconductor companies, including GUC. GUC is a leading provider of ASIC design services and has been actively seeking to diversify its customer base and reduce its reliance on Chinese customers.

As for CoWoS, Taiwan's Chip-on-Wafer-Substrate (CoWoS) technology is considered to be one of the most advanced in the world. CoWoS technology allows for the integration of multiple chips onto a single wafer, which can significantly reduce the cost of manufacturing and improve the performance of the chips. Taiwan's semiconductor industry has been investing heavily in CoWoS technology, and it is expected to play a crucial role in the future of the semiconductor industry. However, the US-China trade war has also affected the development of CoWoS technology in Taiwan, as some Taiwanese companies have been forced to shift their manufacturing operations to China due to the US sanctions.
------------------------------
[NodeWithScore(node=TextNode(id_='d7623656-4df0-4b4b-a7e0-8d0251a2be44', embedding=None, metadata={'page_label': '7', 'file_name': 'Google_Gemini_Eats_The_World.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Google_Gemini_Eats_The_World.pdf', 'file_type': 'application/pdf', 'file_size': 316586, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='e460bb45-03c2-43aa-9f67-1495e1eb5900', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '7', 'file_name': 'Google_Gemini_Eats_The_World.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/Google_Gemini_Eats_The_World.pdf', 'file_type': 'application/pdf', 'file_size': 316586, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='ff5eb0dd408f265f7cec6d9bc51fcb4751f2a347c1082d8b17549a8ed461e6e9')}, text='Chairman of Andes, the highest volume RISC -V based firm in the world with over 1 billion \ncores shipped a year.  Register here  if you can come!', mimetype='text/plain', start_char_idx=0, end_char_idx=145, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.6925882103829958), NodeWithScore(node=TextNode(id_='dd589efe-7db5-40f5-83c5-4466dfc75594', embedding=None, metadata={'page_label': '13', 'file_name': 'China_AI_Semiconductors_Rise_US_Sanctions_Have_Failed.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/China_AI_Semiconductors_Rise_US_Sanctions_Have_Failed.pdf', 'file_type': 'application/pdf', 'file_size': 706459, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f92f5f54-984b-4daf-9260-5e13e9083da3', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '13', 'file_name': 'China_AI_Semiconductors_Rise_US_Sanctions_Have_Failed.pdf', 'file_path': '/home/intern/LYZ/大模型微调/archinfo/China_AI_Semiconductors_Rise_US_Sanctions_Have_Failed.pdf', 'file_type': 'application/pdf', 'file_size': 706459, 'creation_date': '2024-07-19', 'last_modified_date': '2024-07-19'}, hash='a158d8ebbd0b4026066cb03e70301a1ca0d3cdb47ab18bb8796114b14ef62f6d')}, text='17. Limit chips that have >1000TOPS of performance.  \n18. Limit the  licensing of 200G SerDes . \n19. Limit EDA tools.  \n20. Limit Joint Ventures and inbound investments.  \nHalf measures will not work, but a full -scale assault will make it so the cost of replicating the \nsemiconductor supply chain domestically is neigh on impossible. While we aren’t advocating \nfor any of these specifically, it is clear the west can still stop China’s rise if decisive action is \ntaken.  \nThanks to Jordan and Doug for help pulling together this piece. Check \nout FabricatedKnoweledge  and ChinaTalk , they’re great. Also tune in later this week \nto ChinaTalk  + Transistor Radio where we’ll continue the discussion around SMIC and Huawei, \nfurther explain the arsenal of options, reflect on Semicon Taiwan, global impact of Chinese \nEVs, and more.', mimetype='text/plain', start_char_idx=0, end_char_idx=835, text_template='{metadata_str}\n\n{content}', metadata_template='{key}: {value}', metadata_seperator='\n'), score=0.6917170784428622)]
------------------------------
Google_Gemini_Eats_The_World.pdf
------------------------------
Chairman of Andes, the highest volume RISC -V based firm in the world with over 1 billion 
cores shipped a year.  Register here  if you can come!
######################################
