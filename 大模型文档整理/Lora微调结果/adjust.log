nohup: ignoring input
2024-07-11 07:47:54,490 - modelscope - INFO - PyTorch version 2.3.1 Found.
2024-07-11 07:47:54,490 - modelscope - INFO - Loading ast index from /root/.cache/modelscope/ast_indexer
2024-07-11 07:47:54,557 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 43bd9f5c993c4b3a3137b8e00a2e15d8 and a total number of 976 components indexed
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.10s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.07s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.05s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.01s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.03s/it]

Map:   0%|          | 0/591 [00:00<?, ? examples/s]
Map:   6%|▌         | 36/591 [00:00<00:01, 311.27 examples/s]
Map:  15%|█▍        | 87/591 [00:00<00:01, 417.96 examples/s]
Map:  25%|██▌       | 150/591 [00:00<00:00, 505.65 examples/s]
Map:  36%|███▌      | 213/591 [00:00<00:00, 548.58 examples/s]
Map:  47%|████▋     | 276/591 [00:00<00:00, 572.83 examples/s]
Map:  59%|█████▉    | 350/591 [00:00<00:00, 626.78 examples/s]
Map:  73%|███████▎  | 430/591 [00:00<00:00, 679.03 examples/s]
Map:  86%|████████▋ | 511/591 [00:00<00:00, 716.19 examples/s]
Map: 100%|██████████| 591/591 [00:00<00:00, 629.94 examples/s]
Map: 100%|██████████| 591/591 [00:00<00:00, 598.67 examples/s]
Detected kernel version 4.15.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.

  0%|          | 0/1850 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  0%|          | 1/1850 [00:05<2:49:06,  5.49s/it]
                                                  
{'loss': 4.248, 'grad_norm': 4.46875, 'learning_rate': 9.994594594594596e-05, 'epoch': 0.03}

  0%|          | 1/1850 [00:05<2:49:06,  5.49s/it]
  0%|          | 2/1850 [00:09<2:26:28,  4.76s/it]
                                                  
{'loss': 4.0703, 'grad_norm': 4.75, 'learning_rate': 9.98918918918919e-05, 'epoch': 0.05}

  0%|          | 2/1850 [00:09<2:26:28,  4.76s/it]
  0%|          | 3/1850 [00:13<2:13:16,  4.33s/it]
                                                  
{'loss': 3.7649, 'grad_norm': 5.25, 'learning_rate': 9.983783783783784e-05, 'epoch': 0.08}

  0%|          | 3/1850 [00:13<2:13:16,  4.33s/it]
  0%|          | 4/1850 [00:17<2:04:38,  4.05s/it]
                                                  
{'loss': 3.1079, 'grad_norm': 3.4375, 'learning_rate': 9.97837837837838e-05, 'epoch': 0.11}

  0%|          | 4/1850 [00:17<2:04:38,  4.05s/it]
  0%|          | 5/1850 [00:21<2:08:13,  4.17s/it]
                                                  
{'loss': 2.8836, 'grad_norm': 3.40625, 'learning_rate': 9.972972972972973e-05, 'epoch': 0.14}

  0%|          | 5/1850 [00:21<2:08:13,  4.17s/it]
  0%|          | 6/1850 [00:25<2:08:56,  4.20s/it]
                                                  
{'loss': 2.8116, 'grad_norm': 2.640625, 'learning_rate': 9.967567567567569e-05, 'epoch': 0.16}

  0%|          | 6/1850 [00:25<2:08:56,  4.20s/it]
  0%|          | 7/1850 [00:29<2:08:21,  4.18s/it]
                                                  
{'loss': 2.2276, 'grad_norm': 1.6953125, 'learning_rate': 9.962162162162164e-05, 'epoch': 0.19}

  0%|          | 7/1850 [00:29<2:08:21,  4.18s/it]
  0%|          | 8/1850 [00:33<2:01:48,  3.97s/it]
                                                  
{'loss': 2.3895, 'grad_norm': 1.7421875, 'learning_rate': 9.956756756756757e-05, 'epoch': 0.22}

  0%|          | 8/1850 [00:33<2:01:48,  3.97s/it]
  0%|          | 9/1850 [00:37<2:01:29,  3.96s/it]
                                                  
{'loss': 2.2205, 'grad_norm': 1.5, 'learning_rate': 9.951351351351353e-05, 'epoch': 0.24}

  0%|          | 9/1850 [00:37<2:01:29,  3.96s/it]
  1%|          | 10/1850 [00:40<1:55:15,  3.76s/it]
                                                   
{'loss': 2.0539, 'grad_norm': 1.5078125, 'learning_rate': 9.945945945945948e-05, 'epoch': 0.27}

  1%|          | 10/1850 [00:40<1:55:15,  3.76s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  1%|          | 11/1850 [00:44<1:58:22,  3.86s/it]
                                                   
{'loss': 2.1602, 'grad_norm': 1.3515625, 'learning_rate': 9.94054054054054e-05, 'epoch': 0.3}

  1%|          | 11/1850 [00:44<1:58:22,  3.86s/it]
  1%|          | 12/1850 [00:48<1:55:40,  3.78s/it]
                                                   
{'loss': 1.862, 'grad_norm': 1.640625, 'learning_rate': 9.935135135135135e-05, 'epoch': 0.32}

  1%|          | 12/1850 [00:48<1:55:40,  3.78s/it]
  1%|          | 13/1850 [00:51<1:52:06,  3.66s/it]
                                                   
{'loss': 1.8461, 'grad_norm': 1.3515625, 'learning_rate': 9.92972972972973e-05, 'epoch': 0.35}

  1%|          | 13/1850 [00:51<1:52:06,  3.66s/it]
  1%|          | 14/1850 [00:55<1:52:34,  3.68s/it]
                                                   
{'loss': 1.6602, 'grad_norm': 1.234375, 'learning_rate': 9.924324324324324e-05, 'epoch': 0.38}

  1%|          | 14/1850 [00:55<1:52:34,  3.68s/it]
  1%|          | 15/1850 [00:58<1:49:31,  3.58s/it]
                                                   
{'loss': 1.7237, 'grad_norm': 2.171875, 'learning_rate': 9.918918918918919e-05, 'epoch': 0.41}

  1%|          | 15/1850 [00:58<1:49:31,  3.58s/it]
  1%|          | 16/1850 [01:02<1:48:23,  3.55s/it]
                                                   
{'loss': 1.855, 'grad_norm': 1.2578125, 'learning_rate': 9.913513513513514e-05, 'epoch': 0.43}

  1%|          | 16/1850 [01:02<1:48:23,  3.55s/it]
  1%|          | 17/1850 [01:05<1:49:22,  3.58s/it]
                                                   
{'loss': 1.7471, 'grad_norm': 1.546875, 'learning_rate': 9.908108108108108e-05, 'epoch': 0.46}

  1%|          | 17/1850 [01:05<1:49:22,  3.58s/it]
  1%|          | 18/1850 [01:09<1:52:20,  3.68s/it]
                                                   
{'loss': 1.9455, 'grad_norm': 1.0546875, 'learning_rate': 9.902702702702703e-05, 'epoch': 0.49}

  1%|          | 18/1850 [01:09<1:52:20,  3.68s/it]
  1%|          | 19/1850 [01:13<1:53:36,  3.72s/it]
                                                   
{'loss': 1.8, 'grad_norm': 1.3828125, 'learning_rate': 9.897297297297298e-05, 'epoch': 0.51}

  1%|          | 19/1850 [01:13<1:53:36,  3.72s/it]
  1%|          | 20/1850 [01:17<1:58:15,  3.88s/it]
                                                   
{'loss': 1.7417, 'grad_norm': 1.3515625, 'learning_rate': 9.891891891891892e-05, 'epoch': 0.54}

  1%|          | 20/1850 [01:17<1:58:15,  3.88s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  1%|          | 21/1850 [01:22<2:02:59,  4.03s/it]
                                                   
{'loss': 1.5505, 'grad_norm': 0.98828125, 'learning_rate': 9.886486486486487e-05, 'epoch': 0.57}

  1%|          | 21/1850 [01:22<2:02:59,  4.03s/it]
  1%|          | 22/1850 [01:26<2:00:54,  3.97s/it]
                                                   
{'loss': 1.6476, 'grad_norm': 1.0390625, 'learning_rate': 9.881081081081082e-05, 'epoch': 0.59}

  1%|          | 22/1850 [01:26<2:00:54,  3.97s/it]
  1%|          | 23/1850 [01:29<1:56:22,  3.82s/it]
                                                   
{'loss': 1.8212, 'grad_norm': 1.171875, 'learning_rate': 9.875675675675676e-05, 'epoch': 0.62}

  1%|          | 23/1850 [01:29<1:56:22,  3.82s/it]
  1%|▏         | 24/1850 [01:33<1:54:12,  3.75s/it]
                                                   
{'loss': 1.7056, 'grad_norm': 1.171875, 'learning_rate': 9.870270270270271e-05, 'epoch': 0.65}

  1%|▏         | 24/1850 [01:33<1:54:12,  3.75s/it]
  1%|▏         | 25/1850 [01:37<1:58:16,  3.89s/it]
                                                   
{'loss': 1.5759, 'grad_norm': 1.171875, 'learning_rate': 9.864864864864865e-05, 'epoch': 0.68}

  1%|▏         | 25/1850 [01:37<1:58:16,  3.89s/it]
  1%|▏         | 26/1850 [01:41<2:01:51,  4.01s/it]
                                                   
{'loss': 1.8996, 'grad_norm': 1.1328125, 'learning_rate': 9.85945945945946e-05, 'epoch': 0.7}

  1%|▏         | 26/1850 [01:41<2:01:51,  4.01s/it]
  1%|▏         | 27/1850 [01:46<2:08:03,  4.21s/it]
                                                   
{'loss': 1.6641, 'grad_norm': 0.94921875, 'learning_rate': 9.854054054054055e-05, 'epoch': 0.73}

  1%|▏         | 27/1850 [01:46<2:08:03,  4.21s/it]
  2%|▏         | 28/1850 [01:50<2:09:51,  4.28s/it]
                                                   
{'loss': 1.8253, 'grad_norm': 1.3359375, 'learning_rate': 9.848648648648649e-05, 'epoch': 0.76}

  2%|▏         | 28/1850 [01:50<2:09:51,  4.28s/it]
  2%|▏         | 29/1850 [01:54<2:07:15,  4.19s/it]
                                                   
{'loss': 1.7284, 'grad_norm': 1.2734375, 'learning_rate': 9.843243243243244e-05, 'epoch': 0.78}

  2%|▏         | 29/1850 [01:54<2:07:15,  4.19s/it]
  2%|▏         | 30/1850 [01:59<2:07:00,  4.19s/it]
                                                   
{'loss': 1.809, 'grad_norm': 1.203125, 'learning_rate': 9.837837837837839e-05, 'epoch': 0.81}

  2%|▏         | 30/1850 [01:59<2:07:00,  4.19s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  2%|▏         | 31/1850 [02:03<2:06:05,  4.16s/it]
                                                   
{'loss': 1.5972, 'grad_norm': 1.25, 'learning_rate': 9.832432432432433e-05, 'epoch': 0.84}

  2%|▏         | 31/1850 [02:03<2:06:05,  4.16s/it]
  2%|▏         | 32/1850 [02:08<2:13:26,  4.40s/it]
                                                   
{'loss': 1.6768, 'grad_norm': 1.0703125, 'learning_rate': 9.827027027027027e-05, 'epoch': 0.86}

  2%|▏         | 32/1850 [02:08<2:13:26,  4.40s/it]
  2%|▏         | 33/1850 [02:12<2:11:25,  4.34s/it]
                                                   
{'loss': 1.8191, 'grad_norm': 1.1953125, 'learning_rate': 9.821621621621622e-05, 'epoch': 0.89}

  2%|▏         | 33/1850 [02:12<2:11:25,  4.34s/it]
  2%|▏         | 34/1850 [02:16<2:10:53,  4.32s/it]
                                                   
{'loss': 1.8812, 'grad_norm': 1.0078125, 'learning_rate': 9.816216216216216e-05, 'epoch': 0.92}

  2%|▏         | 34/1850 [02:16<2:10:53,  4.32s/it]
  2%|▏         | 35/1850 [02:19<2:02:31,  4.05s/it]
                                                   
{'loss': 1.1564, 'grad_norm': 0.83984375, 'learning_rate': 9.810810810810811e-05, 'epoch': 0.95}

  2%|▏         | 35/1850 [02:19<2:02:31,  4.05s/it]
  2%|▏         | 36/1850 [02:24<2:02:54,  4.07s/it]
                                                   
{'loss': 1.2996, 'grad_norm': 0.9609375, 'learning_rate': 9.805405405405406e-05, 'epoch': 0.97}

  2%|▏         | 36/1850 [02:24<2:02:54,  4.07s/it]
  2%|▏         | 37/1850 [02:27<1:59:29,  3.95s/it]
                                                   
{'loss': 1.7737, 'grad_norm': 1.2265625, 'learning_rate': 9.8e-05, 'epoch': 1.0}

  2%|▏         | 37/1850 [02:27<1:59:29,  3.95s/it]
  2%|▏         | 38/1850 [02:32<2:03:38,  4.09s/it]
                                                   
{'loss': 1.8043, 'grad_norm': 1.0234375, 'learning_rate': 9.794594594594595e-05, 'epoch': 1.03}

  2%|▏         | 38/1850 [02:32<2:03:38,  4.09s/it]
  2%|▏         | 39/1850 [02:35<1:57:25,  3.89s/it]
                                                   
{'loss': 1.5495, 'grad_norm': 1.109375, 'learning_rate': 9.78918918918919e-05, 'epoch': 1.05}

  2%|▏         | 39/1850 [02:35<1:57:25,  3.89s/it]
  2%|▏         | 40/1850 [02:39<2:01:14,  4.02s/it]
                                                   
{'loss': 1.5965, 'grad_norm': 0.91015625, 'learning_rate': 9.783783783783784e-05, 'epoch': 1.08}

  2%|▏         | 40/1850 [02:39<2:01:14,  4.02s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  2%|▏         | 41/1850 [02:43<1:59:48,  3.97s/it]
                                                   
{'loss': 1.1599, 'grad_norm': 0.9609375, 'learning_rate': 9.778378378378379e-05, 'epoch': 1.11}

  2%|▏         | 41/1850 [02:43<1:59:48,  3.97s/it]
  2%|▏         | 42/1850 [02:47<1:58:53,  3.95s/it]
                                                   
{'loss': 1.4893, 'grad_norm': 0.890625, 'learning_rate': 9.772972972972974e-05, 'epoch': 1.14}

  2%|▏         | 42/1850 [02:47<1:58:53,  3.95s/it]
  2%|▏         | 43/1850 [02:52<2:03:30,  4.10s/it]
                                                   
{'loss': 1.8955, 'grad_norm': 0.9765625, 'learning_rate': 9.767567567567568e-05, 'epoch': 1.16}

  2%|▏         | 43/1850 [02:52<2:03:30,  4.10s/it]
  2%|▏         | 44/1850 [02:57<2:10:39,  4.34s/it]
                                                   
{'loss': 1.4819, 'grad_norm': 1.1796875, 'learning_rate': 9.762162162162163e-05, 'epoch': 1.19}

  2%|▏         | 44/1850 [02:57<2:10:39,  4.34s/it]
  2%|▏         | 45/1850 [03:00<2:06:23,  4.20s/it]
                                                   
{'loss': 1.5766, 'grad_norm': 1.03125, 'learning_rate': 9.756756756756758e-05, 'epoch': 1.22}

  2%|▏         | 45/1850 [03:00<2:06:23,  4.20s/it]
  2%|▏         | 46/1850 [03:04<2:02:29,  4.07s/it]
                                                   
{'loss': 1.2856, 'grad_norm': 1.0625, 'learning_rate': 9.751351351351352e-05, 'epoch': 1.24}

  2%|▏         | 46/1850 [03:04<2:02:29,  4.07s/it]
  3%|▎         | 47/1850 [03:08<1:59:34,  3.98s/it]
                                                   
{'loss': 1.4122, 'grad_norm': 1.2578125, 'learning_rate': 9.745945945945947e-05, 'epoch': 1.27}

  3%|▎         | 47/1850 [03:08<1:59:34,  3.98s/it]
  3%|▎         | 48/1850 [03:12<2:00:01,  4.00s/it]
                                                   
{'loss': 1.6103, 'grad_norm': 1.1484375, 'learning_rate': 9.740540540540542e-05, 'epoch': 1.3}

  3%|▎         | 48/1850 [03:12<2:00:01,  4.00s/it]
  3%|▎         | 49/1850 [03:16<2:03:01,  4.10s/it]
                                                   
{'loss': 1.4836, 'grad_norm': 1.1328125, 'learning_rate': 9.735135135135136e-05, 'epoch': 1.32}

  3%|▎         | 49/1850 [03:16<2:03:01,  4.10s/it]
  3%|▎         | 50/1850 [03:21<2:06:13,  4.21s/it]
                                                   
{'loss': 1.5172, 'grad_norm': 1.078125, 'learning_rate': 9.729729729729731e-05, 'epoch': 1.35}

  3%|▎         | 50/1850 [03:21<2:06:13,  4.21s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  3%|▎         | 51/1850 [03:25<2:07:15,  4.24s/it]
                                                   
{'loss': 1.3896, 'grad_norm': 1.2109375, 'learning_rate': 9.724324324324325e-05, 'epoch': 1.38}

  3%|▎         | 51/1850 [03:25<2:07:15,  4.24s/it]
  3%|▎         | 52/1850 [03:29<2:01:37,  4.06s/it]
                                                   
{'loss': 1.4736, 'grad_norm': 1.21875, 'learning_rate': 9.718918918918918e-05, 'epoch': 1.41}

  3%|▎         | 52/1850 [03:29<2:01:37,  4.06s/it]
  3%|▎         | 53/1850 [03:33<2:03:53,  4.14s/it]
                                                   
{'loss': 1.6712, 'grad_norm': 1.2109375, 'learning_rate': 9.713513513513514e-05, 'epoch': 1.43}

  3%|▎         | 53/1850 [03:33<2:03:53,  4.14s/it]
  3%|▎         | 54/1850 [03:37<2:03:30,  4.13s/it]
                                                   
{'loss': 1.5815, 'grad_norm': 1.1953125, 'learning_rate': 9.708108108108109e-05, 'epoch': 1.46}

  3%|▎         | 54/1850 [03:37<2:03:30,  4.13s/it]
  3%|▎         | 55/1850 [03:41<1:59:37,  4.00s/it]
                                                   
{'loss': 1.5803, 'grad_norm': 1.234375, 'learning_rate': 9.702702702702702e-05, 'epoch': 1.49}

  3%|▎         | 55/1850 [03:41<1:59:37,  4.00s/it]
  3%|▎         | 56/1850 [03:45<1:59:28,  4.00s/it]
                                                   
{'loss': 1.5915, 'grad_norm': 1.25, 'learning_rate': 9.697297297297298e-05, 'epoch': 1.51}

  3%|▎         | 56/1850 [03:45<1:59:28,  4.00s/it]
  3%|▎         | 57/1850 [03:49<2:00:17,  4.03s/it]
                                                   
{'loss': 1.575, 'grad_norm': 1.1796875, 'learning_rate': 9.691891891891893e-05, 'epoch': 1.54}

  3%|▎         | 57/1850 [03:49<2:00:17,  4.03s/it]
  3%|▎         | 58/1850 [03:53<2:02:52,  4.11s/it]
                                                   
{'loss': 1.4412, 'grad_norm': 1.234375, 'learning_rate': 9.686486486486486e-05, 'epoch': 1.57}

  3%|▎         | 58/1850 [03:53<2:02:52,  4.11s/it]
  3%|▎         | 59/1850 [03:58<2:04:24,  4.17s/it]
                                                   
{'loss': 1.6265, 'grad_norm': 1.3046875, 'learning_rate': 9.681081081081082e-05, 'epoch': 1.59}

  3%|▎         | 59/1850 [03:58<2:04:24,  4.17s/it]
  3%|▎         | 60/1850 [04:01<2:00:46,  4.05s/it]
                                                   
{'loss': 1.5063, 'grad_norm': 1.328125, 'learning_rate': 9.675675675675677e-05, 'epoch': 1.62}

  3%|▎         | 60/1850 [04:01<2:00:46,  4.05s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  3%|▎         | 61/1850 [04:06<2:02:50,  4.12s/it]
                                                   
{'loss': 1.657, 'grad_norm': 1.5, 'learning_rate': 9.67027027027027e-05, 'epoch': 1.65}

  3%|▎         | 61/1850 [04:06<2:02:50,  4.12s/it]
  3%|▎         | 62/1850 [04:09<1:55:28,  3.88s/it]
                                                   
{'loss': 1.2373, 'grad_norm': 1.3828125, 'learning_rate': 9.664864864864866e-05, 'epoch': 1.68}

  3%|▎         | 62/1850 [04:09<1:55:28,  3.88s/it]
  3%|▎         | 63/1850 [04:13<1:54:06,  3.83s/it]
                                                   
{'loss': 1.2919, 'grad_norm': 1.359375, 'learning_rate': 9.65945945945946e-05, 'epoch': 1.7}

  3%|▎         | 63/1850 [04:13<1:54:06,  3.83s/it]
  3%|▎         | 64/1850 [04:17<1:54:31,  3.85s/it]
                                                   
{'loss': 1.4302, 'grad_norm': 1.3984375, 'learning_rate': 9.654054054054054e-05, 'epoch': 1.73}

  3%|▎         | 64/1850 [04:17<1:54:31,  3.85s/it]
  4%|▎         | 65/1850 [04:20<1:46:45,  3.59s/it]
                                                   
{'loss': 1.0337, 'grad_norm': 1.4765625, 'learning_rate': 9.64864864864865e-05, 'epoch': 1.76}

  4%|▎         | 65/1850 [04:20<1:46:45,  3.59s/it]
  4%|▎         | 66/1850 [04:24<1:50:43,  3.72s/it]
                                                   
{'loss': 1.4456, 'grad_norm': 1.5390625, 'learning_rate': 9.643243243243245e-05, 'epoch': 1.78}

  4%|▎         | 66/1850 [04:24<1:50:43,  3.72s/it]
  4%|▎         | 67/1850 [04:27<1:51:38,  3.76s/it]
                                                   
{'loss': 1.3216, 'grad_norm': 1.3046875, 'learning_rate': 9.637837837837838e-05, 'epoch': 1.81}

  4%|▎         | 67/1850 [04:27<1:51:38,  3.76s/it]
  4%|▎         | 68/1850 [04:30<1:42:46,  3.46s/it]
                                                   
{'loss': 1.0755, 'grad_norm': 1.4765625, 'learning_rate': 9.632432432432434e-05, 'epoch': 1.84}

  4%|▎         | 68/1850 [04:30<1:42:46,  3.46s/it]
  4%|▎         | 69/1850 [04:34<1:47:29,  3.62s/it]
                                                   
{'loss': 1.333, 'grad_norm': 1.3984375, 'learning_rate': 9.627027027027027e-05, 'epoch': 1.86}

  4%|▎         | 69/1850 [04:34<1:47:29,  3.62s/it]
  4%|▍         | 70/1850 [04:38<1:47:40,  3.63s/it]
                                                   
{'loss': 1.2708, 'grad_norm': 1.6953125, 'learning_rate': 9.621621621621622e-05, 'epoch': 1.89}

  4%|▍         | 70/1850 [04:38<1:47:40,  3.63s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  4%|▍         | 71/1850 [04:42<1:53:43,  3.84s/it]
                                                   
{'loss': 1.1739, 'grad_norm': 1.359375, 'learning_rate': 9.616216216216216e-05, 'epoch': 1.92}

  4%|▍         | 71/1850 [04:42<1:53:43,  3.84s/it]
  4%|▍         | 72/1850 [04:47<1:59:48,  4.04s/it]
                                                   
{'loss': 1.4979, 'grad_norm': 1.5078125, 'learning_rate': 9.610810810810811e-05, 'epoch': 1.95}

  4%|▍         | 72/1850 [04:47<1:59:48,  4.04s/it]
  4%|▍         | 73/1850 [04:51<2:00:41,  4.07s/it]
                                                   
{'loss': 1.4934, 'grad_norm': 1.4296875, 'learning_rate': 9.605405405405405e-05, 'epoch': 1.97}

  4%|▍         | 73/1850 [04:51<2:00:41,  4.07s/it]
  4%|▍         | 74/1850 [04:55<2:00:16,  4.06s/it]
                                                   
{'loss': 1.4674, 'grad_norm': 1.5078125, 'learning_rate': 9.6e-05, 'epoch': 2.0}

  4%|▍         | 74/1850 [04:55<2:00:16,  4.06s/it]
  4%|▍         | 75/1850 [04:59<1:58:13,  4.00s/it]
                                                   
{'loss': 1.1846, 'grad_norm': 1.3671875, 'learning_rate': 9.594594594594595e-05, 'epoch': 2.03}

  4%|▍         | 75/1850 [04:59<1:58:13,  4.00s/it]
  4%|▍         | 76/1850 [05:03<1:58:40,  4.01s/it]
                                                   
{'loss': 1.2222, 'grad_norm': 1.375, 'learning_rate': 9.589189189189189e-05, 'epoch': 2.05}

  4%|▍         | 76/1850 [05:03<1:58:40,  4.01s/it]
  4%|▍         | 77/1850 [05:07<2:00:12,  4.07s/it]
                                                   
{'loss': 1.2257, 'grad_norm': 1.296875, 'learning_rate': 9.583783783783784e-05, 'epoch': 2.08}

  4%|▍         | 77/1850 [05:07<2:00:12,  4.07s/it]
  4%|▍         | 78/1850 [05:11<1:57:26,  3.98s/it]
                                                   
{'loss': 1.3053, 'grad_norm': 1.484375, 'learning_rate': 9.57837837837838e-05, 'epoch': 2.11}

  4%|▍         | 78/1850 [05:11<1:57:26,  3.98s/it]
  4%|▍         | 79/1850 [05:14<1:53:35,  3.85s/it]
                                                   
{'loss': 0.9418, 'grad_norm': 1.390625, 'learning_rate': 9.572972972972973e-05, 'epoch': 2.14}

  4%|▍         | 79/1850 [05:14<1:53:35,  3.85s/it]
  4%|▍         | 80/1850 [05:18<1:53:46,  3.86s/it]
                                                   
{'loss': 1.1923, 'grad_norm': 1.5703125, 'learning_rate': 9.567567567567568e-05, 'epoch': 2.16}

  4%|▍         | 80/1850 [05:18<1:53:46,  3.86s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  4%|▍         | 81/1850 [05:22<1:53:55,  3.86s/it]
                                                   
{'loss': 1.0563, 'grad_norm': 1.828125, 'learning_rate': 9.562162162162162e-05, 'epoch': 2.19}

  4%|▍         | 81/1850 [05:22<1:53:55,  3.86s/it]
  4%|▍         | 82/1850 [05:27<2:05:35,  4.26s/it]
                                                   
{'loss': 1.4966, 'grad_norm': 1.6171875, 'learning_rate': 9.556756756756757e-05, 'epoch': 2.22}

  4%|▍         | 82/1850 [05:27<2:05:35,  4.26s/it]
  4%|▍         | 83/1850 [05:31<2:02:47,  4.17s/it]
                                                   
{'loss': 1.3557, 'grad_norm': 2.140625, 'learning_rate': 9.551351351351352e-05, 'epoch': 2.24}

  4%|▍         | 83/1850 [05:31<2:02:47,  4.17s/it]
  5%|▍         | 84/1850 [05:35<2:02:02,  4.15s/it]
                                                   
{'loss': 1.1344, 'grad_norm': 1.765625, 'learning_rate': 9.545945945945946e-05, 'epoch': 2.27}

  5%|▍         | 84/1850 [05:35<2:02:02,  4.15s/it]
  5%|▍         | 85/1850 [05:39<2:00:51,  4.11s/it]
                                                   
{'loss': 1.3167, 'grad_norm': 1.8203125, 'learning_rate': 9.540540540540541e-05, 'epoch': 2.3}

  5%|▍         | 85/1850 [05:39<2:00:51,  4.11s/it]
  5%|▍         | 86/1850 [05:44<2:03:25,  4.20s/it]
                                                   
{'loss': 1.2304, 'grad_norm': 2.015625, 'learning_rate': 9.535135135135136e-05, 'epoch': 2.32}

  5%|▍         | 86/1850 [05:44<2:03:25,  4.20s/it]
  5%|▍         | 87/1850 [05:48<2:03:08,  4.19s/it]
                                                   
{'loss': 1.3454, 'grad_norm': 2.015625, 'learning_rate': 9.52972972972973e-05, 'epoch': 2.35}

  5%|▍         | 87/1850 [05:48<2:03:08,  4.19s/it]
  5%|▍         | 88/1850 [05:52<2:05:33,  4.28s/it]
                                                   
{'loss': 1.2477, 'grad_norm': 1.8984375, 'learning_rate': 9.524324324324325e-05, 'epoch': 2.38}

  5%|▍         | 88/1850 [05:52<2:05:33,  4.28s/it]
  5%|▍         | 89/1850 [05:56<2:02:58,  4.19s/it]
                                                   
{'loss': 1.1661, 'grad_norm': 2.046875, 'learning_rate': 9.51891891891892e-05, 'epoch': 2.41}

  5%|▍         | 89/1850 [05:56<2:02:58,  4.19s/it]
  5%|▍         | 90/1850 [06:01<2:03:30,  4.21s/it]
                                                   
{'loss': 1.248, 'grad_norm': 1.859375, 'learning_rate': 9.513513513513514e-05, 'epoch': 2.43}

  5%|▍         | 90/1850 [06:01<2:03:30,  4.21s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  5%|▍         | 91/1850 [06:04<1:58:15,  4.03s/it]
                                                   
{'loss': 1.1551, 'grad_norm': 2.34375, 'learning_rate': 9.508108108108108e-05, 'epoch': 2.46}

  5%|▍         | 91/1850 [06:04<1:58:15,  4.03s/it]
  5%|▍         | 92/1850 [06:08<1:55:55,  3.96s/it]
                                                   
{'loss': 1.0634, 'grad_norm': 1.7578125, 'learning_rate': 9.502702702702703e-05, 'epoch': 2.49}

  5%|▍         | 92/1850 [06:08<1:55:55,  3.96s/it]
  5%|▌         | 93/1850 [06:12<1:54:21,  3.91s/it]
                                                   
{'loss': 1.1138, 'grad_norm': 2.171875, 'learning_rate': 9.497297297297297e-05, 'epoch': 2.51}

  5%|▌         | 93/1850 [06:12<1:54:21,  3.91s/it]
  5%|▌         | 94/1850 [06:15<1:52:09,  3.83s/it]
                                                   
{'loss': 1.0307, 'grad_norm': 2.046875, 'learning_rate': 9.491891891891892e-05, 'epoch': 2.54}

  5%|▌         | 94/1850 [06:15<1:52:09,  3.83s/it]
  5%|▌         | 95/1850 [06:19<1:49:56,  3.76s/it]
                                                   
{'loss': 1.144, 'grad_norm': 2.046875, 'learning_rate': 9.486486486486487e-05, 'epoch': 2.57}

  5%|▌         | 95/1850 [06:19<1:49:56,  3.76s/it]
  5%|▌         | 96/1850 [06:24<1:56:34,  3.99s/it]
                                                   
{'loss': 1.3065, 'grad_norm': 1.9140625, 'learning_rate': 9.481081081081081e-05, 'epoch': 2.59}

  5%|▌         | 96/1850 [06:24<1:56:34,  3.99s/it]
  5%|▌         | 97/1850 [06:28<1:57:40,  4.03s/it]
                                                   
{'loss': 1.1114, 'grad_norm': 2.25, 'learning_rate': 9.475675675675676e-05, 'epoch': 2.62}

  5%|▌         | 97/1850 [06:28<1:57:40,  4.03s/it]
  5%|▌         | 98/1850 [06:32<1:56:48,  4.00s/it]
                                                   
{'loss': 1.2033, 'grad_norm': 2.34375, 'learning_rate': 9.470270270270271e-05, 'epoch': 2.65}

  5%|▌         | 98/1850 [06:32<1:56:48,  4.00s/it]
  5%|▌         | 99/1850 [06:36<1:57:44,  4.03s/it]
                                                   
{'loss': 1.2467, 'grad_norm': 2.234375, 'learning_rate': 9.464864864864865e-05, 'epoch': 2.68}

  5%|▌         | 99/1850 [06:36<1:57:44,  4.03s/it]
  5%|▌         | 100/1850 [06:40<1:58:49,  4.07s/it]
                                                    
{'loss': 1.263, 'grad_norm': 2.25, 'learning_rate': 9.45945945945946e-05, 'epoch': 2.7}

  5%|▌         | 100/1850 [06:40<1:58:49,  4.07s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  5%|▌         | 101/1850 [06:44<1:59:56,  4.11s/it]
                                                    
{'loss': 1.1257, 'grad_norm': 2.40625, 'learning_rate': 9.454054054054055e-05, 'epoch': 2.73}

  5%|▌         | 101/1850 [06:44<1:59:56,  4.11s/it]
  6%|▌         | 102/1850 [06:48<2:02:23,  4.20s/it]
                                                    
{'loss': 1.2363, 'grad_norm': 2.34375, 'learning_rate': 9.448648648648649e-05, 'epoch': 2.76}

  6%|▌         | 102/1850 [06:48<2:02:23,  4.20s/it]
  6%|▌         | 103/1850 [06:53<2:02:44,  4.22s/it]
                                                    
{'loss': 1.2409, 'grad_norm': 2.4375, 'learning_rate': 9.443243243243244e-05, 'epoch': 2.78}

  6%|▌         | 103/1850 [06:53<2:02:44,  4.22s/it]
  6%|▌         | 104/1850 [06:57<2:00:25,  4.14s/it]
                                                    
{'loss': 1.0837, 'grad_norm': 2.359375, 'learning_rate': 9.437837837837839e-05, 'epoch': 2.81}

  6%|▌         | 104/1850 [06:57<2:00:25,  4.14s/it]
  6%|▌         | 105/1850 [07:01<1:57:52,  4.05s/it]
                                                    
{'loss': 1.1455, 'grad_norm': 2.296875, 'learning_rate': 9.432432432432433e-05, 'epoch': 2.84}

  6%|▌         | 105/1850 [07:01<1:57:52,  4.05s/it]
  6%|▌         | 106/1850 [07:05<1:58:46,  4.09s/it]
                                                    
{'loss': 1.223, 'grad_norm': 2.4375, 'learning_rate': 9.427027027027028e-05, 'epoch': 2.86}

  6%|▌         | 106/1850 [07:05<1:58:46,  4.09s/it]
  6%|▌         | 107/1850 [07:08<1:56:04,  4.00s/it]
                                                    
{'loss': 1.1371, 'grad_norm': 2.15625, 'learning_rate': 9.421621621621623e-05, 'epoch': 2.89}

  6%|▌         | 107/1850 [07:08<1:56:04,  4.00s/it]
  6%|▌         | 108/1850 [07:12<1:53:16,  3.90s/it]
                                                    
{'loss': 0.9591, 'grad_norm': 2.25, 'learning_rate': 9.416216216216217e-05, 'epoch': 2.92}

  6%|▌         | 108/1850 [07:12<1:53:16,  3.90s/it]
  6%|▌         | 109/1850 [07:16<1:52:16,  3.87s/it]
                                                    
{'loss': 1.1263, 'grad_norm': 2.375, 'learning_rate': 9.410810810810812e-05, 'epoch': 2.95}

  6%|▌         | 109/1850 [07:16<1:52:16,  3.87s/it]
  6%|▌         | 110/1850 [07:20<1:49:28,  3.78s/it]
                                                    
{'loss': 0.8381, 'grad_norm': 2.3125, 'learning_rate': 9.405405405405407e-05, 'epoch': 2.97}

  6%|▌         | 110/1850 [07:20<1:49:28,  3.78s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  6%|▌         | 111/1850 [07:24<1:57:24,  4.05s/it]
                                                    
{'loss': 1.0841, 'grad_norm': 2.671875, 'learning_rate': 9.4e-05, 'epoch': 3.0}

  6%|▌         | 111/1850 [07:24<1:57:24,  4.05s/it]
  6%|▌         | 112/1850 [07:28<1:56:20,  4.02s/it]
                                                    
{'loss': 0.8044, 'grad_norm': 2.1875, 'learning_rate': 9.394594594594595e-05, 'epoch': 3.03}

  6%|▌         | 112/1850 [07:28<1:56:20,  4.02s/it]
  6%|▌         | 113/1850 [07:32<1:57:40,  4.06s/it]
                                                    
{'loss': 1.0154, 'grad_norm': 2.203125, 'learning_rate': 9.38918918918919e-05, 'epoch': 3.05}

  6%|▌         | 113/1850 [07:32<1:57:40,  4.06s/it]
  6%|▌         | 114/1850 [07:36<1:57:48,  4.07s/it]
                                                    
{'loss': 0.8317, 'grad_norm': 2.25, 'learning_rate': 9.383783783783783e-05, 'epoch': 3.08}

  6%|▌         | 114/1850 [07:36<1:57:48,  4.07s/it]
  6%|▌         | 115/1850 [07:40<1:55:24,  3.99s/it]
                                                    
{'loss': 0.8459, 'grad_norm': 2.515625, 'learning_rate': 9.378378378378379e-05, 'epoch': 3.11}

  6%|▌         | 115/1850 [07:40<1:55:24,  3.99s/it]
  6%|▋         | 116/1850 [07:45<2:02:06,  4.23s/it]
                                                    
{'loss': 0.9586, 'grad_norm': 2.625, 'learning_rate': 9.372972972972974e-05, 'epoch': 3.14}

  6%|▋         | 116/1850 [07:45<2:02:06,  4.23s/it]
  6%|▋         | 117/1850 [07:49<1:58:19,  4.10s/it]
                                                    
{'loss': 0.932, 'grad_norm': 3.0625, 'learning_rate': 9.367567567567567e-05, 'epoch': 3.16}

  6%|▋         | 117/1850 [07:49<1:58:19,  4.10s/it]
  6%|▋         | 118/1850 [07:53<1:58:33,  4.11s/it]
                                                    
{'loss': 0.7373, 'grad_norm': 3.78125, 'learning_rate': 9.362162162162163e-05, 'epoch': 3.19}

  6%|▋         | 118/1850 [07:53<1:58:33,  4.11s/it]
  6%|▋         | 119/1850 [07:56<1:47:08,  3.71s/it]
                                                    
{'loss': 0.6024, 'grad_norm': 3.765625, 'learning_rate': 9.356756756756758e-05, 'epoch': 3.22}

  6%|▋         | 119/1850 [07:56<1:47:08,  3.71s/it]
  6%|▋         | 120/1850 [08:00<1:53:53,  3.95s/it]
                                                    
{'loss': 0.9973, 'grad_norm': 3.53125, 'learning_rate': 9.351351351351351e-05, 'epoch': 3.24}

  6%|▋         | 120/1850 [08:00<1:53:53,  3.95s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  7%|▋         | 121/1850 [08:05<1:58:25,  4.11s/it]
                                                    
{'loss': 0.7271, 'grad_norm': 3.140625, 'learning_rate': 9.345945945945947e-05, 'epoch': 3.27}

  7%|▋         | 121/1850 [08:05<1:58:25,  4.11s/it]
  7%|▋         | 122/1850 [08:09<1:58:16,  4.11s/it]
                                                    
{'loss': 0.9441, 'grad_norm': 3.515625, 'learning_rate': 9.340540540540542e-05, 'epoch': 3.3}

  7%|▋         | 122/1850 [08:09<1:58:16,  4.11s/it]
  7%|▋         | 123/1850 [08:12<1:51:59,  3.89s/it]
                                                    
{'loss': 0.7362, 'grad_norm': 2.859375, 'learning_rate': 9.335135135135135e-05, 'epoch': 3.32}

  7%|▋         | 123/1850 [08:12<1:51:59,  3.89s/it]
  7%|▋         | 124/1850 [08:16<1:53:21,  3.94s/it]
                                                    
{'loss': 0.8041, 'grad_norm': 2.9375, 'learning_rate': 9.32972972972973e-05, 'epoch': 3.35}

  7%|▋         | 124/1850 [08:16<1:53:21,  3.94s/it]
  7%|▋         | 125/1850 [08:20<1:51:00,  3.86s/it]
                                                    
{'loss': 0.6962, 'grad_norm': 2.8125, 'learning_rate': 9.324324324324324e-05, 'epoch': 3.38}

  7%|▋         | 125/1850 [08:20<1:51:00,  3.86s/it]
  7%|▋         | 126/1850 [08:24<1:50:19,  3.84s/it]
                                                    
{'loss': 0.8724, 'grad_norm': 3.15625, 'learning_rate': 9.31891891891892e-05, 'epoch': 3.41}

  7%|▋         | 126/1850 [08:24<1:50:19,  3.84s/it]
  7%|▋         | 127/1850 [08:28<1:53:45,  3.96s/it]
                                                    
{'loss': 0.8604, 'grad_norm': 2.8125, 'learning_rate': 9.313513513513515e-05, 'epoch': 3.43}

  7%|▋         | 127/1850 [08:28<1:53:45,  3.96s/it]
  7%|▋         | 128/1850 [08:32<1:53:20,  3.95s/it]
                                                    
{'loss': 0.9573, 'grad_norm': 2.890625, 'learning_rate': 9.308108108108108e-05, 'epoch': 3.46}

  7%|▋         | 128/1850 [08:32<1:53:20,  3.95s/it]
  7%|▋         | 129/1850 [08:36<1:56:20,  4.06s/it]
                                                    
{'loss': 0.8118, 'grad_norm': 2.78125, 'learning_rate': 9.302702702702704e-05, 'epoch': 3.49}

  7%|▋         | 129/1850 [08:36<1:56:20,  4.06s/it]
  7%|▋         | 130/1850 [08:39<1:47:40,  3.76s/it]
                                                    
{'loss': 0.7459, 'grad_norm': 3.140625, 'learning_rate': 9.297297297297299e-05, 'epoch': 3.51}

  7%|▋         | 130/1850 [08:39<1:47:40,  3.76s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  7%|▋         | 131/1850 [08:44<1:53:45,  3.97s/it]
                                                    
{'loss': 0.7481, 'grad_norm': 2.859375, 'learning_rate': 9.291891891891892e-05, 'epoch': 3.54}

  7%|▋         | 131/1850 [08:44<1:53:45,  3.97s/it]
  7%|▋         | 132/1850 [08:48<1:53:51,  3.98s/it]
                                                    
{'loss': 0.6377, 'grad_norm': 2.875, 'learning_rate': 9.286486486486486e-05, 'epoch': 3.57}

  7%|▋         | 132/1850 [08:48<1:53:51,  3.98s/it]
  7%|▋         | 133/1850 [08:51<1:47:49,  3.77s/it]
                                                    
{'loss': 0.6156, 'grad_norm': 3.234375, 'learning_rate': 9.281081081081081e-05, 'epoch': 3.59}

  7%|▋         | 133/1850 [08:51<1:47:49,  3.77s/it]
  7%|▋         | 134/1850 [08:55<1:46:17,  3.72s/it]
                                                    
{'loss': 0.8608, 'grad_norm': 3.625, 'learning_rate': 9.275675675675676e-05, 'epoch': 3.62}

  7%|▋         | 134/1850 [08:55<1:46:17,  3.72s/it]
  7%|▋         | 135/1850 [08:59<1:51:22,  3.90s/it]
                                                    
{'loss': 0.8989, 'grad_norm': 3.3125, 'learning_rate': 9.27027027027027e-05, 'epoch': 3.65}

  7%|▋         | 135/1850 [08:59<1:51:22,  3.90s/it]
  7%|▋         | 136/1850 [09:03<1:56:06,  4.06s/it]
                                                    
{'loss': 0.9112, 'grad_norm': 3.296875, 'learning_rate': 9.264864864864865e-05, 'epoch': 3.68}

  7%|▋         | 136/1850 [09:03<1:56:06,  4.06s/it]
  7%|▋         | 137/1850 [09:08<1:57:15,  4.11s/it]
                                                    
{'loss': 0.8492, 'grad_norm': 3.3125, 'learning_rate': 9.259459459459459e-05, 'epoch': 3.7}

  7%|▋         | 137/1850 [09:08<1:57:15,  4.11s/it]
  7%|▋         | 138/1850 [09:12<1:58:54,  4.17s/it]
                                                    
{'loss': 0.8974, 'grad_norm': 3.375, 'learning_rate': 9.254054054054054e-05, 'epoch': 3.73}

  7%|▋         | 138/1850 [09:12<1:58:54,  4.17s/it]
  8%|▊         | 139/1850 [09:15<1:52:36,  3.95s/it]
                                                    
{'loss': 0.8303, 'grad_norm': 3.5625, 'learning_rate': 9.24864864864865e-05, 'epoch': 3.76}

  8%|▊         | 139/1850 [09:15<1:52:36,  3.95s/it]
  8%|▊         | 140/1850 [09:20<1:55:53,  4.07s/it]
                                                    
{'loss': 0.8483, 'grad_norm': 3.34375, 'learning_rate': 9.243243243243243e-05, 'epoch': 3.78}

  8%|▊         | 140/1850 [09:20<1:55:53,  4.07s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  8%|▊         | 141/1850 [09:24<1:54:11,  4.01s/it]
                                                    
{'loss': 0.8566, 'grad_norm': 3.6875, 'learning_rate': 9.237837837837838e-05, 'epoch': 3.81}

  8%|▊         | 141/1850 [09:24<1:54:11,  4.01s/it]
  8%|▊         | 142/1850 [09:28<1:54:01,  4.01s/it]
                                                    
{'loss': 0.6827, 'grad_norm': 2.96875, 'learning_rate': 9.232432432432433e-05, 'epoch': 3.84}

  8%|▊         | 142/1850 [09:28<1:54:01,  4.01s/it]
  8%|▊         | 143/1850 [09:32<1:55:01,  4.04s/it]
                                                    
{'loss': 0.807, 'grad_norm': 3.34375, 'learning_rate': 9.227027027027027e-05, 'epoch': 3.86}

  8%|▊         | 143/1850 [09:32<1:55:01,  4.04s/it]
  8%|▊         | 144/1850 [09:35<1:51:54,  3.94s/it]
                                                    
{'loss': 0.8278, 'grad_norm': 3.359375, 'learning_rate': 9.221621621621622e-05, 'epoch': 3.89}

  8%|▊         | 144/1850 [09:35<1:51:54,  3.94s/it]
  8%|▊         | 145/1850 [09:39<1:49:08,  3.84s/it]
                                                    
{'loss': 0.7017, 'grad_norm': 3.375, 'learning_rate': 9.216216216216217e-05, 'epoch': 3.92}

  8%|▊         | 145/1850 [09:39<1:49:08,  3.84s/it]
  8%|▊         | 146/1850 [09:44<2:00:49,  4.25s/it]
                                                    
{'loss': 1.1526, 'grad_norm': 3.25, 'learning_rate': 9.210810810810811e-05, 'epoch': 3.95}

  8%|▊         | 146/1850 [09:44<2:00:49,  4.25s/it]
  8%|▊         | 147/1850 [09:48<1:54:28,  4.03s/it]
                                                    
{'loss': 0.9005, 'grad_norm': 3.546875, 'learning_rate': 9.205405405405406e-05, 'epoch': 3.97}

  8%|▊         | 147/1850 [09:48<1:54:28,  4.03s/it]
  8%|▊         | 148/1850 [09:52<1:57:11,  4.13s/it]
                                                    
{'loss': 0.9882, 'grad_norm': 2.9375, 'learning_rate': 9.200000000000001e-05, 'epoch': 4.0}

  8%|▊         | 148/1850 [09:52<1:57:11,  4.13s/it]
  8%|▊         | 149/1850 [09:56<1:52:17,  3.96s/it]
                                                    
{'loss': 0.5302, 'grad_norm': 3.34375, 'learning_rate': 9.194594594594595e-05, 'epoch': 4.03}

  8%|▊         | 149/1850 [09:56<1:52:17,  3.96s/it]
  8%|▊         | 150/1850 [10:00<1:55:32,  4.08s/it]
                                                    
{'loss': 0.6579, 'grad_norm': 2.796875, 'learning_rate': 9.18918918918919e-05, 'epoch': 4.05}

  8%|▊         | 150/1850 [10:00<1:55:32,  4.08s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  8%|▊         | 151/1850 [10:04<1:55:06,  4.06s/it]
                                                    
{'loss': 0.568, 'grad_norm': 2.953125, 'learning_rate': 9.183783783783784e-05, 'epoch': 4.08}

  8%|▊         | 151/1850 [10:04<1:55:06,  4.06s/it]
  8%|▊         | 152/1850 [10:07<1:49:31,  3.87s/it]
                                                    
{'loss': 0.3768, 'grad_norm': 3.296875, 'learning_rate': 9.178378378378378e-05, 'epoch': 4.11}

  8%|▊         | 152/1850 [10:07<1:49:31,  3.87s/it]
  8%|▊         | 153/1850 [10:11<1:48:50,  3.85s/it]
                                                    
{'loss': 0.4827, 'grad_norm': 3.265625, 'learning_rate': 9.172972972972973e-05, 'epoch': 4.14}

  8%|▊         | 153/1850 [10:11<1:48:50,  3.85s/it]
  8%|▊         | 154/1850 [10:16<1:53:49,  4.03s/it]
                                                    
{'loss': 0.5682, 'grad_norm': 3.765625, 'learning_rate': 9.167567567567568e-05, 'epoch': 4.16}

  8%|▊         | 154/1850 [10:16<1:53:49,  4.03s/it]
  8%|▊         | 155/1850 [10:20<1:56:55,  4.14s/it]
                                                    
{'loss': 0.5464, 'grad_norm': 4.375, 'learning_rate': 9.162162162162162e-05, 'epoch': 4.19}

  8%|▊         | 155/1850 [10:20<1:56:55,  4.14s/it]
  8%|▊         | 156/1850 [10:24<1:57:27,  4.16s/it]
                                                    
{'loss': 0.5973, 'grad_norm': 5.0, 'learning_rate': 9.156756756756757e-05, 'epoch': 4.22}

  8%|▊         | 156/1850 [10:24<1:57:27,  4.16s/it]
  8%|▊         | 157/1850 [10:28<1:58:02,  4.18s/it]
                                                    
{'loss': 0.6211, 'grad_norm': 5.96875, 'learning_rate': 9.151351351351352e-05, 'epoch': 4.24}

  8%|▊         | 157/1850 [10:28<1:58:02,  4.18s/it]
  9%|▊         | 158/1850 [10:33<1:57:12,  4.16s/it]
                                                    
{'loss': 0.4369, 'grad_norm': 4.84375, 'learning_rate': 9.145945945945946e-05, 'epoch': 4.27}

  9%|▊         | 158/1850 [10:33<1:57:12,  4.16s/it]
  9%|▊         | 159/1850 [10:37<1:58:57,  4.22s/it]
                                                    
{'loss': 0.6031, 'grad_norm': 5.25, 'learning_rate': 9.140540540540541e-05, 'epoch': 4.3}

  9%|▊         | 159/1850 [10:37<1:58:57,  4.22s/it]
  9%|▊         | 160/1850 [10:41<2:01:16,  4.31s/it]
                                                    
{'loss': 0.6499, 'grad_norm': 4.53125, 'learning_rate': 9.135135135135136e-05, 'epoch': 4.32}

  9%|▊         | 160/1850 [10:41<2:01:16,  4.31s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  9%|▊         | 161/1850 [10:46<2:00:38,  4.29s/it]
                                                    
{'loss': 0.4997, 'grad_norm': 4.3125, 'learning_rate': 9.12972972972973e-05, 'epoch': 4.35}

  9%|▊         | 161/1850 [10:46<2:00:38,  4.29s/it]
  9%|▉         | 162/1850 [10:50<2:02:15,  4.35s/it]
                                                    
{'loss': 0.5458, 'grad_norm': 4.1875, 'learning_rate': 9.124324324324325e-05, 'epoch': 4.38}

  9%|▉         | 162/1850 [10:50<2:02:15,  4.35s/it]
  9%|▉         | 163/1850 [10:55<2:01:59,  4.34s/it]
                                                    
{'loss': 0.5037, 'grad_norm': 3.9375, 'learning_rate': 9.11891891891892e-05, 'epoch': 4.41}

  9%|▉         | 163/1850 [10:55<2:01:59,  4.34s/it]
  9%|▉         | 164/1850 [10:58<1:56:45,  4.15s/it]
                                                    
{'loss': 0.3909, 'grad_norm': 2.8125, 'learning_rate': 9.113513513513514e-05, 'epoch': 4.43}

  9%|▉         | 164/1850 [10:58<1:56:45,  4.15s/it]
  9%|▉         | 165/1850 [11:02<1:49:56,  3.91s/it]
                                                    
{'loss': 0.4303, 'grad_norm': 3.65625, 'learning_rate': 9.108108108108109e-05, 'epoch': 4.46}

  9%|▉         | 165/1850 [11:02<1:49:56,  3.91s/it]
  9%|▉         | 166/1850 [11:05<1:49:29,  3.90s/it]
                                                    
{'loss': 0.514, 'grad_norm': 3.34375, 'learning_rate': 9.102702702702704e-05, 'epoch': 4.49}

  9%|▉         | 166/1850 [11:05<1:49:29,  3.90s/it]
  9%|▉         | 167/1850 [11:09<1:49:19,  3.90s/it]
                                                    
{'loss': 0.4886, 'grad_norm': 3.140625, 'learning_rate': 9.097297297297298e-05, 'epoch': 4.51}

  9%|▉         | 167/1850 [11:09<1:49:19,  3.90s/it]
  9%|▉         | 168/1850 [11:13<1:51:18,  3.97s/it]
                                                    
{'loss': 0.5401, 'grad_norm': 3.625, 'learning_rate': 9.091891891891893e-05, 'epoch': 4.54}

  9%|▉         | 168/1850 [11:13<1:51:18,  3.97s/it]
  9%|▉         | 169/1850 [11:17<1:47:29,  3.84s/it]
                                                    
{'loss': 0.4275, 'grad_norm': 3.609375, 'learning_rate': 9.086486486486487e-05, 'epoch': 4.57}

  9%|▉         | 169/1850 [11:17<1:47:29,  3.84s/it]
  9%|▉         | 170/1850 [11:21<1:49:11,  3.90s/it]
                                                    
{'loss': 0.4679, 'grad_norm': 3.46875, 'learning_rate': 9.081081081081082e-05, 'epoch': 4.59}

  9%|▉         | 170/1850 [11:21<1:49:11,  3.90s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

  9%|▉         | 171/1850 [11:25<1:51:18,  3.98s/it]
                                                    
{'loss': 0.454, 'grad_norm': 3.359375, 'learning_rate': 9.075675675675676e-05, 'epoch': 4.62}

  9%|▉         | 171/1850 [11:25<1:51:18,  3.98s/it]
  9%|▉         | 172/1850 [11:29<1:51:43,  4.00s/it]
                                                    
{'loss': 0.5205, 'grad_norm': 3.609375, 'learning_rate': 9.070270270270271e-05, 'epoch': 4.65}

  9%|▉         | 172/1850 [11:29<1:51:43,  4.00s/it]
  9%|▉         | 173/1850 [11:33<1:49:59,  3.94s/it]
                                                    
{'loss': 0.4954, 'grad_norm': 4.3125, 'learning_rate': 9.064864864864865e-05, 'epoch': 4.68}

  9%|▉         | 173/1850 [11:33<1:49:59,  3.94s/it]
  9%|▉         | 174/1850 [11:37<1:48:32,  3.89s/it]
                                                    
{'loss': 0.3948, 'grad_norm': 4.4375, 'learning_rate': 9.05945945945946e-05, 'epoch': 4.7}

  9%|▉         | 174/1850 [11:37<1:48:32,  3.89s/it]
  9%|▉         | 175/1850 [11:40<1:45:14,  3.77s/it]
                                                    
{'loss': 0.4439, 'grad_norm': 4.65625, 'learning_rate': 9.054054054054055e-05, 'epoch': 4.73}

  9%|▉         | 175/1850 [11:40<1:45:14,  3.77s/it]
 10%|▉         | 176/1850 [11:44<1:47:27,  3.85s/it]
                                                    
{'loss': 0.5254, 'grad_norm': 4.28125, 'learning_rate': 9.048648648648649e-05, 'epoch': 4.76}

 10%|▉         | 176/1850 [11:44<1:47:27,  3.85s/it]
 10%|▉         | 177/1850 [11:49<1:53:42,  4.08s/it]
                                                    
{'loss': 0.822, 'grad_norm': 4.8125, 'learning_rate': 9.043243243243244e-05, 'epoch': 4.78}

 10%|▉         | 177/1850 [11:49<1:53:42,  4.08s/it]
 10%|▉         | 178/1850 [11:52<1:48:52,  3.91s/it]
                                                    
{'loss': 0.4553, 'grad_norm': 4.5, 'learning_rate': 9.037837837837839e-05, 'epoch': 4.81}

 10%|▉         | 178/1850 [11:52<1:48:52,  3.91s/it]
 10%|▉         | 179/1850 [11:57<1:50:17,  3.96s/it]
                                                    
{'loss': 0.5763, 'grad_norm': 4.09375, 'learning_rate': 9.032432432432433e-05, 'epoch': 4.84}

 10%|▉         | 179/1850 [11:57<1:50:17,  3.96s/it]
 10%|▉         | 180/1850 [12:01<1:53:53,  4.09s/it]
                                                    
{'loss': 0.3716, 'grad_norm': 3.34375, 'learning_rate': 9.027027027027028e-05, 'epoch': 4.86}

 10%|▉         | 180/1850 [12:01<1:53:53,  4.09s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 10%|▉         | 181/1850 [12:05<1:56:05,  4.17s/it]
                                                    
{'loss': 0.4859, 'grad_norm': 3.703125, 'learning_rate': 9.021621621621621e-05, 'epoch': 4.89}

 10%|▉         | 181/1850 [12:05<1:56:05,  4.17s/it]
 10%|▉         | 182/1850 [12:09<1:54:25,  4.12s/it]
                                                    
{'loss': 0.5911, 'grad_norm': 3.984375, 'learning_rate': 9.016216216216217e-05, 'epoch': 4.92}

 10%|▉         | 182/1850 [12:09<1:54:25,  4.12s/it]
 10%|▉         | 183/1850 [12:13<1:52:25,  4.05s/it]
                                                    
{'loss': 0.4888, 'grad_norm': 3.640625, 'learning_rate': 9.010810810810812e-05, 'epoch': 4.95}

 10%|▉         | 183/1850 [12:13<1:52:25,  4.05s/it]
 10%|▉         | 184/1850 [12:17<1:52:10,  4.04s/it]
                                                    
{'loss': 0.5549, 'grad_norm': 3.6875, 'learning_rate': 9.005405405405405e-05, 'epoch': 4.97}

 10%|▉         | 184/1850 [12:17<1:52:10,  4.04s/it]
 10%|█         | 185/1850 [12:21<1:50:35,  3.99s/it]
                                                    
{'loss': 0.571, 'grad_norm': 4.15625, 'learning_rate': 9e-05, 'epoch': 5.0}

 10%|█         | 185/1850 [12:21<1:50:35,  3.99s/it]
 10%|█         | 186/1850 [12:25<1:51:17,  4.01s/it]
                                                    
{'loss': 0.3822, 'grad_norm': 3.046875, 'learning_rate': 8.994594594594596e-05, 'epoch': 5.03}

 10%|█         | 186/1850 [12:25<1:51:17,  4.01s/it]
 10%|█         | 187/1850 [12:29<1:53:46,  4.10s/it]
                                                    
{'loss': 0.3557, 'grad_norm': 3.359375, 'learning_rate': 8.98918918918919e-05, 'epoch': 5.05}

 10%|█         | 187/1850 [12:29<1:53:46,  4.10s/it]
 10%|█         | 188/1850 [12:34<1:57:56,  4.26s/it]
                                                    
{'loss': 0.3673, 'grad_norm': 3.015625, 'learning_rate': 8.983783783783785e-05, 'epoch': 5.08}

 10%|█         | 188/1850 [12:34<1:57:56,  4.26s/it]
 10%|█         | 189/1850 [12:38<1:54:12,  4.13s/it]
                                                    
{'loss': 0.2834, 'grad_norm': 3.203125, 'learning_rate': 8.97837837837838e-05, 'epoch': 5.11}

 10%|█         | 189/1850 [12:38<1:54:12,  4.13s/it]
 10%|█         | 190/1850 [12:42<1:50:16,  3.99s/it]
                                                    
{'loss': 0.268, 'grad_norm': 3.703125, 'learning_rate': 8.972972972972973e-05, 'epoch': 5.14}

 10%|█         | 190/1850 [12:42<1:50:16,  3.99s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 10%|█         | 191/1850 [12:45<1:48:55,  3.94s/it]
                                                    
{'loss': 0.2092, 'grad_norm': 4.21875, 'learning_rate': 8.967567567567569e-05, 'epoch': 5.16}

 10%|█         | 191/1850 [12:45<1:48:55,  3.94s/it]
 10%|█         | 192/1850 [12:50<1:51:56,  4.05s/it]
                                                    
{'loss': 0.3297, 'grad_norm': 4.3125, 'learning_rate': 8.962162162162162e-05, 'epoch': 5.19}

 10%|█         | 192/1850 [12:50<1:51:56,  4.05s/it]
 10%|█         | 193/1850 [12:54<1:51:24,  4.03s/it]
                                                    
{'loss': 0.2188, 'grad_norm': 4.15625, 'learning_rate': 8.956756756756756e-05, 'epoch': 5.22}

 10%|█         | 193/1850 [12:54<1:51:24,  4.03s/it]
 10%|█         | 194/1850 [12:57<1:48:55,  3.95s/it]
                                                    
{'loss': 0.2115, 'grad_norm': 4.375, 'learning_rate': 8.951351351351351e-05, 'epoch': 5.24}

 10%|█         | 194/1850 [12:57<1:48:55,  3.95s/it]
 11%|█         | 195/1850 [13:01<1:45:44,  3.83s/it]
                                                    
{'loss': 0.2679, 'grad_norm': 5.59375, 'learning_rate': 8.945945945945946e-05, 'epoch': 5.27}

 11%|█         | 195/1850 [13:01<1:45:44,  3.83s/it]
 11%|█         | 196/1850 [13:05<1:48:13,  3.93s/it]
                                                    
{'loss': 0.3521, 'grad_norm': 5.125, 'learning_rate': 8.94054054054054e-05, 'epoch': 5.3}

 11%|█         | 196/1850 [13:05<1:48:13,  3.93s/it]
 11%|█         | 197/1850 [13:09<1:48:56,  3.95s/it]
                                                    
{'loss': 0.2598, 'grad_norm': 4.84375, 'learning_rate': 8.935135135135135e-05, 'epoch': 5.32}

 11%|█         | 197/1850 [13:09<1:48:56,  3.95s/it]
 11%|█         | 198/1850 [13:13<1:49:06,  3.96s/it]
                                                    
{'loss': 0.2664, 'grad_norm': 4.5, 'learning_rate': 8.92972972972973e-05, 'epoch': 5.35}

 11%|█         | 198/1850 [13:13<1:49:06,  3.96s/it]
 11%|█         | 199/1850 [13:17<1:47:34,  3.91s/it]
                                                    
{'loss': 0.3109, 'grad_norm': 4.84375, 'learning_rate': 8.924324324324324e-05, 'epoch': 5.38}

 11%|█         | 199/1850 [13:17<1:47:34,  3.91s/it]
 11%|█         | 200/1850 [13:21<1:47:31,  3.91s/it]
                                                    
{'loss': 0.2439, 'grad_norm': 4.09375, 'learning_rate': 8.918918918918919e-05, 'epoch': 5.41}

 11%|█         | 200/1850 [13:21<1:47:31,  3.91s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 11%|█         | 201/1850 [13:26<1:53:41,  4.14s/it]
                                                    
{'loss': 0.3112, 'grad_norm': 4.15625, 'learning_rate': 8.913513513513514e-05, 'epoch': 5.43}

 11%|█         | 201/1850 [13:26<1:53:41,  4.14s/it]
 11%|█         | 202/1850 [13:30<1:57:24,  4.27s/it]
                                                    
{'loss': 0.2862, 'grad_norm': 3.953125, 'learning_rate': 8.908108108108108e-05, 'epoch': 5.46}

 11%|█         | 202/1850 [13:30<1:57:24,  4.27s/it]
 11%|█         | 203/1850 [13:34<1:54:10,  4.16s/it]
                                                    
{'loss': 0.2335, 'grad_norm': 3.65625, 'learning_rate': 8.902702702702703e-05, 'epoch': 5.49}

 11%|█         | 203/1850 [13:34<1:54:10,  4.16s/it]
 11%|█         | 204/1850 [13:38<1:56:27,  4.25s/it]
                                                    
{'loss': 0.2876, 'grad_norm': 4.15625, 'learning_rate': 8.897297297297298e-05, 'epoch': 5.51}

 11%|█         | 204/1850 [13:38<1:56:27,  4.25s/it]
 11%|█         | 205/1850 [13:44<2:04:50,  4.55s/it]
                                                    
{'loss': 0.5202, 'grad_norm': 4.40625, 'learning_rate': 8.891891891891892e-05, 'epoch': 5.54}

 11%|█         | 205/1850 [13:44<2:04:50,  4.55s/it]
 11%|█         | 206/1850 [13:49<2:08:07,  4.68s/it]
                                                    
{'loss': 0.3932, 'grad_norm': 3.71875, 'learning_rate': 8.886486486486487e-05, 'epoch': 5.57}

 11%|█         | 206/1850 [13:49<2:08:07,  4.68s/it]
 11%|█         | 207/1850 [13:52<1:56:57,  4.27s/it]
                                                    
{'loss': 0.1938, 'grad_norm': 4.1875, 'learning_rate': 8.881081081081082e-05, 'epoch': 5.59}

 11%|█         | 207/1850 [13:52<1:56:57,  4.27s/it]
 11%|█         | 208/1850 [13:56<1:53:20,  4.14s/it]
                                                    
{'loss': 0.2214, 'grad_norm': 3.375, 'learning_rate': 8.875675675675676e-05, 'epoch': 5.62}

 11%|█         | 208/1850 [13:56<1:53:20,  4.14s/it]
 11%|█▏        | 209/1850 [13:59<1:46:01,  3.88s/it]
                                                    
{'loss': 0.1914, 'grad_norm': 3.4375, 'learning_rate': 8.870270270270271e-05, 'epoch': 5.65}

 11%|█▏        | 209/1850 [13:59<1:46:01,  3.88s/it]
 11%|█▏        | 210/1850 [14:03<1:47:25,  3.93s/it]
                                                    
{'loss': 0.3541, 'grad_norm': 4.09375, 'learning_rate': 8.864864864864866e-05, 'epoch': 5.68}

 11%|█▏        | 210/1850 [14:03<1:47:25,  3.93s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 11%|█▏        | 211/1850 [14:08<1:54:27,  4.19s/it]
                                                    
{'loss': 0.3574, 'grad_norm': 4.34375, 'learning_rate': 8.85945945945946e-05, 'epoch': 5.7}

 11%|█▏        | 211/1850 [14:08<1:54:27,  4.19s/it]
 11%|█▏        | 212/1850 [14:12<1:55:16,  4.22s/it]
                                                    
{'loss': 0.3098, 'grad_norm': 3.640625, 'learning_rate': 8.854054054054054e-05, 'epoch': 5.73}

 11%|█▏        | 212/1850 [14:12<1:55:16,  4.22s/it]
 12%|█▏        | 213/1850 [14:16<1:53:24,  4.16s/it]
                                                    
{'loss': 0.2438, 'grad_norm': 3.71875, 'learning_rate': 8.848648648648649e-05, 'epoch': 5.76}

 12%|█▏        | 213/1850 [14:16<1:53:24,  4.16s/it]
 12%|█▏        | 214/1850 [14:21<1:56:19,  4.27s/it]
                                                    
{'loss': 0.369, 'grad_norm': 4.34375, 'learning_rate': 8.843243243243243e-05, 'epoch': 5.78}

 12%|█▏        | 214/1850 [14:21<1:56:19,  4.27s/it]
 12%|█▏        | 215/1850 [14:25<1:51:59,  4.11s/it]
                                                    
{'loss': 0.2561, 'grad_norm': 4.21875, 'learning_rate': 8.837837837837838e-05, 'epoch': 5.81}

 12%|█▏        | 215/1850 [14:25<1:51:59,  4.11s/it]
 12%|█▏        | 216/1850 [14:29<1:51:46,  4.10s/it]
                                                    
{'loss': 0.2397, 'grad_norm': 3.5625, 'learning_rate': 8.832432432432433e-05, 'epoch': 5.84}

 12%|█▏        | 216/1850 [14:29<1:51:46,  4.10s/it]
 12%|█▏        | 217/1850 [14:32<1:46:44,  3.92s/it]
                                                    
{'loss': 0.2036, 'grad_norm': 3.875, 'learning_rate': 8.827027027027027e-05, 'epoch': 5.86}

 12%|█▏        | 217/1850 [14:32<1:46:44,  3.92s/it]
 12%|█▏        | 218/1850 [14:35<1:41:21,  3.73s/it]
                                                    
{'loss': 0.159, 'grad_norm': 3.703125, 'learning_rate': 8.821621621621622e-05, 'epoch': 5.89}

 12%|█▏        | 218/1850 [14:35<1:41:21,  3.73s/it]
 12%|█▏        | 219/1850 [14:39<1:38:35,  3.63s/it]
                                                    
{'loss': 0.1785, 'grad_norm': 5.0, 'learning_rate': 8.816216216216217e-05, 'epoch': 5.92}

 12%|█▏        | 219/1850 [14:39<1:38:35,  3.63s/it]
 12%|█▏        | 220/1850 [14:43<1:42:09,  3.76s/it]
                                                    
{'loss': 0.2786, 'grad_norm': 4.25, 'learning_rate': 8.810810810810811e-05, 'epoch': 5.95}

 12%|█▏        | 220/1850 [14:43<1:42:09,  3.76s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 12%|█▏        | 221/1850 [14:47<1:45:34,  3.89s/it]
                                                    
{'loss': 0.1818, 'grad_norm': 4.15625, 'learning_rate': 8.805405405405406e-05, 'epoch': 5.97}

 12%|█▏        | 221/1850 [14:47<1:45:34,  3.89s/it]
 12%|█▏        | 222/1850 [14:50<1:39:01,  3.65s/it]
                                                    
{'loss': 0.215, 'grad_norm': 4.375, 'learning_rate': 8.800000000000001e-05, 'epoch': 6.0}

 12%|█▏        | 222/1850 [14:50<1:39:01,  3.65s/it]
 12%|█▏        | 223/1850 [14:53<1:34:38,  3.49s/it]
                                                    
{'loss': 0.093, 'grad_norm': 2.953125, 'learning_rate': 8.794594594594595e-05, 'epoch': 6.03}

 12%|█▏        | 223/1850 [14:53<1:34:38,  3.49s/it]
 12%|█▏        | 224/1850 [14:58<1:43:03,  3.80s/it]
                                                    
{'loss': 0.1704, 'grad_norm': 3.375, 'learning_rate': 8.78918918918919e-05, 'epoch': 6.05}

 12%|█▏        | 224/1850 [14:58<1:43:03,  3.80s/it]
 12%|█▏        | 225/1850 [15:02<1:46:44,  3.94s/it]
                                                    
{'loss': 0.1558, 'grad_norm': 3.21875, 'learning_rate': 8.783783783783784e-05, 'epoch': 6.08}

 12%|█▏        | 225/1850 [15:02<1:46:44,  3.94s/it]
 12%|█▏        | 226/1850 [15:05<1:39:26,  3.67s/it]
                                                    
{'loss': 0.0796, 'grad_norm': 2.671875, 'learning_rate': 8.778378378378379e-05, 'epoch': 6.11}

 12%|█▏        | 226/1850 [15:05<1:39:26,  3.67s/it]
 12%|█▏        | 227/1850 [15:10<1:45:41,  3.91s/it]
                                                    
{'loss': 0.1245, 'grad_norm': 3.046875, 'learning_rate': 8.772972972972974e-05, 'epoch': 6.14}

 12%|█▏        | 227/1850 [15:10<1:45:41,  3.91s/it]
 12%|█▏        | 228/1850 [15:14<1:47:54,  3.99s/it]
                                                    
{'loss': 0.1365, 'grad_norm': 3.015625, 'learning_rate': 8.767567567567568e-05, 'epoch': 6.16}

 12%|█▏        | 228/1850 [15:14<1:47:54,  3.99s/it]
 12%|█▏        | 229/1850 [15:17<1:45:35,  3.91s/it]
                                                    
{'loss': 0.0997, 'grad_norm': 3.171875, 'learning_rate': 8.762162162162163e-05, 'epoch': 6.19}

 12%|█▏        | 229/1850 [15:17<1:45:35,  3.91s/it]
 12%|█▏        | 230/1850 [15:21<1:42:39,  3.80s/it]
                                                    
{'loss': 0.1032, 'grad_norm': 3.484375, 'learning_rate': 8.756756756756758e-05, 'epoch': 6.22}

 12%|█▏        | 230/1850 [15:21<1:42:39,  3.80s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 12%|█▏        | 231/1850 [15:25<1:47:07,  3.97s/it]
                                                    
{'loss': 0.1199, 'grad_norm': 4.09375, 'learning_rate': 8.751351351351352e-05, 'epoch': 6.24}

 12%|█▏        | 231/1850 [15:25<1:47:07,  3.97s/it]
 13%|█▎        | 232/1850 [15:29<1:44:59,  3.89s/it]
                                                    
{'loss': 0.1111, 'grad_norm': 4.28125, 'learning_rate': 8.745945945945946e-05, 'epoch': 6.27}

 13%|█▎        | 232/1850 [15:29<1:44:59,  3.89s/it]
 13%|█▎        | 233/1850 [15:34<1:52:11,  4.16s/it]
                                                    
{'loss': 0.2253, 'grad_norm': 4.3125, 'learning_rate': 8.740540540540541e-05, 'epoch': 6.3}

 13%|█▎        | 233/1850 [15:34<1:52:11,  4.16s/it]
 13%|█▎        | 234/1850 [15:38<1:53:40,  4.22s/it]
                                                    
{'loss': 0.1695, 'grad_norm': 3.6875, 'learning_rate': 8.735135135135136e-05, 'epoch': 6.32}

 13%|█▎        | 234/1850 [15:38<1:53:40,  4.22s/it]
 13%|█▎        | 235/1850 [15:41<1:44:35,  3.89s/it]
                                                    
{'loss': 0.086, 'grad_norm': 4.34375, 'learning_rate': 8.72972972972973e-05, 'epoch': 6.35}

 13%|█▎        | 235/1850 [15:41<1:44:35,  3.89s/it]
 13%|█▎        | 236/1850 [15:45<1:40:00,  3.72s/it]
                                                    
{'loss': 0.0731, 'grad_norm': 3.25, 'learning_rate': 8.724324324324325e-05, 'epoch': 6.38}

 13%|█▎        | 236/1850 [15:45<1:40:00,  3.72s/it]
 13%|█▎        | 237/1850 [15:49<1:42:16,  3.80s/it]
                                                    
{'loss': 0.1449, 'grad_norm': 3.6875, 'learning_rate': 8.718918918918918e-05, 'epoch': 6.41}

 13%|█▎        | 237/1850 [15:49<1:42:16,  3.80s/it]
 13%|█▎        | 238/1850 [15:52<1:40:20,  3.73s/it]
                                                    
{'loss': 0.1169, 'grad_norm': 3.8125, 'learning_rate': 8.713513513513514e-05, 'epoch': 6.43}

 13%|█▎        | 238/1850 [15:52<1:40:20,  3.73s/it]
 13%|█▎        | 239/1850 [15:57<1:44:44,  3.90s/it]
                                                    
{'loss': 0.159, 'grad_norm': 4.40625, 'learning_rate': 8.708108108108109e-05, 'epoch': 6.46}

 13%|█▎        | 239/1850 [15:57<1:44:44,  3.90s/it]
 13%|█▎        | 240/1850 [16:00<1:44:37,  3.90s/it]
                                                    
{'loss': 0.1399, 'grad_norm': 4.5, 'learning_rate': 8.702702702702702e-05, 'epoch': 6.49}

 13%|█▎        | 240/1850 [16:00<1:44:37,  3.90s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 13%|█▎        | 241/1850 [16:05<1:53:14,  4.22s/it]
                                                    
{'loss': 0.1828, 'grad_norm': 3.21875, 'learning_rate': 8.697297297297298e-05, 'epoch': 6.51}

 13%|█▎        | 241/1850 [16:05<1:53:14,  4.22s/it]
 13%|█▎        | 242/1850 [16:10<1:53:55,  4.25s/it]
                                                    
{'loss': 0.1578, 'grad_norm': 4.0625, 'learning_rate': 8.691891891891893e-05, 'epoch': 6.54}

 13%|█▎        | 242/1850 [16:10<1:53:55,  4.25s/it]
 13%|█▎        | 243/1850 [16:13<1:48:12,  4.04s/it]
                                                    
{'loss': 0.1407, 'grad_norm': 3.765625, 'learning_rate': 8.686486486486487e-05, 'epoch': 6.57}

 13%|█▎        | 243/1850 [16:13<1:48:12,  4.04s/it]
 13%|█▎        | 244/1850 [16:17<1:43:45,  3.88s/it]
                                                    
{'loss': 0.0994, 'grad_norm': 3.515625, 'learning_rate': 8.681081081081082e-05, 'epoch': 6.59}

 13%|█▎        | 244/1850 [16:17<1:43:45,  3.88s/it]
 13%|█▎        | 245/1850 [16:20<1:42:02,  3.81s/it]
                                                    
{'loss': 0.1255, 'grad_norm': 4.5, 'learning_rate': 8.675675675675677e-05, 'epoch': 6.62}

 13%|█▎        | 245/1850 [16:20<1:42:02,  3.81s/it]
 13%|█▎        | 246/1850 [16:24<1:43:29,  3.87s/it]
                                                    
{'loss': 0.1017, 'grad_norm': 3.171875, 'learning_rate': 8.67027027027027e-05, 'epoch': 6.65}

 13%|█▎        | 246/1850 [16:24<1:43:29,  3.87s/it]
 13%|█▎        | 247/1850 [16:28<1:42:04,  3.82s/it]
                                                    
{'loss': 0.1585, 'grad_norm': 4.46875, 'learning_rate': 8.664864864864866e-05, 'epoch': 6.68}

 13%|█▎        | 247/1850 [16:28<1:42:04,  3.82s/it]
 13%|█▎        | 248/1850 [16:33<1:48:04,  4.05s/it]
                                                    
{'loss': 0.168, 'grad_norm': 3.234375, 'learning_rate': 8.659459459459461e-05, 'epoch': 6.7}

 13%|█▎        | 248/1850 [16:33<1:48:04,  4.05s/it]
 13%|█▎        | 249/1850 [16:37<1:48:57,  4.08s/it]
                                                    
{'loss': 0.1467, 'grad_norm': 3.5, 'learning_rate': 8.654054054054055e-05, 'epoch': 6.73}

 13%|█▎        | 249/1850 [16:37<1:48:57,  4.08s/it]
 14%|█▎        | 250/1850 [16:41<1:48:31,  4.07s/it]
                                                    
{'loss': 0.1477, 'grad_norm': 3.625, 'learning_rate': 8.64864864864865e-05, 'epoch': 6.76}

 14%|█▎        | 250/1850 [16:41<1:48:31,  4.07s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 14%|█▎        | 251/1850 [16:45<1:50:31,  4.15s/it]
                                                    
{'loss': 0.1059, 'grad_norm': 3.390625, 'learning_rate': 8.643243243243245e-05, 'epoch': 6.78}

 14%|█▎        | 251/1850 [16:45<1:50:31,  4.15s/it]
 14%|█▎        | 252/1850 [16:49<1:50:43,  4.16s/it]
                                                    
{'loss': 0.1414, 'grad_norm': 3.546875, 'learning_rate': 8.637837837837837e-05, 'epoch': 6.81}

 14%|█▎        | 252/1850 [16:49<1:50:43,  4.16s/it]
 14%|█▎        | 253/1850 [16:53<1:49:16,  4.11s/it]
                                                    
{'loss': 0.1568, 'grad_norm': 3.953125, 'learning_rate': 8.632432432432432e-05, 'epoch': 6.84}

 14%|█▎        | 253/1850 [16:53<1:49:16,  4.11s/it]
 14%|█▎        | 254/1850 [16:58<1:49:29,  4.12s/it]
                                                    
{'loss': 0.1023, 'grad_norm': 3.1875, 'learning_rate': 8.627027027027027e-05, 'epoch': 6.86}

 14%|█▎        | 254/1850 [16:58<1:49:29,  4.12s/it]
 14%|█▍        | 255/1850 [17:01<1:46:47,  4.02s/it]
                                                    
{'loss': 0.1567, 'grad_norm': 4.46875, 'learning_rate': 8.621621621621621e-05, 'epoch': 6.89}

 14%|█▍        | 255/1850 [17:01<1:46:47,  4.02s/it]
 14%|█▍        | 256/1850 [17:05<1:46:52,  4.02s/it]
                                                    
{'loss': 0.1597, 'grad_norm': 3.6875, 'learning_rate': 8.616216216216216e-05, 'epoch': 6.92}

 14%|█▍        | 256/1850 [17:05<1:46:52,  4.02s/it]
 14%|█▍        | 257/1850 [17:10<1:48:30,  4.09s/it]
                                                    
{'loss': 0.1447, 'grad_norm': 3.3125, 'learning_rate': 8.610810810810811e-05, 'epoch': 6.95}

 14%|█▍        | 257/1850 [17:10<1:48:30,  4.09s/it]
 14%|█▍        | 258/1850 [17:13<1:44:19,  3.93s/it]
                                                    
{'loss': 0.116, 'grad_norm': 4.1875, 'learning_rate': 8.605405405405405e-05, 'epoch': 6.97}

 14%|█▍        | 258/1850 [17:13<1:44:19,  3.93s/it]
 14%|█▍        | 259/1850 [17:16<1:38:49,  3.73s/it]
                                                    
{'loss': 0.118, 'grad_norm': 3.796875, 'learning_rate': 8.6e-05, 'epoch': 7.0}

 14%|█▍        | 259/1850 [17:16<1:38:49,  3.73s/it]
 14%|█▍        | 260/1850 [17:21<1:47:24,  4.05s/it]
                                                    
{'loss': 0.0727, 'grad_norm': 1.546875, 'learning_rate': 8.594594594594595e-05, 'epoch': 7.03}

 14%|█▍        | 260/1850 [17:21<1:47:24,  4.05s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 14%|█▍        | 261/1850 [17:26<1:49:51,  4.15s/it]
                                                    
{'loss': 0.0497, 'grad_norm': 1.9140625, 'learning_rate': 8.589189189189189e-05, 'epoch': 7.05}

 14%|█▍        | 261/1850 [17:26<1:49:51,  4.15s/it]
 14%|█▍        | 262/1850 [17:30<1:54:55,  4.34s/it]
                                                    
{'loss': 0.1032, 'grad_norm': 2.96875, 'learning_rate': 8.583783783783784e-05, 'epoch': 7.08}

 14%|█▍        | 262/1850 [17:30<1:54:55,  4.34s/it]
 14%|█▍        | 263/1850 [17:34<1:51:49,  4.23s/it]
                                                    
{'loss': 0.0536, 'grad_norm': 1.9921875, 'learning_rate': 8.57837837837838e-05, 'epoch': 7.11}

 14%|█▍        | 263/1850 [17:34<1:51:49,  4.23s/it]
 14%|█▍        | 264/1850 [17:38<1:49:57,  4.16s/it]
                                                    
{'loss': 0.0474, 'grad_norm': 2.21875, 'learning_rate': 8.572972972972973e-05, 'epoch': 7.14}

 14%|█▍        | 264/1850 [17:38<1:49:57,  4.16s/it]
 14%|█▍        | 265/1850 [17:42<1:45:07,  3.98s/it]
                                                    
{'loss': 0.0656, 'grad_norm': 3.390625, 'learning_rate': 8.567567567567568e-05, 'epoch': 7.16}

 14%|█▍        | 265/1850 [17:42<1:45:07,  3.98s/it]
 14%|█▍        | 266/1850 [17:46<1:46:35,  4.04s/it]
                                                    
{'loss': 0.0686, 'grad_norm': 3.109375, 'learning_rate': 8.562162162162163e-05, 'epoch': 7.19}

 14%|█▍        | 266/1850 [17:46<1:46:35,  4.04s/it]
 14%|█▍        | 267/1850 [17:51<1:49:55,  4.17s/it]
                                                    
{'loss': 0.0798, 'grad_norm': 3.09375, 'learning_rate': 8.556756756756757e-05, 'epoch': 7.22}

 14%|█▍        | 267/1850 [17:51<1:49:55,  4.17s/it]
 14%|█▍        | 268/1850 [17:55<1:48:33,  4.12s/it]
                                                    
{'loss': 0.0448, 'grad_norm': 2.375, 'learning_rate': 8.551351351351352e-05, 'epoch': 7.24}

 14%|█▍        | 268/1850 [17:55<1:48:33,  4.12s/it]
 15%|█▍        | 269/1850 [17:59<1:50:34,  4.20s/it]
                                                    
{'loss': 0.1085, 'grad_norm': 3.796875, 'learning_rate': 8.545945945945946e-05, 'epoch': 7.27}

 15%|█▍        | 269/1850 [17:59<1:50:34,  4.20s/it]
 15%|█▍        | 270/1850 [18:03<1:49:27,  4.16s/it]
                                                    
{'loss': 0.0753, 'grad_norm': 3.15625, 'learning_rate': 8.540540540540541e-05, 'epoch': 7.3}

 15%|█▍        | 270/1850 [18:03<1:49:27,  4.16s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 15%|█▍        | 271/1850 [18:07<1:49:33,  4.16s/it]
                                                    
{'loss': 0.0793, 'grad_norm': 3.859375, 'learning_rate': 8.535135135135136e-05, 'epoch': 7.32}

 15%|█▍        | 271/1850 [18:07<1:49:33,  4.16s/it]
 15%|█▍        | 272/1850 [18:11<1:45:26,  4.01s/it]
                                                    
{'loss': 0.0462, 'grad_norm': 2.984375, 'learning_rate': 8.52972972972973e-05, 'epoch': 7.35}

 15%|█▍        | 272/1850 [18:11<1:45:26,  4.01s/it]
 15%|█▍        | 273/1850 [18:15<1:43:43,  3.95s/it]
                                                    
{'loss': 0.062, 'grad_norm': 3.03125, 'learning_rate': 8.524324324324324e-05, 'epoch': 7.38}

 15%|█▍        | 273/1850 [18:15<1:43:43,  3.95s/it]
 15%|█▍        | 274/1850 [18:18<1:41:04,  3.85s/it]
                                                    
{'loss': 0.0609, 'grad_norm': 4.09375, 'learning_rate': 8.518918918918919e-05, 'epoch': 7.41}

 15%|█▍        | 274/1850 [18:18<1:41:04,  3.85s/it]
 15%|█▍        | 275/1850 [18:22<1:41:12,  3.86s/it]
                                                    
{'loss': 0.0683, 'grad_norm': 4.125, 'learning_rate': 8.513513513513514e-05, 'epoch': 7.43}

 15%|█▍        | 275/1850 [18:22<1:41:12,  3.86s/it]
 15%|█▍        | 276/1850 [18:26<1:40:38,  3.84s/it]
                                                    
{'loss': 0.0733, 'grad_norm': 3.40625, 'learning_rate': 8.508108108108108e-05, 'epoch': 7.46}

 15%|█▍        | 276/1850 [18:26<1:40:38,  3.84s/it]
 15%|█▍        | 277/1850 [18:30<1:42:04,  3.89s/it]
                                                    
{'loss': 0.0843, 'grad_norm': 3.390625, 'learning_rate': 8.502702702702703e-05, 'epoch': 7.49}

 15%|█▍        | 277/1850 [18:30<1:42:04,  3.89s/it]
 15%|█▌        | 278/1850 [18:33<1:37:46,  3.73s/it]
                                                    
{'loss': 0.0511, 'grad_norm': 3.25, 'learning_rate': 8.497297297297298e-05, 'epoch': 7.51}

 15%|█▌        | 278/1850 [18:33<1:37:46,  3.73s/it]
 15%|█▌        | 279/1850 [18:37<1:39:38,  3.81s/it]
                                                    
{'loss': 0.0573, 'grad_norm': 2.53125, 'learning_rate': 8.491891891891892e-05, 'epoch': 7.54}

 15%|█▌        | 279/1850 [18:37<1:39:38,  3.81s/it]
 15%|█▌        | 280/1850 [18:41<1:40:26,  3.84s/it]
                                                    
{'loss': 0.0652, 'grad_norm': 3.28125, 'learning_rate': 8.486486486486487e-05, 'epoch': 7.57}

 15%|█▌        | 280/1850 [18:41<1:40:26,  3.84s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 15%|█▌        | 281/1850 [18:45<1:38:52,  3.78s/it]
                                                    
{'loss': 0.054, 'grad_norm': 3.0625, 'learning_rate': 8.481081081081081e-05, 'epoch': 7.59}

 15%|█▌        | 281/1850 [18:45<1:38:52,  3.78s/it]
 15%|█▌        | 282/1850 [18:50<1:46:59,  4.09s/it]
                                                    
{'loss': 0.1166, 'grad_norm': 3.390625, 'learning_rate': 8.475675675675676e-05, 'epoch': 7.62}

 15%|█▌        | 282/1850 [18:50<1:46:59,  4.09s/it]
 15%|█▌        | 283/1850 [18:54<1:45:51,  4.05s/it]
                                                    
{'loss': 0.081, 'grad_norm': 3.21875, 'learning_rate': 8.470270270270271e-05, 'epoch': 7.65}

 15%|█▌        | 283/1850 [18:54<1:45:51,  4.05s/it]
 15%|█▌        | 284/1850 [18:58<1:45:48,  4.05s/it]
                                                    
{'loss': 0.0827, 'grad_norm': 2.828125, 'learning_rate': 8.464864864864865e-05, 'epoch': 7.68}

 15%|█▌        | 284/1850 [18:58<1:45:48,  4.05s/it]
 15%|█▌        | 285/1850 [19:01<1:40:38,  3.86s/it]
                                                    
{'loss': 0.0708, 'grad_norm': 3.40625, 'learning_rate': 8.45945945945946e-05, 'epoch': 7.7}

 15%|█▌        | 285/1850 [19:01<1:40:38,  3.86s/it]
 15%|█▌        | 286/1850 [19:05<1:38:53,  3.79s/it]
                                                    
{'loss': 0.0579, 'grad_norm': 2.921875, 'learning_rate': 8.454054054054055e-05, 'epoch': 7.73}

 15%|█▌        | 286/1850 [19:05<1:38:53,  3.79s/it]
 16%|█▌        | 287/1850 [19:09<1:40:00,  3.84s/it]
                                                    
{'loss': 0.069, 'grad_norm': 3.125, 'learning_rate': 8.448648648648649e-05, 'epoch': 7.76}

 16%|█▌        | 287/1850 [19:09<1:40:00,  3.84s/it]
 16%|█▌        | 288/1850 [19:12<1:38:40,  3.79s/it]
                                                    
{'loss': 0.0659, 'grad_norm': 2.828125, 'learning_rate': 8.443243243243244e-05, 'epoch': 7.78}

 16%|█▌        | 288/1850 [19:12<1:38:40,  3.79s/it]
 16%|█▌        | 289/1850 [19:17<1:43:36,  3.98s/it]
                                                    
{'loss': 0.1309, 'grad_norm': 4.34375, 'learning_rate': 8.437837837837839e-05, 'epoch': 7.81}

 16%|█▌        | 289/1850 [19:17<1:43:36,  3.98s/it]
 16%|█▌        | 290/1850 [19:20<1:40:49,  3.88s/it]
                                                    
{'loss': 0.0481, 'grad_norm': 3.03125, 'learning_rate': 8.432432432432433e-05, 'epoch': 7.84}

 16%|█▌        | 290/1850 [19:20<1:40:49,  3.88s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 16%|█▌        | 291/1850 [19:25<1:47:45,  4.15s/it]
                                                    
{'loss': 0.1133, 'grad_norm': 4.03125, 'learning_rate': 8.427027027027028e-05, 'epoch': 7.86}

 16%|█▌        | 291/1850 [19:25<1:47:45,  4.15s/it]
 16%|█▌        | 292/1850 [19:29<1:42:58,  3.97s/it]
                                                    
{'loss': 0.0996, 'grad_norm': 4.65625, 'learning_rate': 8.421621621621622e-05, 'epoch': 7.89}

 16%|█▌        | 292/1850 [19:29<1:42:58,  3.97s/it]
 16%|█▌        | 293/1850 [19:33<1:43:03,  3.97s/it]
                                                    
{'loss': 0.0654, 'grad_norm': 3.296875, 'learning_rate': 8.416216216216216e-05, 'epoch': 7.92}

 16%|█▌        | 293/1850 [19:33<1:43:03,  3.97s/it]
 16%|█▌        | 294/1850 [19:37<1:43:01,  3.97s/it]
                                                    
{'loss': 0.0864, 'grad_norm': 3.453125, 'learning_rate': 8.41081081081081e-05, 'epoch': 7.95}

 16%|█▌        | 294/1850 [19:37<1:43:01,  3.97s/it]
 16%|█▌        | 295/1850 [19:40<1:41:19,  3.91s/it]
                                                    
{'loss': 0.0617, 'grad_norm': 3.53125, 'learning_rate': 8.405405405405406e-05, 'epoch': 7.97}

 16%|█▌        | 295/1850 [19:40<1:41:19,  3.91s/it]
 16%|█▌        | 296/1850 [19:44<1:41:39,  3.93s/it]
                                                    
{'loss': 0.0771, 'grad_norm': 3.703125, 'learning_rate': 8.4e-05, 'epoch': 8.0}

 16%|█▌        | 296/1850 [19:44<1:41:39,  3.93s/it]
 16%|█▌        | 297/1850 [19:49<1:44:32,  4.04s/it]
                                                    
{'loss': 0.0396, 'grad_norm': 1.5625, 'learning_rate': 8.394594594594595e-05, 'epoch': 8.03}

 16%|█▌        | 297/1850 [19:49<1:44:32,  4.04s/it]
 16%|█▌        | 298/1850 [19:53<1:45:10,  4.07s/it]
                                                    
{'loss': 0.047, 'grad_norm': 2.234375, 'learning_rate': 8.38918918918919e-05, 'epoch': 8.05}

 16%|█▌        | 298/1850 [19:53<1:45:10,  4.07s/it]
 16%|█▌        | 299/1850 [19:57<1:44:10,  4.03s/it]
                                                    
{'loss': 0.0264, 'grad_norm': 1.546875, 'learning_rate': 8.383783783783784e-05, 'epoch': 8.08}

 16%|█▌        | 299/1850 [19:57<1:44:10,  4.03s/it]
 16%|█▌        | 300/1850 [20:02<1:49:31,  4.24s/it]
                                                    
{'loss': 0.0554, 'grad_norm': 1.984375, 'learning_rate': 8.378378378378379e-05, 'epoch': 8.11}

 16%|█▌        | 300/1850 [20:02<1:49:31,  4.24s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 16%|█▋        | 301/1850 [20:07<1:56:55,  4.53s/it]
                                                    
{'loss': 0.0393, 'grad_norm': 1.53125, 'learning_rate': 8.372972972972974e-05, 'epoch': 8.14}

 16%|█▋        | 301/1850 [20:07<1:56:55,  4.53s/it]
 16%|█▋        | 302/1850 [20:10<1:46:10,  4.12s/it]
                                                    
{'loss': 0.0296, 'grad_norm': 2.828125, 'learning_rate': 8.367567567567568e-05, 'epoch': 8.16}

 16%|█▋        | 302/1850 [20:10<1:46:10,  4.12s/it]
 16%|█▋        | 303/1850 [20:14<1:47:26,  4.17s/it]
                                                    
{'loss': 0.0489, 'grad_norm': 2.875, 'learning_rate': 8.362162162162163e-05, 'epoch': 8.19}

 16%|█▋        | 303/1850 [20:14<1:47:26,  4.17s/it]
 16%|█▋        | 304/1850 [20:18<1:44:20,  4.05s/it]
                                                    
{'loss': 0.0426, 'grad_norm': 2.046875, 'learning_rate': 8.356756756756758e-05, 'epoch': 8.22}

 16%|█▋        | 304/1850 [20:18<1:44:20,  4.05s/it]
 16%|█▋        | 305/1850 [20:22<1:44:14,  4.05s/it]
                                                    
{'loss': 0.0565, 'grad_norm': 3.65625, 'learning_rate': 8.351351351351352e-05, 'epoch': 8.24}

 16%|█▋        | 305/1850 [20:22<1:44:14,  4.05s/it]
 17%|█▋        | 306/1850 [20:25<1:38:18,  3.82s/it]
                                                    
{'loss': 0.0265, 'grad_norm': 2.375, 'learning_rate': 8.345945945945947e-05, 'epoch': 8.27}

 17%|█▋        | 306/1850 [20:25<1:38:18,  3.82s/it]
 17%|█▋        | 307/1850 [20:29<1:35:15,  3.70s/it]
                                                    
{'loss': 0.0305, 'grad_norm': 2.140625, 'learning_rate': 8.340540540540542e-05, 'epoch': 8.3}

 17%|█▋        | 307/1850 [20:29<1:35:15,  3.70s/it]
 17%|█▋        | 308/1850 [20:32<1:33:22,  3.63s/it]
                                                    
{'loss': 0.0305, 'grad_norm': 3.125, 'learning_rate': 8.335135135135136e-05, 'epoch': 8.32}

 17%|█▋        | 308/1850 [20:32<1:33:22,  3.63s/it]
 17%|█▋        | 309/1850 [20:36<1:32:01,  3.58s/it]
                                                    
{'loss': 0.0355, 'grad_norm': 3.265625, 'learning_rate': 8.329729729729731e-05, 'epoch': 8.35}

 17%|█▋        | 309/1850 [20:36<1:32:01,  3.58s/it]
 17%|█▋        | 310/1850 [20:40<1:35:36,  3.73s/it]
                                                    
{'loss': 0.0311, 'grad_norm': 2.203125, 'learning_rate': 8.324324324324326e-05, 'epoch': 8.38}

 17%|█▋        | 310/1850 [20:40<1:35:36,  3.73s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 17%|█▋        | 311/1850 [20:44<1:38:07,  3.83s/it]
                                                    
{'loss': 0.0445, 'grad_norm': 4.40625, 'learning_rate': 8.31891891891892e-05, 'epoch': 8.41}

 17%|█▋        | 311/1850 [20:44<1:38:07,  3.83s/it]
 17%|█▋        | 312/1850 [20:48<1:40:58,  3.94s/it]
                                                    
{'loss': 0.0573, 'grad_norm': 3.984375, 'learning_rate': 8.313513513513513e-05, 'epoch': 8.43}

 17%|█▋        | 312/1850 [20:48<1:40:58,  3.94s/it]
 17%|█▋        | 313/1850 [20:52<1:43:38,  4.05s/it]
                                                    
{'loss': 0.0703, 'grad_norm': 3.640625, 'learning_rate': 8.308108108108108e-05, 'epoch': 8.46}

 17%|█▋        | 313/1850 [20:52<1:43:38,  4.05s/it]
 17%|█▋        | 314/1850 [20:56<1:40:08,  3.91s/it]
                                                    
{'loss': 0.0513, 'grad_norm': 4.6875, 'learning_rate': 8.302702702702702e-05, 'epoch': 8.49}

 17%|█▋        | 314/1850 [20:56<1:40:08,  3.91s/it]
 17%|█▋        | 315/1850 [21:00<1:42:13,  4.00s/it]
                                                    
{'loss': 0.0653, 'grad_norm': 3.796875, 'learning_rate': 8.297297297297297e-05, 'epoch': 8.51}

 17%|█▋        | 315/1850 [21:00<1:42:13,  4.00s/it]
 17%|█▋        | 316/1850 [21:04<1:44:38,  4.09s/it]
                                                    
{'loss': 0.063, 'grad_norm': 3.625, 'learning_rate': 8.291891891891892e-05, 'epoch': 8.54}

 17%|█▋        | 316/1850 [21:04<1:44:38,  4.09s/it]
 17%|█▋        | 317/1850 [21:08<1:42:30,  4.01s/it]
                                                    
{'loss': 0.0651, 'grad_norm': 3.296875, 'learning_rate': 8.286486486486486e-05, 'epoch': 8.57}

 17%|█▋        | 317/1850 [21:08<1:42:30,  4.01s/it]
 17%|█▋        | 318/1850 [21:12<1:43:45,  4.06s/it]
                                                    
{'loss': 0.0487, 'grad_norm': 3.0, 'learning_rate': 8.281081081081081e-05, 'epoch': 8.59}

 17%|█▋        | 318/1850 [21:12<1:43:45,  4.06s/it]
 17%|█▋        | 319/1850 [21:17<1:49:11,  4.28s/it]
                                                    
{'loss': 0.0831, 'grad_norm': 3.609375, 'learning_rate': 8.275675675675677e-05, 'epoch': 8.62}

 17%|█▋        | 319/1850 [21:17<1:49:11,  4.28s/it]
 17%|█▋        | 320/1850 [21:20<1:37:47,  3.83s/it]
                                                    
{'loss': 0.0403, 'grad_norm': 3.625, 'learning_rate': 8.27027027027027e-05, 'epoch': 8.65}

 17%|█▋        | 320/1850 [21:20<1:37:47,  3.83s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 17%|█▋        | 321/1850 [21:24<1:41:46,  3.99s/it]
                                                    
{'loss': 0.0597, 'grad_norm': 2.671875, 'learning_rate': 8.264864864864865e-05, 'epoch': 8.68}

 17%|█▋        | 321/1850 [21:24<1:41:46,  3.99s/it]
 17%|█▋        | 322/1850 [21:29<1:43:16,  4.06s/it]
                                                    
{'loss': 0.0705, 'grad_norm': 3.484375, 'learning_rate': 8.25945945945946e-05, 'epoch': 8.7}

 17%|█▋        | 322/1850 [21:29<1:43:16,  4.06s/it]
 17%|█▋        | 323/1850 [21:33<1:43:02,  4.05s/it]
                                                    
{'loss': 0.0471, 'grad_norm': 2.609375, 'learning_rate': 8.254054054054054e-05, 'epoch': 8.73}

 17%|█▋        | 323/1850 [21:33<1:43:02,  4.05s/it]
 18%|█▊        | 324/1850 [21:36<1:37:06,  3.82s/it]
                                                    
{'loss': 0.033, 'grad_norm': 2.171875, 'learning_rate': 8.24864864864865e-05, 'epoch': 8.76}

 18%|█▊        | 324/1850 [21:36<1:37:06,  3.82s/it]
 18%|█▊        | 325/1850 [21:39<1:31:17,  3.59s/it]
                                                    
{'loss': 0.0549, 'grad_norm': 3.625, 'learning_rate': 8.243243243243243e-05, 'epoch': 8.78}

 18%|█▊        | 325/1850 [21:39<1:31:17,  3.59s/it]
 18%|█▊        | 326/1850 [21:43<1:34:50,  3.73s/it]
                                                    
{'loss': 0.0434, 'grad_norm': 2.296875, 'learning_rate': 8.237837837837838e-05, 'epoch': 8.81}

 18%|█▊        | 326/1850 [21:43<1:34:50,  3.73s/it]
 18%|█▊        | 327/1850 [21:47<1:39:36,  3.92s/it]
                                                    
{'loss': 0.067, 'grad_norm': 2.734375, 'learning_rate': 8.232432432432433e-05, 'epoch': 8.84}

 18%|█▊        | 327/1850 [21:47<1:39:36,  3.92s/it]
 18%|█▊        | 328/1850 [21:51<1:39:24,  3.92s/it]
                                                    
{'loss': 0.0443, 'grad_norm': 2.46875, 'learning_rate': 8.227027027027027e-05, 'epoch': 8.86}

 18%|█▊        | 328/1850 [21:51<1:39:24,  3.92s/it]
 18%|█▊        | 329/1850 [21:56<1:43:01,  4.06s/it]
                                                    
{'loss': 0.0513, 'grad_norm': 2.515625, 'learning_rate': 8.221621621621622e-05, 'epoch': 8.89}

 18%|█▊        | 329/1850 [21:56<1:43:01,  4.06s/it]
 18%|█▊        | 330/1850 [22:00<1:43:45,  4.10s/it]
                                                    
{'loss': 0.0471, 'grad_norm': 2.5625, 'learning_rate': 8.216216216216217e-05, 'epoch': 8.92}

 18%|█▊        | 330/1850 [22:00<1:43:45,  4.10s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 18%|█▊        | 331/1850 [22:04<1:47:42,  4.25s/it]
                                                    
{'loss': 0.0642, 'grad_norm': 3.453125, 'learning_rate': 8.210810810810811e-05, 'epoch': 8.95}

 18%|█▊        | 331/1850 [22:04<1:47:42,  4.25s/it]
 18%|█▊        | 332/1850 [22:09<1:46:45,  4.22s/it]
                                                    
{'loss': 0.0573, 'grad_norm': 3.375, 'learning_rate': 8.205405405405405e-05, 'epoch': 8.97}

 18%|█▊        | 332/1850 [22:09<1:46:45,  4.22s/it]
 18%|█▊        | 333/1850 [22:12<1:42:00,  4.03s/it]
                                                    
{'loss': 0.0403, 'grad_norm': 2.484375, 'learning_rate': 8.2e-05, 'epoch': 9.0}

 18%|█▊        | 333/1850 [22:12<1:42:00,  4.03s/it]
 18%|█▊        | 334/1850 [22:16<1:43:46,  4.11s/it]
                                                    
{'loss': 0.0399, 'grad_norm': 1.9609375, 'learning_rate': 8.194594594594595e-05, 'epoch': 9.03}

 18%|█▊        | 334/1850 [22:16<1:43:46,  4.11s/it]
 18%|█▊        | 335/1850 [22:20<1:42:43,  4.07s/it]
                                                    
{'loss': 0.0226, 'grad_norm': 1.9921875, 'learning_rate': 8.189189189189189e-05, 'epoch': 9.05}

 18%|█▊        | 335/1850 [22:20<1:42:43,  4.07s/it]
 18%|█▊        | 336/1850 [22:24<1:41:56,  4.04s/it]
                                                    
{'loss': 0.0195, 'grad_norm': 1.4921875, 'learning_rate': 8.183783783783784e-05, 'epoch': 9.08}

 18%|█▊        | 336/1850 [22:24<1:41:56,  4.04s/it]
 18%|█▊        | 337/1850 [22:28<1:41:55,  4.04s/it]
                                                    
{'loss': 0.0185, 'grad_norm': 1.1953125, 'learning_rate': 8.178378378378378e-05, 'epoch': 9.11}

 18%|█▊        | 337/1850 [22:28<1:41:55,  4.04s/it]
 18%|█▊        | 338/1850 [22:32<1:39:02,  3.93s/it]
                                                    
{'loss': 0.0239, 'grad_norm': 1.8125, 'learning_rate': 8.172972972972973e-05, 'epoch': 9.14}

 18%|█▊        | 338/1850 [22:32<1:39:02,  3.93s/it]
 18%|█▊        | 339/1850 [22:37<1:45:41,  4.20s/it]
                                                    
{'loss': 0.0323, 'grad_norm': 2.46875, 'learning_rate': 8.167567567567568e-05, 'epoch': 9.16}

 18%|█▊        | 339/1850 [22:37<1:45:41,  4.20s/it]
 18%|█▊        | 340/1850 [22:41<1:42:21,  4.07s/it]
                                                    
{'loss': 0.037, 'grad_norm': 3.203125, 'learning_rate': 8.162162162162162e-05, 'epoch': 9.19}

 18%|█▊        | 340/1850 [22:41<1:42:21,  4.07s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 18%|█▊        | 341/1850 [22:45<1:42:17,  4.07s/it]
                                                    
{'loss': 0.0337, 'grad_norm': 2.53125, 'learning_rate': 8.156756756756757e-05, 'epoch': 9.22}

 18%|█▊        | 341/1850 [22:45<1:42:17,  4.07s/it]
 18%|█▊        | 342/1850 [22:49<1:41:49,  4.05s/it]
                                                    
{'loss': 0.0225, 'grad_norm': 1.859375, 'learning_rate': 8.151351351351352e-05, 'epoch': 9.24}

 18%|█▊        | 342/1850 [22:49<1:41:49,  4.05s/it]
 19%|█▊        | 343/1850 [22:53<1:41:10,  4.03s/it]
                                                    
{'loss': 0.07, 'grad_norm': 4.5625, 'learning_rate': 8.145945945945946e-05, 'epoch': 9.27}

 19%|█▊        | 343/1850 [22:53<1:41:10,  4.03s/it]
 19%|█▊        | 344/1850 [22:58<1:48:29,  4.32s/it]
                                                    
{'loss': 0.0451, 'grad_norm': 3.25, 'learning_rate': 8.140540540540541e-05, 'epoch': 9.3}

 19%|█▊        | 344/1850 [22:58<1:48:29,  4.32s/it]
 19%|█▊        | 345/1850 [23:02<1:44:30,  4.17s/it]
                                                    
{'loss': 0.0144, 'grad_norm': 1.1328125, 'learning_rate': 8.135135135135136e-05, 'epoch': 9.32}

 19%|█▊        | 345/1850 [23:02<1:44:30,  4.17s/it]
 19%|█▊        | 346/1850 [23:06<1:44:38,  4.17s/it]
                                                    
{'loss': 0.0532, 'grad_norm': 3.296875, 'learning_rate': 8.12972972972973e-05, 'epoch': 9.35}

 19%|█▊        | 346/1850 [23:06<1:44:38,  4.17s/it]
 19%|█▉        | 347/1850 [23:10<1:46:48,  4.26s/it]
                                                    
{'loss': 0.0453, 'grad_norm': 3.4375, 'learning_rate': 8.124324324324325e-05, 'epoch': 9.38}

 19%|█▉        | 347/1850 [23:10<1:46:48,  4.26s/it]
 19%|█▉        | 348/1850 [23:14<1:42:10,  4.08s/it]
                                                    
{'loss': 0.0464, 'grad_norm': 2.671875, 'learning_rate': 8.11891891891892e-05, 'epoch': 9.41}

 19%|█▉        | 348/1850 [23:14<1:42:10,  4.08s/it]
 19%|█▉        | 349/1850 [23:18<1:42:49,  4.11s/it]
                                                    
{'loss': 0.0393, 'grad_norm': 3.78125, 'learning_rate': 8.113513513513514e-05, 'epoch': 9.43}

 19%|█▉        | 349/1850 [23:18<1:42:49,  4.11s/it]
 19%|█▉        | 350/1850 [23:21<1:37:22,  3.89s/it]
                                                    
{'loss': 0.0349, 'grad_norm': 2.6875, 'learning_rate': 8.108108108108109e-05, 'epoch': 9.46}

 19%|█▉        | 350/1850 [23:21<1:37:22,  3.89s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 19%|█▉        | 351/1850 [23:26<1:41:44,  4.07s/it]
                                                    
{'loss': 0.042, 'grad_norm': 2.703125, 'learning_rate': 8.102702702702704e-05, 'epoch': 9.49}

 19%|█▉        | 351/1850 [23:26<1:41:44,  4.07s/it]
 19%|█▉        | 352/1850 [23:30<1:40:13,  4.01s/it]
                                                    
{'loss': 0.0378, 'grad_norm': 2.796875, 'learning_rate': 8.097297297297297e-05, 'epoch': 9.51}

 19%|█▉        | 352/1850 [23:30<1:40:13,  4.01s/it]
 19%|█▉        | 353/1850 [23:33<1:36:47,  3.88s/it]
                                                    
{'loss': 0.0309, 'grad_norm': 2.75, 'learning_rate': 8.091891891891892e-05, 'epoch': 9.54}

 19%|█▉        | 353/1850 [23:33<1:36:47,  3.88s/it]
 19%|█▉        | 354/1850 [23:37<1:35:17,  3.82s/it]
                                                    
{'loss': 0.0425, 'grad_norm': 3.03125, 'learning_rate': 8.086486486486487e-05, 'epoch': 9.57}

 19%|█▉        | 354/1850 [23:37<1:35:17,  3.82s/it]
 19%|█▉        | 355/1850 [23:40<1:31:17,  3.66s/it]
                                                    
{'loss': 0.0455, 'grad_norm': 4.03125, 'learning_rate': 8.08108108108108e-05, 'epoch': 9.59}

 19%|█▉        | 355/1850 [23:40<1:31:17,  3.66s/it]
 19%|█▉        | 356/1850 [23:43<1:26:29,  3.47s/it]
                                                    
{'loss': 0.0253, 'grad_norm': 2.109375, 'learning_rate': 8.075675675675676e-05, 'epoch': 9.62}

 19%|█▉        | 356/1850 [23:43<1:26:29,  3.47s/it]
 19%|█▉        | 357/1850 [23:47<1:29:44,  3.61s/it]
                                                    
{'loss': 0.0393, 'grad_norm': 2.59375, 'learning_rate': 8.070270270270271e-05, 'epoch': 9.65}

 19%|█▉        | 357/1850 [23:47<1:29:44,  3.61s/it]
 19%|█▉        | 358/1850 [23:52<1:35:01,  3.82s/it]
                                                    
{'loss': 0.0488, 'grad_norm': 2.21875, 'learning_rate': 8.064864864864865e-05, 'epoch': 9.68}

 19%|█▉        | 358/1850 [23:52<1:35:01,  3.82s/it]
 19%|█▉        | 359/1850 [23:55<1:34:36,  3.81s/it]
                                                    
{'loss': 0.0435, 'grad_norm': 2.6875, 'learning_rate': 8.05945945945946e-05, 'epoch': 9.7}

 19%|█▉        | 359/1850 [23:55<1:34:36,  3.81s/it]
 19%|█▉        | 360/1850 [24:00<1:42:17,  4.12s/it]
                                                    
{'loss': 0.0547, 'grad_norm': 2.796875, 'learning_rate': 8.054054054054055e-05, 'epoch': 9.73}

 19%|█▉        | 360/1850 [24:00<1:42:17,  4.12s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 20%|█▉        | 361/1850 [24:05<1:43:54,  4.19s/it]
                                                    
{'loss': 0.0392, 'grad_norm': 2.234375, 'learning_rate': 8.048648648648649e-05, 'epoch': 9.76}

 20%|█▉        | 361/1850 [24:05<1:43:54,  4.19s/it]
 20%|█▉        | 362/1850 [24:09<1:44:28,  4.21s/it]
                                                    
{'loss': 0.0548, 'grad_norm': 2.515625, 'learning_rate': 8.043243243243244e-05, 'epoch': 9.78}

 20%|█▉        | 362/1850 [24:09<1:44:28,  4.21s/it]
 20%|█▉        | 363/1850 [24:13<1:40:54,  4.07s/it]
                                                    
{'loss': 0.0379, 'grad_norm': 2.96875, 'learning_rate': 8.037837837837839e-05, 'epoch': 9.81}

 20%|█▉        | 363/1850 [24:13<1:40:54,  4.07s/it]
 20%|█▉        | 364/1850 [24:17<1:39:49,  4.03s/it]
                                                    
{'loss': 0.031, 'grad_norm': 2.125, 'learning_rate': 8.032432432432433e-05, 'epoch': 9.84}

 20%|█▉        | 364/1850 [24:17<1:39:49,  4.03s/it]
 20%|█▉        | 365/1850 [24:21<1:40:07,  4.05s/it]
                                                    
{'loss': 0.0335, 'grad_norm': 2.234375, 'learning_rate': 8.027027027027028e-05, 'epoch': 9.86}

 20%|█▉        | 365/1850 [24:21<1:40:07,  4.05s/it]
 20%|█▉        | 366/1850 [24:24<1:33:58,  3.80s/it]
                                                    
{'loss': 0.0556, 'grad_norm': 4.09375, 'learning_rate': 8.021621621621623e-05, 'epoch': 9.89}

 20%|█▉        | 366/1850 [24:24<1:33:58,  3.80s/it]
 20%|█▉        | 367/1850 [24:28<1:34:20,  3.82s/it]
                                                    
{'loss': 0.0448, 'grad_norm': 3.125, 'learning_rate': 8.016216216216217e-05, 'epoch': 9.92}

 20%|█▉        | 367/1850 [24:28<1:34:20,  3.82s/it]
 20%|█▉        | 368/1850 [24:31<1:31:20,  3.70s/it]
                                                    
{'loss': 0.0617, 'grad_norm': 3.703125, 'learning_rate': 8.010810810810812e-05, 'epoch': 9.95}

 20%|█▉        | 368/1850 [24:31<1:31:20,  3.70s/it]
 20%|█▉        | 369/1850 [24:35<1:33:04,  3.77s/it]
                                                    
{'loss': 0.039, 'grad_norm': 2.5625, 'learning_rate': 8.005405405405406e-05, 'epoch': 9.97}

 20%|█▉        | 369/1850 [24:35<1:33:04,  3.77s/it]
 20%|██        | 370/1850 [24:39<1:32:59,  3.77s/it]
                                                    
{'loss': 0.0381, 'grad_norm': 2.3125, 'learning_rate': 8e-05, 'epoch': 10.0}

 20%|██        | 370/1850 [24:39<1:32:59,  3.77s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 20%|██        | 371/1850 [24:42<1:28:28,  3.59s/it]
                                                    
{'loss': 0.0163, 'grad_norm': 1.5859375, 'learning_rate': 7.994594594594596e-05, 'epoch': 10.03}

 20%|██        | 371/1850 [24:42<1:28:28,  3.59s/it]
 20%|██        | 372/1850 [24:46<1:31:07,  3.70s/it]
                                                    
{'loss': 0.0164, 'grad_norm': 1.7109375, 'learning_rate': 7.98918918918919e-05, 'epoch': 10.05}

 20%|██        | 372/1850 [24:46<1:31:07,  3.70s/it]
 20%|██        | 373/1850 [24:50<1:31:59,  3.74s/it]
                                                    
{'loss': 0.0151, 'grad_norm': 1.2890625, 'learning_rate': 7.983783783783783e-05, 'epoch': 10.08}

 20%|██        | 373/1850 [24:50<1:31:59,  3.74s/it]
 20%|██        | 374/1850 [24:53<1:31:34,  3.72s/it]
                                                    
{'loss': 0.0217, 'grad_norm': 1.8515625, 'learning_rate': 7.978378378378378e-05, 'epoch': 10.11}

 20%|██        | 374/1850 [24:53<1:31:34,  3.72s/it]
 20%|██        | 375/1850 [24:57<1:30:50,  3.70s/it]
                                                    
{'loss': 0.0451, 'grad_norm': 2.75, 'learning_rate': 7.972972972972974e-05, 'epoch': 10.14}

 20%|██        | 375/1850 [24:57<1:30:50,  3.70s/it]
 20%|██        | 376/1850 [25:01<1:35:28,  3.89s/it]
                                                    
{'loss': 0.0254, 'grad_norm': 1.9609375, 'learning_rate': 7.967567567567567e-05, 'epoch': 10.16}

 20%|██        | 376/1850 [25:01<1:35:28,  3.89s/it]
 20%|██        | 377/1850 [25:05<1:36:10,  3.92s/it]
                                                    
{'loss': 0.0191, 'grad_norm': 1.4765625, 'learning_rate': 7.962162162162162e-05, 'epoch': 10.19}

 20%|██        | 377/1850 [25:05<1:36:10,  3.92s/it]
 20%|██        | 378/1850 [25:09<1:35:09,  3.88s/it]
                                                    
{'loss': 0.0361, 'grad_norm': 3.109375, 'learning_rate': 7.956756756756758e-05, 'epoch': 10.22}

 20%|██        | 378/1850 [25:09<1:35:09,  3.88s/it]
 20%|██        | 379/1850 [25:13<1:34:42,  3.86s/it]
                                                    
{'loss': 0.0214, 'grad_norm': 2.65625, 'learning_rate': 7.951351351351351e-05, 'epoch': 10.24}

 20%|██        | 379/1850 [25:13<1:34:42,  3.86s/it]
 21%|██        | 380/1850 [25:17<1:35:15,  3.89s/it]
                                                    
{'loss': 0.018, 'grad_norm': 1.4921875, 'learning_rate': 7.945945945945946e-05, 'epoch': 10.27}

 21%|██        | 380/1850 [25:17<1:35:15,  3.89s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 21%|██        | 381/1850 [25:21<1:33:15,  3.81s/it]
                                                    
{'loss': 0.0161, 'grad_norm': 1.6171875, 'learning_rate': 7.94054054054054e-05, 'epoch': 10.3}

 21%|██        | 381/1850 [25:21<1:33:15,  3.81s/it]
 21%|██        | 382/1850 [25:25<1:37:51,  4.00s/it]
                                                    
{'loss': 0.0249, 'grad_norm': 1.421875, 'learning_rate': 7.935135135135135e-05, 'epoch': 10.32}

 21%|██        | 382/1850 [25:25<1:37:51,  4.00s/it]
 21%|██        | 383/1850 [25:29<1:39:50,  4.08s/it]
                                                    
{'loss': 0.0273, 'grad_norm': 2.375, 'learning_rate': 7.92972972972973e-05, 'epoch': 10.35}

 21%|██        | 383/1850 [25:29<1:39:50,  4.08s/it]
 21%|██        | 384/1850 [25:33<1:37:50,  4.00s/it]
                                                    
{'loss': 0.0412, 'grad_norm': 3.6875, 'learning_rate': 7.924324324324324e-05, 'epoch': 10.38}

 21%|██        | 384/1850 [25:33<1:37:50,  4.00s/it]
 21%|██        | 385/1850 [25:37<1:39:05,  4.06s/it]
                                                    
{'loss': 0.0249, 'grad_norm': 2.109375, 'learning_rate': 7.91891891891892e-05, 'epoch': 10.41}

 21%|██        | 385/1850 [25:37<1:39:05,  4.06s/it]
 21%|██        | 386/1850 [25:41<1:38:39,  4.04s/it]
                                                    
{'loss': 0.0174, 'grad_norm': 1.8671875, 'learning_rate': 7.913513513513514e-05, 'epoch': 10.43}

 21%|██        | 386/1850 [25:41<1:38:39,  4.04s/it]
 21%|██        | 387/1850 [25:46<1:40:13,  4.11s/it]
                                                    
{'loss': 0.0383, 'grad_norm': 2.59375, 'learning_rate': 7.908108108108108e-05, 'epoch': 10.46}

 21%|██        | 387/1850 [25:46<1:40:13,  4.11s/it]
 21%|██        | 388/1850 [25:50<1:41:23,  4.16s/it]
                                                    
{'loss': 0.0509, 'grad_norm': 4.0625, 'learning_rate': 7.902702702702703e-05, 'epoch': 10.49}

 21%|██        | 388/1850 [25:50<1:41:23,  4.16s/it]
 21%|██        | 389/1850 [25:54<1:39:25,  4.08s/it]
                                                    
{'loss': 0.0326, 'grad_norm': 2.859375, 'learning_rate': 7.897297297297298e-05, 'epoch': 10.51}

 21%|██        | 389/1850 [25:54<1:39:25,  4.08s/it]
 21%|██        | 390/1850 [25:58<1:39:57,  4.11s/it]
                                                    
{'loss': 0.0396, 'grad_norm': 4.09375, 'learning_rate': 7.891891891891892e-05, 'epoch': 10.54}

 21%|██        | 390/1850 [25:58<1:39:57,  4.11s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 21%|██        | 391/1850 [26:02<1:40:52,  4.15s/it]
                                                    
{'loss': 0.0362, 'grad_norm': 2.375, 'learning_rate': 7.886486486486487e-05, 'epoch': 10.57}

 21%|██        | 391/1850 [26:02<1:40:52,  4.15s/it]
 21%|██        | 392/1850 [26:06<1:36:20,  3.96s/it]
                                                    
{'loss': 0.0223, 'grad_norm': 2.484375, 'learning_rate': 7.881081081081083e-05, 'epoch': 10.59}

 21%|██        | 392/1850 [26:06<1:36:20,  3.96s/it]
 21%|██        | 393/1850 [26:10<1:38:12,  4.04s/it]
                                                    
{'loss': 0.0196, 'grad_norm': 1.4453125, 'learning_rate': 7.875675675675675e-05, 'epoch': 10.62}

 21%|██        | 393/1850 [26:10<1:38:12,  4.04s/it]
 21%|██▏       | 394/1850 [26:14<1:35:51,  3.95s/it]
                                                    
{'loss': 0.023, 'grad_norm': 2.203125, 'learning_rate': 7.87027027027027e-05, 'epoch': 10.65}

 21%|██▏       | 394/1850 [26:14<1:35:51,  3.95s/it]
 21%|██▏       | 395/1850 [26:18<1:34:59,  3.92s/it]
                                                    
{'loss': 0.0284, 'grad_norm': 2.015625, 'learning_rate': 7.864864864864865e-05, 'epoch': 10.68}

 21%|██▏       | 395/1850 [26:18<1:34:59,  3.92s/it]
 21%|██▏       | 396/1850 [26:21<1:33:58,  3.88s/it]
                                                    
{'loss': 0.0326, 'grad_norm': 2.21875, 'learning_rate': 7.859459459459459e-05, 'epoch': 10.7}

 21%|██▏       | 396/1850 [26:21<1:33:58,  3.88s/it]
 21%|██▏       | 397/1850 [26:25<1:31:51,  3.79s/it]
                                                    
{'loss': 0.0179, 'grad_norm': 2.140625, 'learning_rate': 7.854054054054054e-05, 'epoch': 10.73}

 21%|██▏       | 397/1850 [26:25<1:31:51,  3.79s/it]
 22%|██▏       | 398/1850 [26:29<1:35:34,  3.95s/it]
                                                    
{'loss': 0.0321, 'grad_norm': 2.453125, 'learning_rate': 7.848648648648649e-05, 'epoch': 10.76}

 22%|██▏       | 398/1850 [26:29<1:35:34,  3.95s/it]
 22%|██▏       | 399/1850 [26:33<1:36:30,  3.99s/it]
                                                    
{'loss': 0.0288, 'grad_norm': 2.046875, 'learning_rate': 7.843243243243243e-05, 'epoch': 10.78}

 22%|██▏       | 399/1850 [26:33<1:36:30,  3.99s/it]
 22%|██▏       | 400/1850 [26:38<1:37:52,  4.05s/it]
                                                    
{'loss': 0.0377, 'grad_norm': 2.3125, 'learning_rate': 7.837837837837838e-05, 'epoch': 10.81}

 22%|██▏       | 400/1850 [26:38<1:37:52,  4.05s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 22%|██▏       | 401/1850 [26:43<1:46:10,  4.40s/it]
                                                    
{'loss': 0.0328, 'grad_norm': 2.171875, 'learning_rate': 7.832432432432433e-05, 'epoch': 10.84}

 22%|██▏       | 401/1850 [26:43<1:46:10,  4.40s/it]
 22%|██▏       | 402/1850 [26:47<1:43:46,  4.30s/it]
                                                    
{'loss': 0.0322, 'grad_norm': 2.484375, 'learning_rate': 7.827027027027027e-05, 'epoch': 10.86}

 22%|██▏       | 402/1850 [26:47<1:43:46,  4.30s/it]
 22%|██▏       | 403/1850 [26:51<1:39:35,  4.13s/it]
                                                    
{'loss': 0.0283, 'grad_norm': 2.703125, 'learning_rate': 7.821621621621622e-05, 'epoch': 10.89}

 22%|██▏       | 403/1850 [26:51<1:39:35,  4.13s/it]
 22%|██▏       | 404/1850 [26:55<1:39:39,  4.14s/it]
                                                    
{'loss': 0.0476, 'grad_norm': 3.25, 'learning_rate': 7.816216216216217e-05, 'epoch': 10.92}

 22%|██▏       | 404/1850 [26:55<1:39:39,  4.14s/it]
 22%|██▏       | 405/1850 [26:59<1:39:03,  4.11s/it]
                                                    
{'loss': 0.0432, 'grad_norm': 3.671875, 'learning_rate': 7.810810810810811e-05, 'epoch': 10.95}

 22%|██▏       | 405/1850 [26:59<1:39:03,  4.11s/it]
 22%|██▏       | 406/1850 [27:03<1:41:42,  4.23s/it]
                                                    
{'loss': 0.0384, 'grad_norm': 2.890625, 'learning_rate': 7.805405405405406e-05, 'epoch': 10.97}

 22%|██▏       | 406/1850 [27:03<1:41:42,  4.23s/it]
 22%|██▏       | 407/1850 [27:07<1:39:16,  4.13s/it]
                                                    
{'loss': 0.0366, 'grad_norm': 2.578125, 'learning_rate': 7.800000000000001e-05, 'epoch': 11.0}

 22%|██▏       | 407/1850 [27:07<1:39:16,  4.13s/it]
 22%|██▏       | 408/1850 [27:11<1:38:43,  4.11s/it]
                                                    
{'loss': 0.0174, 'grad_norm': 1.71875, 'learning_rate': 7.794594594594595e-05, 'epoch': 11.03}

 22%|██▏       | 408/1850 [27:11<1:38:43,  4.11s/it]
 22%|██▏       | 409/1850 [27:15<1:33:27,  3.89s/it]
                                                    
{'loss': 0.0114, 'grad_norm': 1.28125, 'learning_rate': 7.78918918918919e-05, 'epoch': 11.05}

 22%|██▏       | 409/1850 [27:15<1:33:27,  3.89s/it]
 22%|██▏       | 410/1850 [27:19<1:37:58,  4.08s/it]
                                                    
{'loss': 0.0155, 'grad_norm': 1.7109375, 'learning_rate': 7.783783783783785e-05, 'epoch': 11.08}

 22%|██▏       | 410/1850 [27:19<1:37:58,  4.08s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 22%|██▏       | 411/1850 [27:24<1:40:57,  4.21s/it]
                                                    
{'loss': 0.026, 'grad_norm': 3.328125, 'learning_rate': 7.778378378378379e-05, 'epoch': 11.11}

 22%|██▏       | 411/1850 [27:24<1:40:57,  4.21s/it]
 22%|██▏       | 412/1850 [27:27<1:35:14,  3.97s/it]
                                                    
{'loss': 0.0126, 'grad_norm': 1.8203125, 'learning_rate': 7.772972972972974e-05, 'epoch': 11.14}

 22%|██▏       | 412/1850 [27:27<1:35:14,  3.97s/it]
 22%|██▏       | 413/1850 [27:31<1:37:56,  4.09s/it]
                                                    
{'loss': 0.0218, 'grad_norm': 2.34375, 'learning_rate': 7.767567567567568e-05, 'epoch': 11.16}

 22%|██▏       | 413/1850 [27:31<1:37:56,  4.09s/it]
 22%|██▏       | 414/1850 [27:36<1:39:32,  4.16s/it]
                                                    
{'loss': 0.0529, 'grad_norm': 3.03125, 'learning_rate': 7.762162162162162e-05, 'epoch': 11.19}

 22%|██▏       | 414/1850 [27:36<1:39:32,  4.16s/it]
 22%|██▏       | 415/1850 [27:40<1:42:15,  4.28s/it]
                                                    
{'loss': 0.0274, 'grad_norm': 1.8671875, 'learning_rate': 7.756756756756757e-05, 'epoch': 11.22}

 22%|██▏       | 415/1850 [27:40<1:42:15,  4.28s/it]
 22%|██▏       | 416/1850 [27:44<1:39:08,  4.15s/it]
                                                    
{'loss': 0.0164, 'grad_norm': 1.71875, 'learning_rate': 7.751351351351352e-05, 'epoch': 11.24}

 22%|██▏       | 416/1850 [27:44<1:39:08,  4.15s/it]
 23%|██▎       | 417/1850 [27:47<1:32:47,  3.89s/it]
                                                    
{'loss': 0.0214, 'grad_norm': 2.03125, 'learning_rate': 7.745945945945946e-05, 'epoch': 11.27}

 23%|██▎       | 417/1850 [27:47<1:32:47,  3.89s/it]
 23%|██▎       | 418/1850 [27:51<1:34:00,  3.94s/it]
                                                    
{'loss': 0.0191, 'grad_norm': 2.484375, 'learning_rate': 7.740540540540541e-05, 'epoch': 11.3}

 23%|██▎       | 418/1850 [27:51<1:34:00,  3.94s/it]
 23%|██▎       | 419/1850 [27:55<1:34:13,  3.95s/it]
                                                    
{'loss': 0.0232, 'grad_norm': 2.8125, 'learning_rate': 7.735135135135136e-05, 'epoch': 11.32}

 23%|██▎       | 419/1850 [27:55<1:34:13,  3.95s/it]
 23%|██▎       | 420/1850 [28:00<1:40:25,  4.21s/it]
                                                    
{'loss': 0.0217, 'grad_norm': 2.125, 'learning_rate': 7.72972972972973e-05, 'epoch': 11.35}

 23%|██▎       | 420/1850 [28:00<1:40:25,  4.21s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 23%|██▎       | 421/1850 [28:04<1:36:36,  4.06s/it]
                                                    
{'loss': 0.0237, 'grad_norm': 3.125, 'learning_rate': 7.724324324324325e-05, 'epoch': 11.38}

 23%|██▎       | 421/1850 [28:04<1:36:36,  4.06s/it]
 23%|██▎       | 422/1850 [28:07<1:32:18,  3.88s/it]
                                                    
{'loss': 0.0213, 'grad_norm': 2.984375, 'learning_rate': 7.71891891891892e-05, 'epoch': 11.41}

 23%|██▎       | 422/1850 [28:07<1:32:18,  3.88s/it]
 23%|██▎       | 423/1850 [28:12<1:36:41,  4.07s/it]
                                                    
{'loss': 0.017, 'grad_norm': 1.7421875, 'learning_rate': 7.713513513513514e-05, 'epoch': 11.43}

 23%|██▎       | 423/1850 [28:12<1:36:41,  4.07s/it]
 23%|██▎       | 424/1850 [28:15<1:32:44,  3.90s/it]
                                                    
{'loss': 0.0211, 'grad_norm': 2.453125, 'learning_rate': 7.708108108108109e-05, 'epoch': 11.46}

 23%|██▎       | 424/1850 [28:15<1:32:44,  3.90s/it]
 23%|██▎       | 425/1850 [28:19<1:29:58,  3.79s/it]
                                                    
{'loss': 0.0339, 'grad_norm': 4.0, 'learning_rate': 7.702702702702703e-05, 'epoch': 11.49}

 23%|██▎       | 425/1850 [28:19<1:29:58,  3.79s/it]
 23%|██▎       | 426/1850 [28:23<1:31:41,  3.86s/it]
                                                    
{'loss': 0.0352, 'grad_norm': 2.765625, 'learning_rate': 7.697297297297298e-05, 'epoch': 11.51}

 23%|██▎       | 426/1850 [28:23<1:31:41,  3.86s/it]
 23%|██▎       | 427/1850 [28:27<1:30:50,  3.83s/it]
                                                    
{'loss': 0.0142, 'grad_norm': 1.8515625, 'learning_rate': 7.691891891891893e-05, 'epoch': 11.54}

 23%|██▎       | 427/1850 [28:27<1:30:50,  3.83s/it]
 23%|██▎       | 428/1850 [28:31<1:31:10,  3.85s/it]
                                                    
{'loss': 0.0215, 'grad_norm': 2.28125, 'learning_rate': 7.686486486486487e-05, 'epoch': 11.57}

 23%|██▎       | 428/1850 [28:31<1:31:10,  3.85s/it]
 23%|██▎       | 429/1850 [28:34<1:27:51,  3.71s/it]
                                                    
{'loss': 0.0202, 'grad_norm': 2.703125, 'learning_rate': 7.681081081081082e-05, 'epoch': 11.59}

 23%|██▎       | 429/1850 [28:34<1:27:51,  3.71s/it]
 23%|██▎       | 430/1850 [28:39<1:34:03,  3.97s/it]
                                                    
{'loss': 0.0247, 'grad_norm': 2.71875, 'learning_rate': 7.675675675675677e-05, 'epoch': 11.62}

 23%|██▎       | 430/1850 [28:39<1:34:03,  3.97s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 23%|██▎       | 431/1850 [28:44<1:43:19,  4.37s/it]
                                                    
{'loss': 0.0274, 'grad_norm': 2.1875, 'learning_rate': 7.67027027027027e-05, 'epoch': 11.65}

 23%|██▎       | 431/1850 [28:44<1:43:19,  4.37s/it]
 23%|██▎       | 432/1850 [28:48<1:41:51,  4.31s/it]
                                                    
{'loss': 0.032, 'grad_norm': 2.453125, 'learning_rate': 7.664864864864866e-05, 'epoch': 11.68}

 23%|██▎       | 432/1850 [28:48<1:41:51,  4.31s/it]
 23%|██▎       | 433/1850 [28:52<1:38:52,  4.19s/it]
                                                    
{'loss': 0.0202, 'grad_norm': 2.4375, 'learning_rate': 7.65945945945946e-05, 'epoch': 11.7}

 23%|██▎       | 433/1850 [28:52<1:38:52,  4.19s/it]
 23%|██▎       | 434/1850 [28:56<1:36:04,  4.07s/it]
                                                    
{'loss': 0.0256, 'grad_norm': 2.78125, 'learning_rate': 7.654054054054055e-05, 'epoch': 11.73}

 23%|██▎       | 434/1850 [28:56<1:36:04,  4.07s/it]
 24%|██▎       | 435/1850 [29:00<1:33:41,  3.97s/it]
                                                    
{'loss': 0.0291, 'grad_norm': 2.8125, 'learning_rate': 7.648648648648648e-05, 'epoch': 11.76}

 24%|██▎       | 435/1850 [29:00<1:33:41,  3.97s/it]
 24%|██▎       | 436/1850 [29:03<1:33:15,  3.96s/it]
                                                    
{'loss': 0.0488, 'grad_norm': 4.3125, 'learning_rate': 7.643243243243244e-05, 'epoch': 11.78}

 24%|██▎       | 436/1850 [29:03<1:33:15,  3.96s/it]
 24%|██▎       | 437/1850 [29:08<1:37:56,  4.16s/it]
                                                    
{'loss': 0.0358, 'grad_norm': 2.265625, 'learning_rate': 7.637837837837837e-05, 'epoch': 11.81}

 24%|██▎       | 437/1850 [29:08<1:37:56,  4.16s/it]
 24%|██▎       | 438/1850 [29:12<1:32:56,  3.95s/it]
                                                    
{'loss': 0.0143, 'grad_norm': 1.203125, 'learning_rate': 7.632432432432432e-05, 'epoch': 11.84}

 24%|██▎       | 438/1850 [29:12<1:32:56,  3.95s/it]
 24%|██▎       | 439/1850 [29:15<1:30:23,  3.84s/it]
                                                    
{'loss': 0.0288, 'grad_norm': 3.125, 'learning_rate': 7.627027027027028e-05, 'epoch': 11.86}

 24%|██▎       | 439/1850 [29:15<1:30:23,  3.84s/it]
 24%|██▍       | 440/1850 [29:19<1:31:22,  3.89s/it]
                                                    
{'loss': 0.042, 'grad_norm': 2.890625, 'learning_rate': 7.621621621621621e-05, 'epoch': 11.89}

 24%|██▍       | 440/1850 [29:19<1:31:22,  3.89s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 24%|██▍       | 441/1850 [29:24<1:37:51,  4.17s/it]
                                                    
{'loss': 0.035, 'grad_norm': 2.921875, 'learning_rate': 7.616216216216216e-05, 'epoch': 11.92}

 24%|██▍       | 441/1850 [29:24<1:37:51,  4.17s/it]
 24%|██▍       | 442/1850 [29:28<1:35:53,  4.09s/it]
                                                    
{'loss': 0.0254, 'grad_norm': 1.890625, 'learning_rate': 7.610810810810812e-05, 'epoch': 11.95}

 24%|██▍       | 442/1850 [29:28<1:35:53,  4.09s/it]
 24%|██▍       | 443/1850 [29:32<1:35:34,  4.08s/it]
                                                    
{'loss': 0.0359, 'grad_norm': 2.859375, 'learning_rate': 7.605405405405405e-05, 'epoch': 11.97}

 24%|██▍       | 443/1850 [29:32<1:35:34,  4.08s/it]
 24%|██▍       | 444/1850 [29:36<1:33:44,  4.00s/it]
                                                    
{'loss': 0.0501, 'grad_norm': 3.078125, 'learning_rate': 7.6e-05, 'epoch': 12.0}

 24%|██▍       | 444/1850 [29:36<1:33:44,  4.00s/it]
 24%|██▍       | 445/1850 [29:41<1:39:21,  4.24s/it]
                                                    
{'loss': 0.0134, 'grad_norm': 1.2734375, 'learning_rate': 7.594594594594596e-05, 'epoch': 12.03}

 24%|██▍       | 445/1850 [29:41<1:39:21,  4.24s/it]
 24%|██▍       | 446/1850 [29:44<1:32:48,  3.97s/it]
                                                    
{'loss': 0.0167, 'grad_norm': 1.6640625, 'learning_rate': 7.589189189189189e-05, 'epoch': 12.05}

 24%|██▍       | 446/1850 [29:44<1:32:48,  3.97s/it]
 24%|██▍       | 447/1850 [29:48<1:37:03,  4.15s/it]
                                                    
{'loss': 0.0278, 'grad_norm': 1.9765625, 'learning_rate': 7.583783783783784e-05, 'epoch': 12.08}

 24%|██▍       | 447/1850 [29:48<1:37:03,  4.15s/it]
 24%|██▍       | 448/1850 [29:53<1:38:04,  4.20s/it]
                                                    
{'loss': 0.016, 'grad_norm': 1.703125, 'learning_rate': 7.57837837837838e-05, 'epoch': 12.11}

 24%|██▍       | 448/1850 [29:53<1:38:04,  4.20s/it]
 24%|██▍       | 449/1850 [29:56<1:31:31,  3.92s/it]
                                                    
{'loss': 0.0108, 'grad_norm': 1.484375, 'learning_rate': 7.572972972972973e-05, 'epoch': 12.14}

 24%|██▍       | 449/1850 [29:56<1:31:31,  3.92s/it]
 24%|██▍       | 450/1850 [30:00<1:30:18,  3.87s/it]
                                                    
{'loss': 0.0098, 'grad_norm': 1.1484375, 'learning_rate': 7.567567567567568e-05, 'epoch': 12.16}

 24%|██▍       | 450/1850 [30:00<1:30:18,  3.87s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 24%|██▍       | 451/1850 [30:04<1:33:19,  4.00s/it]
                                                    
{'loss': 0.0216, 'grad_norm': 1.6171875, 'learning_rate': 7.562162162162164e-05, 'epoch': 12.19}

 24%|██▍       | 451/1850 [30:04<1:33:19,  4.00s/it]
 24%|██▍       | 452/1850 [30:09<1:40:30,  4.31s/it]
                                                    
{'loss': 0.0144, 'grad_norm': 1.171875, 'learning_rate': 7.556756756756757e-05, 'epoch': 12.22}

 24%|██▍       | 452/1850 [30:09<1:40:30,  4.31s/it]
 24%|██▍       | 453/1850 [30:13<1:35:29,  4.10s/it]
                                                    
{'loss': 0.0254, 'grad_norm': 3.296875, 'learning_rate': 7.551351351351351e-05, 'epoch': 12.24}

 24%|██▍       | 453/1850 [30:13<1:35:29,  4.10s/it]
 25%|██▍       | 454/1850 [30:16<1:29:52,  3.86s/it]
                                                    
{'loss': 0.013, 'grad_norm': 1.9140625, 'learning_rate': 7.545945945945946e-05, 'epoch': 12.27}

 25%|██▍       | 454/1850 [30:16<1:29:52,  3.86s/it]
 25%|██▍       | 455/1850 [30:20<1:31:39,  3.94s/it]
                                                    
{'loss': 0.0151, 'grad_norm': 1.5, 'learning_rate': 7.54054054054054e-05, 'epoch': 12.3}

 25%|██▍       | 455/1850 [30:20<1:31:39,  3.94s/it]
 25%|██▍       | 456/1850 [30:24<1:30:11,  3.88s/it]
                                                    
{'loss': 0.0178, 'grad_norm': 2.90625, 'learning_rate': 7.535135135135135e-05, 'epoch': 12.32}

 25%|██▍       | 456/1850 [30:24<1:30:11,  3.88s/it]
 25%|██▍       | 457/1850 [30:28<1:28:25,  3.81s/it]
                                                    
{'loss': 0.0223, 'grad_norm': 2.234375, 'learning_rate': 7.52972972972973e-05, 'epoch': 12.35}

 25%|██▍       | 457/1850 [30:28<1:28:25,  3.81s/it]
 25%|██▍       | 458/1850 [30:31<1:26:59,  3.75s/it]
                                                    
{'loss': 0.0254, 'grad_norm': 3.34375, 'learning_rate': 7.524324324324324e-05, 'epoch': 12.38}

 25%|██▍       | 458/1850 [30:31<1:26:59,  3.75s/it]
 25%|██▍       | 459/1850 [30:35<1:26:52,  3.75s/it]
                                                    
{'loss': 0.0167, 'grad_norm': 2.234375, 'learning_rate': 7.518918918918919e-05, 'epoch': 12.41}

 25%|██▍       | 459/1850 [30:35<1:26:52,  3.75s/it]
 25%|██▍       | 460/1850 [30:39<1:28:33,  3.82s/it]
                                                    
{'loss': 0.0154, 'grad_norm': 1.1796875, 'learning_rate': 7.513513513513514e-05, 'epoch': 12.43}

 25%|██▍       | 460/1850 [30:39<1:28:33,  3.82s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 25%|██▍       | 461/1850 [30:43<1:30:21,  3.90s/it]
                                                    
{'loss': 0.0159, 'grad_norm': 1.3828125, 'learning_rate': 7.508108108108108e-05, 'epoch': 12.46}

 25%|██▍       | 461/1850 [30:43<1:30:21,  3.90s/it]
 25%|██▍       | 462/1850 [30:47<1:28:10,  3.81s/it]
                                                    
{'loss': 0.028, 'grad_norm': 2.890625, 'learning_rate': 7.502702702702703e-05, 'epoch': 12.49}

 25%|██▍       | 462/1850 [30:47<1:28:10,  3.81s/it]
 25%|██▌       | 463/1850 [30:51<1:30:15,  3.90s/it]
                                                    
{'loss': 0.0252, 'grad_norm': 2.671875, 'learning_rate': 7.497297297297298e-05, 'epoch': 12.51}

 25%|██▌       | 463/1850 [30:51<1:30:15,  3.90s/it]
 25%|██▌       | 464/1850 [30:55<1:30:14,  3.91s/it]
                                                    
{'loss': 0.028, 'grad_norm': 3.015625, 'learning_rate': 7.491891891891892e-05, 'epoch': 12.54}

 25%|██▌       | 464/1850 [30:55<1:30:14,  3.91s/it]
 25%|██▌       | 465/1850 [30:58<1:28:39,  3.84s/it]
                                                    
{'loss': 0.0316, 'grad_norm': 2.484375, 'learning_rate': 7.486486486486487e-05, 'epoch': 12.57}

 25%|██▌       | 465/1850 [30:58<1:28:39,  3.84s/it]
 25%|██▌       | 466/1850 [31:02<1:27:52,  3.81s/it]
                                                    
{'loss': 0.0151, 'grad_norm': 2.0625, 'learning_rate': 7.481081081081082e-05, 'epoch': 12.59}

 25%|██▌       | 466/1850 [31:02<1:27:52,  3.81s/it]
 25%|██▌       | 467/1850 [31:06<1:28:49,  3.85s/it]
                                                    
{'loss': 0.0193, 'grad_norm': 1.90625, 'learning_rate': 7.475675675675676e-05, 'epoch': 12.62}

 25%|██▌       | 467/1850 [31:06<1:28:49,  3.85s/it]
 25%|██▌       | 468/1850 [31:10<1:32:22,  4.01s/it]
                                                    
{'loss': 0.0251, 'grad_norm': 2.1875, 'learning_rate': 7.470270270270271e-05, 'epoch': 12.65}

 25%|██▌       | 468/1850 [31:10<1:32:22,  4.01s/it]
 25%|██▌       | 469/1850 [31:14<1:31:34,  3.98s/it]
                                                    
{'loss': 0.0229, 'grad_norm': 2.734375, 'learning_rate': 7.464864864864865e-05, 'epoch': 12.68}

 25%|██▌       | 469/1850 [31:14<1:31:34,  3.98s/it]
 25%|██▌       | 470/1850 [31:18<1:31:33,  3.98s/it]
                                                    
{'loss': 0.0298, 'grad_norm': 2.34375, 'learning_rate': 7.45945945945946e-05, 'epoch': 12.7}

 25%|██▌       | 470/1850 [31:18<1:31:33,  3.98s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 25%|██▌       | 471/1850 [31:23<1:35:13,  4.14s/it]
                                                    
{'loss': 0.0463, 'grad_norm': 3.765625, 'learning_rate': 7.454054054054055e-05, 'epoch': 12.73}

 25%|██▌       | 471/1850 [31:23<1:35:13,  4.14s/it]
 26%|██▌       | 472/1850 [31:26<1:29:30,  3.90s/it]
                                                    
{'loss': 0.0166, 'grad_norm': 1.8828125, 'learning_rate': 7.448648648648649e-05, 'epoch': 12.76}

 26%|██▌       | 472/1850 [31:26<1:29:30,  3.90s/it]
 26%|██▌       | 473/1850 [31:31<1:36:30,  4.20s/it]
                                                    
{'loss': 0.0216, 'grad_norm': 1.71875, 'learning_rate': 7.443243243243243e-05, 'epoch': 12.78}

 26%|██▌       | 473/1850 [31:31<1:36:30,  4.20s/it]
 26%|██▌       | 474/1850 [31:36<1:38:36,  4.30s/it]
                                                    
{'loss': 0.0147, 'grad_norm': 1.015625, 'learning_rate': 7.437837837837838e-05, 'epoch': 12.81}

 26%|██▌       | 474/1850 [31:36<1:38:36,  4.30s/it]
 26%|██▌       | 475/1850 [31:40<1:38:51,  4.31s/it]
                                                    
{'loss': 0.0263, 'grad_norm': 2.75, 'learning_rate': 7.432432432432433e-05, 'epoch': 12.84}

 26%|██▌       | 475/1850 [31:40<1:38:51,  4.31s/it]
 26%|██▌       | 476/1850 [31:43<1:33:56,  4.10s/it]
                                                    
{'loss': 0.0284, 'grad_norm': 2.0625, 'learning_rate': 7.427027027027027e-05, 'epoch': 12.86}

 26%|██▌       | 476/1850 [31:43<1:33:56,  4.10s/it]
 26%|██▌       | 477/1850 [31:48<1:35:00,  4.15s/it]
                                                    
{'loss': 0.022, 'grad_norm': 1.484375, 'learning_rate': 7.421621621621622e-05, 'epoch': 12.89}

 26%|██▌       | 477/1850 [31:48<1:35:00,  4.15s/it]
 26%|██▌       | 478/1850 [31:52<1:34:20,  4.13s/it]
                                                    
{'loss': 0.0479, 'grad_norm': 3.34375, 'learning_rate': 7.416216216216217e-05, 'epoch': 12.92}

 26%|██▌       | 478/1850 [31:52<1:34:20,  4.13s/it]
 26%|██▌       | 479/1850 [31:56<1:34:14,  4.12s/it]
                                                    
{'loss': 0.0212, 'grad_norm': 1.8046875, 'learning_rate': 7.410810810810811e-05, 'epoch': 12.95}

 26%|██▌       | 479/1850 [31:56<1:34:14,  4.12s/it]
 26%|██▌       | 480/1850 [32:00<1:32:41,  4.06s/it]
                                                    
{'loss': 0.0267, 'grad_norm': 2.515625, 'learning_rate': 7.405405405405406e-05, 'epoch': 12.97}

 26%|██▌       | 480/1850 [32:00<1:32:41,  4.06s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 26%|██▌       | 481/1850 [32:04<1:32:20,  4.05s/it]
                                                    
{'loss': 0.0188, 'grad_norm': 1.8984375, 'learning_rate': 7.4e-05, 'epoch': 13.0}

 26%|██▌       | 481/1850 [32:04<1:32:20,  4.05s/it]
 26%|██▌       | 482/1850 [32:07<1:28:00,  3.86s/it]
                                                    
{'loss': 0.0093, 'grad_norm': 1.5859375, 'learning_rate': 7.394594594594595e-05, 'epoch': 13.03}

 26%|██▌       | 482/1850 [32:07<1:28:00,  3.86s/it]
 26%|██▌       | 483/1850 [32:11<1:28:10,  3.87s/it]
                                                    
{'loss': 0.0154, 'grad_norm': 1.96875, 'learning_rate': 7.38918918918919e-05, 'epoch': 13.05}

 26%|██▌       | 483/1850 [32:11<1:28:10,  3.87s/it]
 26%|██▌       | 484/1850 [32:15<1:25:47,  3.77s/it]
                                                    
{'loss': 0.0119, 'grad_norm': 1.6171875, 'learning_rate': 7.383783783783784e-05, 'epoch': 13.08}

 26%|██▌       | 484/1850 [32:15<1:25:47,  3.77s/it]
 26%|██▌       | 485/1850 [32:18<1:22:48,  3.64s/it]
                                                    
{'loss': 0.0186, 'grad_norm': 3.375, 'learning_rate': 7.378378378378379e-05, 'epoch': 13.11}

 26%|██▌       | 485/1850 [32:18<1:22:48,  3.64s/it]
 26%|██▋       | 486/1850 [32:22<1:25:44,  3.77s/it]
                                                    
{'loss': 0.0114, 'grad_norm': 1.0859375, 'learning_rate': 7.372972972972974e-05, 'epoch': 13.14}

 26%|██▋       | 486/1850 [32:22<1:25:44,  3.77s/it]
 26%|██▋       | 487/1850 [32:26<1:25:00,  3.74s/it]
                                                    
{'loss': 0.008, 'grad_norm': 0.6015625, 'learning_rate': 7.367567567567568e-05, 'epoch': 13.16}

 26%|██▋       | 487/1850 [32:26<1:25:00,  3.74s/it]
 26%|██▋       | 488/1850 [32:29<1:22:49,  3.65s/it]
                                                    
{'loss': 0.013, 'grad_norm': 1.7265625, 'learning_rate': 7.362162162162163e-05, 'epoch': 13.19}

 26%|██▋       | 488/1850 [32:29<1:22:49,  3.65s/it]
 26%|██▋       | 489/1850 [32:34<1:30:38,  4.00s/it]
                                                    
{'loss': 0.044, 'grad_norm': 2.515625, 'learning_rate': 7.356756756756758e-05, 'epoch': 13.22}

 26%|██▋       | 489/1850 [32:34<1:30:38,  4.00s/it]
 26%|██▋       | 490/1850 [32:38<1:32:08,  4.07s/it]
                                                    
{'loss': 0.0113, 'grad_norm': 1.1953125, 'learning_rate': 7.351351351351352e-05, 'epoch': 13.24}

 26%|██▋       | 490/1850 [32:38<1:32:08,  4.07s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 27%|██▋       | 491/1850 [32:42<1:31:53,  4.06s/it]
                                                    
{'loss': 0.0148, 'grad_norm': 1.609375, 'learning_rate': 7.345945945945947e-05, 'epoch': 13.27}

 27%|██▋       | 491/1850 [32:42<1:31:53,  4.06s/it]
 27%|██▋       | 492/1850 [32:47<1:33:11,  4.12s/it]
                                                    
{'loss': 0.0183, 'grad_norm': 1.5390625, 'learning_rate': 7.340540540540542e-05, 'epoch': 13.3}

 27%|██▋       | 492/1850 [32:47<1:33:11,  4.12s/it]
 27%|██▋       | 493/1850 [32:51<1:34:46,  4.19s/it]
                                                    
{'loss': 0.0234, 'grad_norm': 2.09375, 'learning_rate': 7.335135135135134e-05, 'epoch': 13.32}

 27%|██▋       | 493/1850 [32:51<1:34:46,  4.19s/it]
 27%|██▋       | 494/1850 [32:54<1:28:54,  3.93s/it]
                                                    
{'loss': 0.007, 'grad_norm': 0.73046875, 'learning_rate': 7.32972972972973e-05, 'epoch': 13.35}

 27%|██▋       | 494/1850 [32:54<1:28:54,  3.93s/it]
 27%|██▋       | 495/1850 [32:58<1:28:37,  3.92s/it]
                                                    
{'loss': 0.0112, 'grad_norm': 1.296875, 'learning_rate': 7.324324324324325e-05, 'epoch': 13.38}

 27%|██▋       | 495/1850 [32:58<1:28:37,  3.92s/it]
 27%|██▋       | 496/1850 [33:03<1:33:28,  4.14s/it]
                                                    
{'loss': 0.0135, 'grad_norm': 1.8046875, 'learning_rate': 7.318918918918918e-05, 'epoch': 13.41}

 27%|██▋       | 496/1850 [33:03<1:33:28,  4.14s/it]
 27%|██▋       | 497/1850 [33:07<1:30:42,  4.02s/it]
                                                    
{'loss': 0.0146, 'grad_norm': 1.84375, 'learning_rate': 7.313513513513513e-05, 'epoch': 13.43}

 27%|██▋       | 497/1850 [33:07<1:30:42,  4.02s/it]
 27%|██▋       | 498/1850 [33:12<1:39:19,  4.41s/it]
                                                    
{'loss': 0.0217, 'grad_norm': 1.859375, 'learning_rate': 7.308108108108109e-05, 'epoch': 13.46}

 27%|██▋       | 498/1850 [33:12<1:39:19,  4.41s/it]
 27%|██▋       | 499/1850 [33:16<1:39:26,  4.42s/it]
                                                    
{'loss': 0.0171, 'grad_norm': 1.5390625, 'learning_rate': 7.302702702702702e-05, 'epoch': 13.49}

 27%|██▋       | 499/1850 [33:16<1:39:26,  4.42s/it]
 27%|██▋       | 500/1850 [33:20<1:35:36,  4.25s/it]
                                                    
{'loss': 0.012, 'grad_norm': 1.75, 'learning_rate': 7.297297297297297e-05, 'epoch': 13.51}

 27%|██▋       | 500/1850 [33:20<1:35:36,  4.25s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 27%|██▋       | 501/1850 [33:25<1:37:10,  4.32s/it]
                                                    
{'loss': 0.0158, 'grad_norm': 1.8671875, 'learning_rate': 7.291891891891893e-05, 'epoch': 13.54}

 27%|██▋       | 501/1850 [33:25<1:37:10,  4.32s/it]
 27%|██▋       | 502/1850 [33:28<1:32:00,  4.10s/it]
                                                    
{'loss': 0.0169, 'grad_norm': 2.0625, 'learning_rate': 7.286486486486486e-05, 'epoch': 13.57}

 27%|██▋       | 502/1850 [33:28<1:32:00,  4.10s/it]
 27%|██▋       | 503/1850 [33:32<1:33:02,  4.14s/it]
                                                    
{'loss': 0.0157, 'grad_norm': 1.5859375, 'learning_rate': 7.281081081081081e-05, 'epoch': 13.59}

 27%|██▋       | 503/1850 [33:32<1:33:02,  4.14s/it]
 27%|██▋       | 504/1850 [33:36<1:30:34,  4.04s/it]
                                                    
{'loss': 0.0349, 'grad_norm': 2.5, 'learning_rate': 7.275675675675677e-05, 'epoch': 13.62}

 27%|██▋       | 504/1850 [33:36<1:30:34,  4.04s/it]
 27%|██▋       | 505/1850 [33:40<1:31:12,  4.07s/it]
                                                    
{'loss': 0.0121, 'grad_norm': 1.4140625, 'learning_rate': 7.27027027027027e-05, 'epoch': 13.65}

 27%|██▋       | 505/1850 [33:40<1:31:12,  4.07s/it]
 27%|██▋       | 506/1850 [33:44<1:26:22,  3.86s/it]
                                                    
{'loss': 0.0179, 'grad_norm': 1.4921875, 'learning_rate': 7.264864864864865e-05, 'epoch': 13.68}

 27%|██▋       | 506/1850 [33:44<1:26:22,  3.86s/it]
 27%|██▋       | 507/1850 [33:48<1:26:21,  3.86s/it]
                                                    
{'loss': 0.0307, 'grad_norm': 3.03125, 'learning_rate': 7.25945945945946e-05, 'epoch': 13.7}

 27%|██▋       | 507/1850 [33:48<1:26:21,  3.86s/it]
 27%|██▋       | 508/1850 [33:52<1:29:26,  4.00s/it]
                                                    
{'loss': 0.0134, 'grad_norm': 1.6328125, 'learning_rate': 7.254054054054054e-05, 'epoch': 13.73}

 27%|██▋       | 508/1850 [33:52<1:29:26,  4.00s/it]
 28%|██▊       | 509/1850 [33:56<1:26:33,  3.87s/it]
                                                    
{'loss': 0.0234, 'grad_norm': 3.5, 'learning_rate': 7.24864864864865e-05, 'epoch': 13.76}

 28%|██▊       | 509/1850 [33:56<1:26:33,  3.87s/it]
 28%|██▊       | 510/1850 [34:00<1:32:19,  4.13s/it]
                                                    
{'loss': 0.0304, 'grad_norm': 2.25, 'learning_rate': 7.243243243243245e-05, 'epoch': 13.78}

 28%|██▊       | 510/1850 [34:00<1:32:19,  4.13s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 28%|██▊       | 511/1850 [34:05<1:34:12,  4.22s/it]
                                                    
{'loss': 0.0296, 'grad_norm': 2.453125, 'learning_rate': 7.237837837837838e-05, 'epoch': 13.81}

 28%|██▊       | 511/1850 [34:05<1:34:12,  4.22s/it]
 28%|██▊       | 512/1850 [34:08<1:29:55,  4.03s/it]
                                                    
{'loss': 0.0156, 'grad_norm': 2.03125, 'learning_rate': 7.232432432432434e-05, 'epoch': 13.84}

 28%|██▊       | 512/1850 [34:08<1:29:55,  4.03s/it]
 28%|██▊       | 513/1850 [34:12<1:25:47,  3.85s/it]
                                                    
{'loss': 0.026, 'grad_norm': 1.9609375, 'learning_rate': 7.227027027027027e-05, 'epoch': 13.86}

 28%|██▊       | 513/1850 [34:12<1:25:47,  3.85s/it]
 28%|██▊       | 514/1850 [34:16<1:25:10,  3.83s/it]
                                                    
{'loss': 0.0241, 'grad_norm': 2.984375, 'learning_rate': 7.221621621621621e-05, 'epoch': 13.89}

 28%|██▊       | 514/1850 [34:16<1:25:10,  3.83s/it]
 28%|██▊       | 515/1850 [34:20<1:28:19,  3.97s/it]
                                                    
{'loss': 0.0513, 'grad_norm': 3.0625, 'learning_rate': 7.216216216216216e-05, 'epoch': 13.92}

 28%|██▊       | 515/1850 [34:20<1:28:19,  3.97s/it]
 28%|██▊       | 516/1850 [34:24<1:27:48,  3.95s/it]
                                                    
{'loss': 0.022, 'grad_norm': 1.8515625, 'learning_rate': 7.210810810810811e-05, 'epoch': 13.95}

 28%|██▊       | 516/1850 [34:24<1:27:48,  3.95s/it]
 28%|██▊       | 517/1850 [34:28<1:29:24,  4.02s/it]
                                                    
{'loss': 0.0143, 'grad_norm': 1.6328125, 'learning_rate': 7.205405405405405e-05, 'epoch': 13.97}

 28%|██▊       | 517/1850 [34:28<1:29:24,  4.02s/it]
 28%|██▊       | 518/1850 [34:31<1:24:10,  3.79s/it]
                                                    
{'loss': 0.0099, 'grad_norm': 1.15625, 'learning_rate': 7.2e-05, 'epoch': 14.0}

 28%|██▊       | 518/1850 [34:31<1:24:10,  3.79s/it]
 28%|██▊       | 519/1850 [34:35<1:26:15,  3.89s/it]
                                                    
{'loss': 0.0087, 'grad_norm': 0.71484375, 'learning_rate': 7.194594594594595e-05, 'epoch': 14.03}

 28%|██▊       | 519/1850 [34:35<1:26:15,  3.89s/it]
 28%|██▊       | 520/1850 [34:39<1:26:07,  3.89s/it]
                                                    
{'loss': 0.0183, 'grad_norm': 1.6328125, 'learning_rate': 7.189189189189189e-05, 'epoch': 14.05}

 28%|██▊       | 520/1850 [34:39<1:26:07,  3.89s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 28%|██▊       | 521/1850 [34:43<1:28:41,  4.00s/it]
                                                    
{'loss': 0.0071, 'grad_norm': 0.609375, 'learning_rate': 7.183783783783784e-05, 'epoch': 14.08}

 28%|██▊       | 521/1850 [34:43<1:28:41,  4.00s/it]
 28%|██▊       | 522/1850 [34:47<1:26:03,  3.89s/it]
                                                    
{'loss': 0.0133, 'grad_norm': 1.5859375, 'learning_rate': 7.178378378378379e-05, 'epoch': 14.11}

 28%|██▊       | 522/1850 [34:47<1:26:03,  3.89s/it]
 28%|██▊       | 523/1850 [34:51<1:26:09,  3.90s/it]
                                                    
{'loss': 0.0119, 'grad_norm': 1.6796875, 'learning_rate': 7.172972972972973e-05, 'epoch': 14.14}

 28%|██▊       | 523/1850 [34:51<1:26:09,  3.90s/it]
 28%|██▊       | 524/1850 [34:55<1:27:56,  3.98s/it]
                                                    
{'loss': 0.0094, 'grad_norm': 1.1484375, 'learning_rate': 7.167567567567568e-05, 'epoch': 14.16}

 28%|██▊       | 524/1850 [34:55<1:27:56,  3.98s/it]
 28%|██▊       | 525/1850 [35:00<1:31:14,  4.13s/it]
                                                    
{'loss': 0.0341, 'grad_norm': 3.171875, 'learning_rate': 7.162162162162162e-05, 'epoch': 14.19}

 28%|██▊       | 525/1850 [35:00<1:31:14,  4.13s/it]
 28%|██▊       | 526/1850 [35:03<1:27:06,  3.95s/it]
                                                    
{'loss': 0.0079, 'grad_norm': 1.0234375, 'learning_rate': 7.156756756756757e-05, 'epoch': 14.22}

 28%|██▊       | 526/1850 [35:03<1:27:06,  3.95s/it]
 28%|██▊       | 527/1850 [35:07<1:27:28,  3.97s/it]
                                                    
{'loss': 0.0088, 'grad_norm': 0.9453125, 'learning_rate': 7.151351351351352e-05, 'epoch': 14.24}

 28%|██▊       | 527/1850 [35:07<1:27:28,  3.97s/it]
 29%|██▊       | 528/1850 [35:11<1:28:39,  4.02s/it]
                                                    
{'loss': 0.0239, 'grad_norm': 2.5, 'learning_rate': 7.145945945945946e-05, 'epoch': 14.27}

 29%|██▊       | 528/1850 [35:11<1:28:39,  4.02s/it]
 29%|██▊       | 529/1850 [35:16<1:34:05,  4.27s/it]
                                                    
{'loss': 0.0218, 'grad_norm': 2.03125, 'learning_rate': 7.140540540540541e-05, 'epoch': 14.3}

 29%|██▊       | 529/1850 [35:16<1:34:05,  4.27s/it]
 29%|██▊       | 530/1850 [35:20<1:33:19,  4.24s/it]
                                                    
{'loss': 0.0102, 'grad_norm': 1.3046875, 'learning_rate': 7.135135135135136e-05, 'epoch': 14.32}

 29%|██▊       | 530/1850 [35:20<1:33:19,  4.24s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 29%|██▊       | 531/1850 [35:25<1:34:09,  4.28s/it]
                                                    
{'loss': 0.0097, 'grad_norm': 0.93359375, 'learning_rate': 7.12972972972973e-05, 'epoch': 14.35}

 29%|██▊       | 531/1850 [35:25<1:34:09,  4.28s/it]
 29%|██▉       | 532/1850 [35:29<1:33:33,  4.26s/it]
                                                    
{'loss': 0.0302, 'grad_norm': 2.328125, 'learning_rate': 7.124324324324325e-05, 'epoch': 14.38}

 29%|██▉       | 532/1850 [35:29<1:33:33,  4.26s/it]
 29%|██▉       | 533/1850 [35:33<1:32:22,  4.21s/it]
                                                    
{'loss': 0.0273, 'grad_norm': 2.4375, 'learning_rate': 7.118918918918919e-05, 'epoch': 14.41}

 29%|██▉       | 533/1850 [35:33<1:32:22,  4.21s/it]
 29%|██▉       | 534/1850 [35:37<1:31:56,  4.19s/it]
                                                    
{'loss': 0.0288, 'grad_norm': 3.28125, 'learning_rate': 7.113513513513514e-05, 'epoch': 14.43}

 29%|██▉       | 534/1850 [35:37<1:31:56,  4.19s/it]
 29%|██▉       | 535/1850 [35:41<1:29:04,  4.06s/it]
                                                    
{'loss': 0.007, 'grad_norm': 0.828125, 'learning_rate': 7.108108108108108e-05, 'epoch': 14.46}

 29%|██▉       | 535/1850 [35:41<1:29:04,  4.06s/it]
 29%|██▉       | 536/1850 [35:45<1:28:41,  4.05s/it]
                                                    
{'loss': 0.0205, 'grad_norm': 1.21875, 'learning_rate': 7.102702702702703e-05, 'epoch': 14.49}

 29%|██▉       | 536/1850 [35:45<1:28:41,  4.05s/it]
 29%|██▉       | 537/1850 [35:49<1:27:08,  3.98s/it]
                                                    
{'loss': 0.0185, 'grad_norm': 2.6875, 'learning_rate': 7.097297297297297e-05, 'epoch': 14.51}

 29%|██▉       | 537/1850 [35:49<1:27:08,  3.98s/it]
 29%|██▉       | 538/1850 [35:52<1:23:09,  3.80s/it]
                                                    
{'loss': 0.006, 'grad_norm': 0.4765625, 'learning_rate': 7.091891891891892e-05, 'epoch': 14.54}

 29%|██▉       | 538/1850 [35:52<1:23:09,  3.80s/it]
 29%|██▉       | 539/1850 [35:56<1:24:32,  3.87s/it]
                                                    
{'loss': 0.0186, 'grad_norm': 1.71875, 'learning_rate': 7.086486486486487e-05, 'epoch': 14.57}

 29%|██▉       | 539/1850 [35:56<1:24:32,  3.87s/it]
 29%|██▉       | 540/1850 [35:59<1:20:48,  3.70s/it]
                                                    
{'loss': 0.0158, 'grad_norm': 1.8046875, 'learning_rate': 7.081081081081081e-05, 'epoch': 14.59}

 29%|██▉       | 540/1850 [35:59<1:20:48,  3.70s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 29%|██▉       | 541/1850 [36:03<1:18:40,  3.61s/it]
                                                    
{'loss': 0.0146, 'grad_norm': 1.7578125, 'learning_rate': 7.075675675675676e-05, 'epoch': 14.62}

 29%|██▉       | 541/1850 [36:03<1:18:40,  3.61s/it]
 29%|██▉       | 542/1850 [36:07<1:19:55,  3.67s/it]
                                                    
{'loss': 0.0128, 'grad_norm': 1.5078125, 'learning_rate': 7.070270270270271e-05, 'epoch': 14.65}

 29%|██▉       | 542/1850 [36:07<1:19:55,  3.67s/it]
 29%|██▉       | 543/1850 [36:12<1:27:31,  4.02s/it]
                                                    
{'loss': 0.0122, 'grad_norm': 1.15625, 'learning_rate': 7.064864864864865e-05, 'epoch': 14.68}

 29%|██▉       | 543/1850 [36:12<1:27:31,  4.02s/it]
 29%|██▉       | 544/1850 [36:15<1:26:11,  3.96s/it]
                                                    
{'loss': 0.0136, 'grad_norm': 1.9375, 'learning_rate': 7.05945945945946e-05, 'epoch': 14.7}

 29%|██▉       | 544/1850 [36:15<1:26:11,  3.96s/it]
 29%|██▉       | 545/1850 [36:19<1:26:36,  3.98s/it]
                                                    
{'loss': 0.0243, 'grad_norm': 2.203125, 'learning_rate': 7.054054054054055e-05, 'epoch': 14.73}

 29%|██▉       | 545/1850 [36:19<1:26:36,  3.98s/it]
 30%|██▉       | 546/1850 [36:23<1:25:48,  3.95s/it]
                                                    
{'loss': 0.0183, 'grad_norm': 2.09375, 'learning_rate': 7.048648648648649e-05, 'epoch': 14.76}

 30%|██▉       | 546/1850 [36:23<1:25:48,  3.95s/it]
 30%|██▉       | 547/1850 [36:27<1:25:24,  3.93s/it]
                                                    
{'loss': 0.0086, 'grad_norm': 0.7578125, 'learning_rate': 7.043243243243244e-05, 'epoch': 14.78}

 30%|██▉       | 547/1850 [36:27<1:25:24,  3.93s/it]
 30%|██▉       | 548/1850 [36:31<1:24:32,  3.90s/it]
                                                    
{'loss': 0.0163, 'grad_norm': 1.0625, 'learning_rate': 7.037837837837839e-05, 'epoch': 14.81}

 30%|██▉       | 548/1850 [36:31<1:24:32,  3.90s/it]
 30%|██▉       | 549/1850 [36:35<1:25:03,  3.92s/it]
                                                    
{'loss': 0.0134, 'grad_norm': 1.453125, 'learning_rate': 7.032432432432433e-05, 'epoch': 14.84}

 30%|██▉       | 549/1850 [36:35<1:25:03,  3.92s/it]
 30%|██▉       | 550/1850 [36:39<1:27:27,  4.04s/it]
                                                    
{'loss': 0.0149, 'grad_norm': 1.9453125, 'learning_rate': 7.027027027027028e-05, 'epoch': 14.86}

 30%|██▉       | 550/1850 [36:39<1:27:27,  4.04s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 30%|██▉       | 551/1850 [36:44<1:29:40,  4.14s/it]
                                                    
{'loss': 0.0196, 'grad_norm': 2.25, 'learning_rate': 7.021621621621623e-05, 'epoch': 14.89}

 30%|██▉       | 551/1850 [36:44<1:29:40,  4.14s/it]
 30%|██▉       | 552/1850 [36:47<1:26:22,  3.99s/it]
                                                    
{'loss': 0.0115, 'grad_norm': 1.375, 'learning_rate': 7.016216216216217e-05, 'epoch': 14.92}

 30%|██▉       | 552/1850 [36:47<1:26:22,  3.99s/it]
 30%|██▉       | 553/1850 [36:51<1:23:12,  3.85s/it]
                                                    
{'loss': 0.0122, 'grad_norm': 1.625, 'learning_rate': 7.01081081081081e-05, 'epoch': 14.95}

 30%|██▉       | 553/1850 [36:51<1:23:12,  3.85s/it]
 30%|██▉       | 554/1850 [36:55<1:26:05,  3.99s/it]
                                                    
{'loss': 0.0135, 'grad_norm': 1.5390625, 'learning_rate': 7.005405405405406e-05, 'epoch': 14.97}

 30%|██▉       | 554/1850 [36:55<1:26:05,  3.99s/it]
 30%|███       | 555/1850 [36:59<1:26:02,  3.99s/it]
                                                    
{'loss': 0.0187, 'grad_norm': 2.03125, 'learning_rate': 7e-05, 'epoch': 15.0}

 30%|███       | 555/1850 [36:59<1:26:02,  3.99s/it]
 30%|███       | 556/1850 [37:03<1:28:09,  4.09s/it]
                                                    
{'loss': 0.0058, 'grad_norm': 0.3828125, 'learning_rate': 6.994594594594595e-05, 'epoch': 15.03}

 30%|███       | 556/1850 [37:03<1:28:09,  4.09s/it]
 30%|███       | 557/1850 [37:08<1:30:12,  4.19s/it]
                                                    
{'loss': 0.0073, 'grad_norm': 0.9375, 'learning_rate': 6.98918918918919e-05, 'epoch': 15.05}

 30%|███       | 557/1850 [37:08<1:30:12,  4.19s/it]
 30%|███       | 558/1850 [37:12<1:28:24,  4.11s/it]
                                                    
{'loss': 0.0074, 'grad_norm': 1.0703125, 'learning_rate': 6.983783783783783e-05, 'epoch': 15.08}

 30%|███       | 558/1850 [37:12<1:28:24,  4.11s/it]
 30%|███       | 559/1850 [37:16<1:28:13,  4.10s/it]
                                                    
{'loss': 0.0115, 'grad_norm': 1.8125, 'learning_rate': 6.978378378378379e-05, 'epoch': 15.11}

 30%|███       | 559/1850 [37:16<1:28:13,  4.10s/it]
 30%|███       | 560/1850 [37:20<1:26:01,  4.00s/it]
                                                    
{'loss': 0.0149, 'grad_norm': 1.4296875, 'learning_rate': 6.972972972972974e-05, 'epoch': 15.14}

 30%|███       | 560/1850 [37:20<1:26:01,  4.00s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 30%|███       | 561/1850 [37:24<1:28:05,  4.10s/it]
                                                    
{'loss': 0.0058, 'grad_norm': 0.81640625, 'learning_rate': 6.967567567567567e-05, 'epoch': 15.16}

 30%|███       | 561/1850 [37:24<1:28:05,  4.10s/it]
 30%|███       | 562/1850 [37:28<1:27:54,  4.09s/it]
                                                    
{'loss': 0.0067, 'grad_norm': 0.439453125, 'learning_rate': 6.962162162162163e-05, 'epoch': 15.19}

 30%|███       | 562/1850 [37:28<1:27:54,  4.09s/it]
 30%|███       | 563/1850 [37:32<1:27:34,  4.08s/it]
                                                    
{'loss': 0.0083, 'grad_norm': 1.6484375, 'learning_rate': 6.956756756756758e-05, 'epoch': 15.22}

 30%|███       | 563/1850 [37:32<1:27:34,  4.08s/it]
 30%|███       | 564/1850 [37:37<1:32:07,  4.30s/it]
                                                    
{'loss': 0.0092, 'grad_norm': 1.1484375, 'learning_rate': 6.951351351351351e-05, 'epoch': 15.24}

 30%|███       | 564/1850 [37:37<1:32:07,  4.30s/it]
 31%|███       | 565/1850 [37:40<1:27:26,  4.08s/it]
                                                    
{'loss': 0.0341, 'grad_norm': 2.953125, 'learning_rate': 6.945945945945947e-05, 'epoch': 15.27}

 31%|███       | 565/1850 [37:40<1:27:26,  4.08s/it]
 31%|███       | 566/1850 [37:44<1:24:24,  3.94s/it]
                                                    
{'loss': 0.0083, 'grad_norm': 0.96484375, 'learning_rate': 6.940540540540542e-05, 'epoch': 15.3}

 31%|███       | 566/1850 [37:44<1:24:24,  3.94s/it]
 31%|███       | 567/1850 [37:48<1:26:06,  4.03s/it]
                                                    
{'loss': 0.0197, 'grad_norm': 2.125, 'learning_rate': 6.935135135135135e-05, 'epoch': 15.32}

 31%|███       | 567/1850 [37:48<1:26:06,  4.03s/it]
 31%|███       | 568/1850 [37:52<1:26:32,  4.05s/it]
                                                    
{'loss': 0.0159, 'grad_norm': 1.5, 'learning_rate': 6.92972972972973e-05, 'epoch': 15.35}

 31%|███       | 568/1850 [37:52<1:26:32,  4.05s/it]
 31%|███       | 569/1850 [37:56<1:26:16,  4.04s/it]
                                                    
{'loss': 0.0081, 'grad_norm': 0.98828125, 'learning_rate': 6.924324324324324e-05, 'epoch': 15.38}

 31%|███       | 569/1850 [37:56<1:26:16,  4.04s/it]
 31%|███       | 570/1850 [38:00<1:24:42,  3.97s/it]
                                                    
{'loss': 0.0115, 'grad_norm': 1.1171875, 'learning_rate': 6.91891891891892e-05, 'epoch': 15.41}

 31%|███       | 570/1850 [38:00<1:24:42,  3.97s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 31%|███       | 571/1850 [38:04<1:24:09,  3.95s/it]
                                                    
{'loss': 0.0065, 'grad_norm': 0.7109375, 'learning_rate': 6.913513513513515e-05, 'epoch': 15.43}

 31%|███       | 571/1850 [38:04<1:24:09,  3.95s/it]
 31%|███       | 572/1850 [38:08<1:21:36,  3.83s/it]
                                                    
{'loss': 0.0053, 'grad_norm': 0.640625, 'learning_rate': 6.908108108108108e-05, 'epoch': 15.46}

 31%|███       | 572/1850 [38:08<1:21:36,  3.83s/it]
 31%|███       | 573/1850 [38:12<1:22:50,  3.89s/it]
                                                    
{'loss': 0.0126, 'grad_norm': 1.140625, 'learning_rate': 6.902702702702703e-05, 'epoch': 15.49}

 31%|███       | 573/1850 [38:12<1:22:50,  3.89s/it]
 31%|███       | 574/1850 [38:15<1:21:08,  3.82s/it]
                                                    
{'loss': 0.0051, 'grad_norm': 0.97265625, 'learning_rate': 6.897297297297297e-05, 'epoch': 15.51}

 31%|███       | 574/1850 [38:15<1:21:08,  3.82s/it]
 31%|███       | 575/1850 [38:19<1:19:06,  3.72s/it]
                                                    
{'loss': 0.0068, 'grad_norm': 0.91796875, 'learning_rate': 6.891891891891892e-05, 'epoch': 15.54}

 31%|███       | 575/1850 [38:19<1:19:06,  3.72s/it]
 31%|███       | 576/1850 [38:22<1:16:52,  3.62s/it]
                                                    
{'loss': 0.0109, 'grad_norm': 1.84375, 'learning_rate': 6.886486486486486e-05, 'epoch': 15.57}

 31%|███       | 576/1850 [38:22<1:16:52,  3.62s/it]
 31%|███       | 577/1850 [38:26<1:20:02,  3.77s/it]
                                                    
{'loss': 0.0076, 'grad_norm': 0.75390625, 'learning_rate': 6.881081081081081e-05, 'epoch': 15.59}

 31%|███       | 577/1850 [38:26<1:20:02,  3.77s/it]
 31%|███       | 578/1850 [38:30<1:19:43,  3.76s/it]
                                                    
{'loss': 0.0104, 'grad_norm': 1.6015625, 'learning_rate': 6.875675675675676e-05, 'epoch': 15.62}

 31%|███       | 578/1850 [38:30<1:19:43,  3.76s/it]
 31%|███▏      | 579/1850 [38:34<1:20:46,  3.81s/it]
                                                    
{'loss': 0.0086, 'grad_norm': 1.5859375, 'learning_rate': 6.87027027027027e-05, 'epoch': 15.65}

 31%|███▏      | 579/1850 [38:34<1:20:46,  3.81s/it]
 31%|███▏      | 580/1850 [38:38<1:19:35,  3.76s/it]
                                                    
{'loss': 0.0105, 'grad_norm': 0.87109375, 'learning_rate': 6.864864864864865e-05, 'epoch': 15.68}

 31%|███▏      | 580/1850 [38:38<1:19:35,  3.76s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 31%|███▏      | 581/1850 [38:42<1:24:39,  4.00s/it]
                                                    
{'loss': 0.0144, 'grad_norm': 1.625, 'learning_rate': 6.859459459459459e-05, 'epoch': 15.7}

 31%|███▏      | 581/1850 [38:42<1:24:39,  4.00s/it]
 31%|███▏      | 582/1850 [38:46<1:22:50,  3.92s/it]
                                                    
{'loss': 0.0132, 'grad_norm': 1.3984375, 'learning_rate': 6.854054054054054e-05, 'epoch': 15.73}

 31%|███▏      | 582/1850 [38:46<1:22:50,  3.92s/it]
 32%|███▏      | 583/1850 [38:50<1:20:57,  3.83s/it]
                                                    
{'loss': 0.0267, 'grad_norm': 2.234375, 'learning_rate': 6.848648648648649e-05, 'epoch': 15.76}

 32%|███▏      | 583/1850 [38:50<1:20:57,  3.83s/it]
 32%|███▏      | 584/1850 [38:54<1:21:46,  3.88s/it]
                                                    
{'loss': 0.0075, 'grad_norm': 0.83984375, 'learning_rate': 6.843243243243243e-05, 'epoch': 15.78}

 32%|███▏      | 584/1850 [38:54<1:21:46,  3.88s/it]
 32%|███▏      | 585/1850 [38:57<1:19:48,  3.79s/it]
                                                    
{'loss': 0.0133, 'grad_norm': 2.09375, 'learning_rate': 6.837837837837838e-05, 'epoch': 15.81}

 32%|███▏      | 585/1850 [38:57<1:19:48,  3.79s/it]
 32%|███▏      | 586/1850 [39:02<1:23:28,  3.96s/it]
                                                    
{'loss': 0.0125, 'grad_norm': 1.9296875, 'learning_rate': 6.832432432432433e-05, 'epoch': 15.84}

 32%|███▏      | 586/1850 [39:02<1:23:28,  3.96s/it]
 32%|███▏      | 587/1850 [39:05<1:21:44,  3.88s/it]
                                                    
{'loss': 0.0053, 'grad_norm': 0.515625, 'learning_rate': 6.827027027027027e-05, 'epoch': 15.86}

 32%|███▏      | 587/1850 [39:05<1:21:44,  3.88s/it]
 32%|███▏      | 588/1850 [39:09<1:24:02,  4.00s/it]
                                                    
{'loss': 0.0127, 'grad_norm': 1.1015625, 'learning_rate': 6.821621621621622e-05, 'epoch': 15.89}

 32%|███▏      | 588/1850 [39:09<1:24:02,  4.00s/it]
 32%|███▏      | 589/1850 [39:14<1:27:49,  4.18s/it]
                                                    
{'loss': 0.0208, 'grad_norm': 1.90625, 'learning_rate': 6.816216216216217e-05, 'epoch': 15.92}

 32%|███▏      | 589/1850 [39:14<1:27:49,  4.18s/it]
 32%|███▏      | 590/1850 [39:18<1:26:56,  4.14s/it]
                                                    
{'loss': 0.0126, 'grad_norm': 1.578125, 'learning_rate': 6.810810810810811e-05, 'epoch': 15.95}

 32%|███▏      | 590/1850 [39:18<1:26:56,  4.14s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 32%|███▏      | 591/1850 [39:22<1:27:12,  4.16s/it]
                                                    
{'loss': 0.0141, 'grad_norm': 1.53125, 'learning_rate': 6.805405405405406e-05, 'epoch': 15.97}

 32%|███▏      | 591/1850 [39:22<1:27:12,  4.16s/it]
 32%|███▏      | 592/1850 [39:26<1:27:01,  4.15s/it]
                                                    
{'loss': 0.0284, 'grad_norm': 1.796875, 'learning_rate': 6.800000000000001e-05, 'epoch': 16.0}

 32%|███▏      | 592/1850 [39:26<1:27:01,  4.15s/it]
 32%|███▏      | 593/1850 [39:31<1:28:07,  4.21s/it]
                                                    
{'loss': 0.011, 'grad_norm': 0.75, 'learning_rate': 6.794594594594595e-05, 'epoch': 16.03}

 32%|███▏      | 593/1850 [39:31<1:28:07,  4.21s/it]
 32%|███▏      | 594/1850 [39:35<1:25:04,  4.06s/it]
                                                    
{'loss': 0.013, 'grad_norm': 0.76171875, 'learning_rate': 6.789189189189189e-05, 'epoch': 16.05}

 32%|███▏      | 594/1850 [39:35<1:25:04,  4.06s/it]
 32%|███▏      | 595/1850 [39:39<1:25:58,  4.11s/it]
                                                    
{'loss': 0.013, 'grad_norm': 1.4140625, 'learning_rate': 6.783783783783784e-05, 'epoch': 16.08}

 32%|███▏      | 595/1850 [39:39<1:25:58,  4.11s/it]
 32%|███▏      | 596/1850 [39:43<1:29:39,  4.29s/it]
                                                    
{'loss': 0.005, 'grad_norm': 0.26953125, 'learning_rate': 6.778378378378378e-05, 'epoch': 16.11}

 32%|███▏      | 596/1850 [39:43<1:29:39,  4.29s/it]
 32%|███▏      | 597/1850 [39:47<1:26:45,  4.15s/it]
                                                    
{'loss': 0.0081, 'grad_norm': 1.03125, 'learning_rate': 6.772972972972973e-05, 'epoch': 16.14}

 32%|███▏      | 597/1850 [39:47<1:26:45,  4.15s/it]
 32%|███▏      | 598/1850 [39:51<1:23:20,  3.99s/it]
                                                    
{'loss': 0.0033, 'grad_norm': 0.375, 'learning_rate': 6.767567567567568e-05, 'epoch': 16.16}

 32%|███▏      | 598/1850 [39:51<1:23:20,  3.99s/it]
 32%|███▏      | 599/1850 [39:54<1:20:28,  3.86s/it]
                                                    
{'loss': 0.0121, 'grad_norm': 1.3046875, 'learning_rate': 6.762162162162162e-05, 'epoch': 16.19}

 32%|███▏      | 599/1850 [39:54<1:20:28,  3.86s/it]
 32%|███▏      | 600/1850 [39:58<1:20:59,  3.89s/it]
                                                    
{'loss': 0.0092, 'grad_norm': 1.6171875, 'learning_rate': 6.756756756756757e-05, 'epoch': 16.22}

 32%|███▏      | 600/1850 [39:58<1:20:59,  3.89s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 32%|███▏      | 601/1850 [40:02<1:21:29,  3.91s/it]
                                                    
{'loss': 0.0052, 'grad_norm': 0.83984375, 'learning_rate': 6.751351351351352e-05, 'epoch': 16.24}

 32%|███▏      | 601/1850 [40:02<1:21:29,  3.91s/it]
 33%|███▎      | 602/1850 [40:06<1:18:46,  3.79s/it]
                                                    
{'loss': 0.0125, 'grad_norm': 1.84375, 'learning_rate': 6.745945945945946e-05, 'epoch': 16.27}

 33%|███▎      | 602/1850 [40:06<1:18:46,  3.79s/it]
 33%|███▎      | 603/1850 [40:10<1:20:14,  3.86s/it]
                                                    
{'loss': 0.0057, 'grad_norm': 0.83203125, 'learning_rate': 6.740540540540541e-05, 'epoch': 16.3}

 33%|███▎      | 603/1850 [40:10<1:20:14,  3.86s/it]
 33%|███▎      | 604/1850 [40:13<1:16:54,  3.70s/it]
                                                    
{'loss': 0.0071, 'grad_norm': 1.9375, 'learning_rate': 6.735135135135136e-05, 'epoch': 16.32}

 33%|███▎      | 604/1850 [40:13<1:16:54,  3.70s/it]
 33%|███▎      | 605/1850 [40:17<1:15:41,  3.65s/it]
                                                    
{'loss': 0.0116, 'grad_norm': 1.75, 'learning_rate': 6.72972972972973e-05, 'epoch': 16.35}

 33%|███▎      | 605/1850 [40:17<1:15:41,  3.65s/it]
 33%|███▎      | 606/1850 [40:22<1:22:34,  3.98s/it]
                                                    
{'loss': 0.0116, 'grad_norm': 0.9921875, 'learning_rate': 6.724324324324325e-05, 'epoch': 16.38}

 33%|███▎      | 606/1850 [40:22<1:22:34,  3.98s/it]
 33%|███▎      | 607/1850 [40:26<1:23:02,  4.01s/it]
                                                    
{'loss': 0.0146, 'grad_norm': 1.328125, 'learning_rate': 6.71891891891892e-05, 'epoch': 16.41}

 33%|███▎      | 607/1850 [40:26<1:23:02,  4.01s/it]
 33%|███▎      | 608/1850 [40:30<1:23:32,  4.04s/it]
                                                    
{'loss': 0.0064, 'grad_norm': 0.75, 'learning_rate': 6.713513513513514e-05, 'epoch': 16.43}

 33%|███▎      | 608/1850 [40:30<1:23:32,  4.04s/it]
 33%|███▎      | 609/1850 [40:34<1:25:17,  4.12s/it]
                                                    
{'loss': 0.0091, 'grad_norm': 0.89453125, 'learning_rate': 6.708108108108109e-05, 'epoch': 16.46}

 33%|███▎      | 609/1850 [40:34<1:25:17,  4.12s/it]
 33%|███▎      | 610/1850 [40:38<1:24:53,  4.11s/it]
                                                    
{'loss': 0.0099, 'grad_norm': 1.125, 'learning_rate': 6.702702702702704e-05, 'epoch': 16.49}

 33%|███▎      | 610/1850 [40:38<1:24:53,  4.11s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 33%|███▎      | 611/1850 [40:42<1:23:18,  4.03s/it]
                                                    
{'loss': 0.0145, 'grad_norm': 1.1953125, 'learning_rate': 6.697297297297298e-05, 'epoch': 16.51}

 33%|███▎      | 611/1850 [40:42<1:23:18,  4.03s/it]
 33%|███▎      | 612/1850 [40:46<1:24:29,  4.09s/it]
                                                    
{'loss': 0.0076, 'grad_norm': 1.1875, 'learning_rate': 6.691891891891893e-05, 'epoch': 16.54}

 33%|███▎      | 612/1850 [40:46<1:24:29,  4.09s/it]
 33%|███▎      | 613/1850 [40:50<1:21:50,  3.97s/it]
                                                    
{'loss': 0.0115, 'grad_norm': 0.62890625, 'learning_rate': 6.686486486486487e-05, 'epoch': 16.57}

 33%|███▎      | 613/1850 [40:50<1:21:50,  3.97s/it]
 33%|███▎      | 614/1850 [40:54<1:22:08,  3.99s/it]
                                                    
{'loss': 0.0066, 'grad_norm': 1.0078125, 'learning_rate': 6.68108108108108e-05, 'epoch': 16.59}

 33%|███▎      | 614/1850 [40:54<1:22:08,  3.99s/it]
 33%|███▎      | 615/1850 [40:58<1:23:27,  4.05s/it]
                                                    
{'loss': 0.0069, 'grad_norm': 1.1953125, 'learning_rate': 6.675675675675676e-05, 'epoch': 16.62}

 33%|███▎      | 615/1850 [40:58<1:23:27,  4.05s/it]
 33%|███▎      | 616/1850 [41:02<1:24:23,  4.10s/it]
                                                    
{'loss': 0.0063, 'grad_norm': 0.65625, 'learning_rate': 6.670270270270271e-05, 'epoch': 16.65}

 33%|███▎      | 616/1850 [41:02<1:24:23,  4.10s/it]
 33%|███▎      | 617/1850 [41:06<1:22:27,  4.01s/it]
                                                    
{'loss': 0.0058, 'grad_norm': 1.109375, 'learning_rate': 6.664864864864864e-05, 'epoch': 16.68}

 33%|███▎      | 617/1850 [41:06<1:22:27,  4.01s/it]
 33%|███▎      | 618/1850 [41:10<1:20:38,  3.93s/it]
                                                    
{'loss': 0.014, 'grad_norm': 2.34375, 'learning_rate': 6.65945945945946e-05, 'epoch': 16.7}

 33%|███▎      | 618/1850 [41:10<1:20:38,  3.93s/it]
 33%|███▎      | 619/1850 [41:13<1:17:41,  3.79s/it]
                                                    
{'loss': 0.0132, 'grad_norm': 1.625, 'learning_rate': 6.654054054054055e-05, 'epoch': 16.73}

 33%|███▎      | 619/1850 [41:13<1:17:41,  3.79s/it]
 34%|███▎      | 620/1850 [41:18<1:21:06,  3.96s/it]
                                                    
{'loss': 0.0117, 'grad_norm': 1.8046875, 'learning_rate': 6.648648648648648e-05, 'epoch': 16.76}

 34%|███▎      | 620/1850 [41:18<1:21:06,  3.96s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 34%|███▎      | 621/1850 [41:22<1:24:49,  4.14s/it]
                                                    
{'loss': 0.0061, 'grad_norm': 0.6875, 'learning_rate': 6.643243243243244e-05, 'epoch': 16.78}

 34%|███▎      | 621/1850 [41:22<1:24:49,  4.14s/it]
 34%|███▎      | 622/1850 [41:26<1:23:27,  4.08s/it]
                                                    
{'loss': 0.0127, 'grad_norm': 1.2265625, 'learning_rate': 6.637837837837839e-05, 'epoch': 16.81}

 34%|███▎      | 622/1850 [41:26<1:23:27,  4.08s/it]
 34%|███▎      | 623/1850 [41:30<1:23:36,  4.09s/it]
                                                    
{'loss': 0.0111, 'grad_norm': 1.8671875, 'learning_rate': 6.632432432432432e-05, 'epoch': 16.84}

 34%|███▎      | 623/1850 [41:30<1:23:36,  4.09s/it]
 34%|███▎      | 624/1850 [41:34<1:20:42,  3.95s/it]
                                                    
{'loss': 0.015, 'grad_norm': 2.4375, 'learning_rate': 6.627027027027028e-05, 'epoch': 16.86}

 34%|███▎      | 624/1850 [41:34<1:20:42,  3.95s/it]
 34%|███▍      | 625/1850 [41:38<1:21:17,  3.98s/it]
                                                    
{'loss': 0.0176, 'grad_norm': 2.953125, 'learning_rate': 6.621621621621621e-05, 'epoch': 16.89}

 34%|███▍      | 625/1850 [41:38<1:21:17,  3.98s/it]
 34%|███▍      | 626/1850 [41:42<1:18:51,  3.87s/it]
                                                    
{'loss': 0.0139, 'grad_norm': 1.171875, 'learning_rate': 6.616216216216216e-05, 'epoch': 16.92}

 34%|███▍      | 626/1850 [41:42<1:18:51,  3.87s/it]
 34%|███▍      | 627/1850 [41:46<1:21:02,  3.98s/it]
                                                    
{'loss': 0.0148, 'grad_norm': 1.515625, 'learning_rate': 6.610810810810812e-05, 'epoch': 16.95}

 34%|███▍      | 627/1850 [41:46<1:21:02,  3.98s/it]
 34%|███▍      | 628/1850 [41:50<1:21:55,  4.02s/it]
                                                    
{'loss': 0.0146, 'grad_norm': 1.9609375, 'learning_rate': 6.605405405405405e-05, 'epoch': 16.97}

 34%|███▍      | 628/1850 [41:50<1:21:55,  4.02s/it]
 34%|███▍      | 629/1850 [41:54<1:24:25,  4.15s/it]
                                                    
{'loss': 0.0144, 'grad_norm': 1.4453125, 'learning_rate': 6.6e-05, 'epoch': 17.0}

 34%|███▍      | 629/1850 [41:54<1:24:25,  4.15s/it]
 34%|███▍      | 630/1850 [41:59<1:25:04,  4.18s/it]
                                                    
{'loss': 0.005, 'grad_norm': 0.478515625, 'learning_rate': 6.594594594594596e-05, 'epoch': 17.03}

 34%|███▍      | 630/1850 [41:59<1:25:04,  4.18s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 34%|███▍      | 631/1850 [42:03<1:24:31,  4.16s/it]
                                                    
{'loss': 0.0054, 'grad_norm': 1.0234375, 'learning_rate': 6.58918918918919e-05, 'epoch': 17.05}

 34%|███▍      | 631/1850 [42:03<1:24:31,  4.16s/it]
 34%|███▍      | 632/1850 [42:07<1:23:18,  4.10s/it]
                                                    
{'loss': 0.0082, 'grad_norm': 1.171875, 'learning_rate': 6.583783783783785e-05, 'epoch': 17.08}

 34%|███▍      | 632/1850 [42:07<1:23:18,  4.10s/it]
 34%|███▍      | 633/1850 [42:10<1:21:16,  4.01s/it]
                                                    
{'loss': 0.008, 'grad_norm': 1.7109375, 'learning_rate': 6.57837837837838e-05, 'epoch': 17.11}

 34%|███▍      | 633/1850 [42:10<1:21:16,  4.01s/it]
 34%|███▍      | 634/1850 [42:14<1:19:32,  3.92s/it]
                                                    
{'loss': 0.0101, 'grad_norm': 0.82421875, 'learning_rate': 6.572972972972973e-05, 'epoch': 17.14}

 34%|███▍      | 634/1850 [42:14<1:19:32,  3.92s/it]
 34%|███▍      | 635/1850 [42:18<1:18:14,  3.86s/it]
                                                    
{'loss': 0.0097, 'grad_norm': 1.75, 'learning_rate': 6.567567567567567e-05, 'epoch': 17.16}

 34%|███▍      | 635/1850 [42:18<1:18:14,  3.86s/it]
 34%|███▍      | 636/1850 [42:22<1:21:21,  4.02s/it]
                                                    
{'loss': 0.0063, 'grad_norm': 0.8203125, 'learning_rate': 6.562162162162162e-05, 'epoch': 17.19}

 34%|███▍      | 636/1850 [42:22<1:21:21,  4.02s/it]
 34%|███▍      | 637/1850 [42:26<1:19:18,  3.92s/it]
                                                    
{'loss': 0.004, 'grad_norm': 0.2373046875, 'learning_rate': 6.556756756756756e-05, 'epoch': 17.22}

 34%|███▍      | 637/1850 [42:26<1:19:18,  3.92s/it]
 34%|███▍      | 638/1850 [42:31<1:27:32,  4.33s/it]
                                                    
{'loss': 0.0183, 'grad_norm': 1.953125, 'learning_rate': 6.551351351351351e-05, 'epoch': 17.24}

 34%|███▍      | 638/1850 [42:31<1:27:32,  4.33s/it]
 35%|███▍      | 639/1850 [42:35<1:24:03,  4.16s/it]
                                                    
{'loss': 0.0032, 'grad_norm': 0.40625, 'learning_rate': 6.545945945945946e-05, 'epoch': 17.27}

 35%|███▍      | 639/1850 [42:35<1:24:03,  4.16s/it]
 35%|███▍      | 640/1850 [42:39<1:24:45,  4.20s/it]
                                                    
{'loss': 0.0079, 'grad_norm': 1.0390625, 'learning_rate': 6.54054054054054e-05, 'epoch': 17.3}

 35%|███▍      | 640/1850 [42:39<1:24:45,  4.20s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 35%|███▍      | 641/1850 [42:43<1:22:45,  4.11s/it]
                                                    
{'loss': 0.0051, 'grad_norm': 0.90625, 'learning_rate': 6.535135135135135e-05, 'epoch': 17.32}

 35%|███▍      | 641/1850 [42:43<1:22:45,  4.11s/it]
 35%|███▍      | 642/1850 [42:47<1:18:12,  3.88s/it]
                                                    
{'loss': 0.007, 'grad_norm': 1.8515625, 'learning_rate': 6.52972972972973e-05, 'epoch': 17.35}

 35%|███▍      | 642/1850 [42:47<1:18:12,  3.88s/it]
 35%|███▍      | 643/1850 [42:51<1:19:11,  3.94s/it]
                                                    
{'loss': 0.0163, 'grad_norm': 1.2265625, 'learning_rate': 6.524324324324324e-05, 'epoch': 17.38}

 35%|███▍      | 643/1850 [42:51<1:19:11,  3.94s/it]
 35%|███▍      | 644/1850 [42:55<1:21:37,  4.06s/it]
                                                    
{'loss': 0.008, 'grad_norm': 0.98828125, 'learning_rate': 6.518918918918919e-05, 'epoch': 17.41}

 35%|███▍      | 644/1850 [42:55<1:21:37,  4.06s/it]
 35%|███▍      | 645/1850 [42:59<1:21:36,  4.06s/it]
                                                    
{'loss': 0.0237, 'grad_norm': 1.03125, 'learning_rate': 6.513513513513514e-05, 'epoch': 17.43}

 35%|███▍      | 645/1850 [42:59<1:21:36,  4.06s/it]
 35%|███▍      | 646/1850 [43:03<1:18:01,  3.89s/it]
                                                    
{'loss': 0.0042, 'grad_norm': 0.294921875, 'learning_rate': 6.508108108108108e-05, 'epoch': 17.46}

 35%|███▍      | 646/1850 [43:03<1:18:01,  3.89s/it]
 35%|███▍      | 647/1850 [43:06<1:17:54,  3.89s/it]
                                                    
{'loss': 0.008, 'grad_norm': 1.6484375, 'learning_rate': 6.502702702702703e-05, 'epoch': 17.49}

 35%|███▍      | 647/1850 [43:06<1:17:54,  3.89s/it]
 35%|███▌      | 648/1850 [43:10<1:18:25,  3.91s/it]
                                                    
{'loss': 0.0077, 'grad_norm': 0.84765625, 'learning_rate': 6.497297297297298e-05, 'epoch': 17.51}

 35%|███▌      | 648/1850 [43:10<1:18:25,  3.91s/it]
 35%|███▌      | 649/1850 [43:14<1:16:36,  3.83s/it]
                                                    
{'loss': 0.0087, 'grad_norm': 1.40625, 'learning_rate': 6.491891891891892e-05, 'epoch': 17.54}

 35%|███▌      | 649/1850 [43:14<1:16:36,  3.83s/it]
 35%|███▌      | 650/1850 [43:18<1:16:17,  3.81s/it]
                                                    
{'loss': 0.0072, 'grad_norm': 1.7109375, 'learning_rate': 6.486486486486487e-05, 'epoch': 17.57}

 35%|███▌      | 650/1850 [43:18<1:16:17,  3.81s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 35%|███▌      | 651/1850 [43:22<1:20:05,  4.01s/it]
                                                    
{'loss': 0.0152, 'grad_norm': 1.828125, 'learning_rate': 6.481081081081082e-05, 'epoch': 17.59}

 35%|███▌      | 651/1850 [43:22<1:20:05,  4.01s/it]
 35%|███▌      | 652/1850 [43:26<1:18:58,  3.96s/it]
                                                    
{'loss': 0.0078, 'grad_norm': 1.59375, 'learning_rate': 6.475675675675676e-05, 'epoch': 17.62}

 35%|███▌      | 652/1850 [43:26<1:18:58,  3.96s/it]
 35%|███▌      | 653/1850 [43:31<1:23:05,  4.17s/it]
                                                    
{'loss': 0.0164, 'grad_norm': 2.4375, 'learning_rate': 6.470270270270271e-05, 'epoch': 17.65}

 35%|███▌      | 653/1850 [43:31<1:23:05,  4.17s/it]
 35%|███▌      | 654/1850 [43:35<1:23:54,  4.21s/it]
                                                    
{'loss': 0.005, 'grad_norm': 0.482421875, 'learning_rate': 6.464864864864865e-05, 'epoch': 17.68}

 35%|███▌      | 654/1850 [43:35<1:23:54,  4.21s/it]
 35%|███▌      | 655/1850 [43:39<1:22:22,  4.14s/it]
                                                    
{'loss': 0.0084, 'grad_norm': 1.4609375, 'learning_rate': 6.459459459459459e-05, 'epoch': 17.7}

 35%|███▌      | 655/1850 [43:39<1:22:22,  4.14s/it]
 35%|███▌      | 656/1850 [43:43<1:23:50,  4.21s/it]
                                                    
{'loss': 0.0107, 'grad_norm': 1.625, 'learning_rate': 6.454054054054054e-05, 'epoch': 17.73}

 35%|███▌      | 656/1850 [43:43<1:23:50,  4.21s/it]
 36%|███▌      | 657/1850 [43:48<1:27:43,  4.41s/it]
                                                    
{'loss': 0.0075, 'grad_norm': 0.90625, 'learning_rate': 6.448648648648649e-05, 'epoch': 17.76}

 36%|███▌      | 657/1850 [43:48<1:27:43,  4.41s/it]
 36%|███▌      | 658/1850 [43:52<1:25:00,  4.28s/it]
                                                    
{'loss': 0.0189, 'grad_norm': 2.140625, 'learning_rate': 6.443243243243243e-05, 'epoch': 17.78}

 36%|███▌      | 658/1850 [43:52<1:25:00,  4.28s/it]
 36%|███▌      | 659/1850 [43:56<1:20:50,  4.07s/it]
                                                    
{'loss': 0.0141, 'grad_norm': 2.71875, 'learning_rate': 6.437837837837838e-05, 'epoch': 17.81}

 36%|███▌      | 659/1850 [43:56<1:20:50,  4.07s/it]
 36%|███▌      | 660/1850 [44:00<1:18:32,  3.96s/it]
                                                    
{'loss': 0.0046, 'grad_norm': 0.474609375, 'learning_rate': 6.432432432432433e-05, 'epoch': 17.84}

 36%|███▌      | 660/1850 [44:00<1:18:32,  3.96s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 36%|███▌      | 661/1850 [44:04<1:23:40,  4.22s/it]
                                                    
{'loss': 0.0245, 'grad_norm': 2.4375, 'learning_rate': 6.427027027027027e-05, 'epoch': 17.86}

 36%|███▌      | 661/1850 [44:04<1:23:40,  4.22s/it]
 36%|███▌      | 662/1850 [44:08<1:21:10,  4.10s/it]
                                                    
{'loss': 0.0066, 'grad_norm': 1.234375, 'learning_rate': 6.421621621621622e-05, 'epoch': 17.89}

 36%|███▌      | 662/1850 [44:08<1:21:10,  4.10s/it]
 36%|███▌      | 663/1850 [44:12<1:21:30,  4.12s/it]
                                                    
{'loss': 0.0048, 'grad_norm': 0.51171875, 'learning_rate': 6.416216216216217e-05, 'epoch': 17.92}

 36%|███▌      | 663/1850 [44:12<1:21:30,  4.12s/it]
 36%|███▌      | 664/1850 [44:17<1:21:21,  4.12s/it]
                                                    
{'loss': 0.0047, 'grad_norm': 0.41015625, 'learning_rate': 6.410810810810811e-05, 'epoch': 17.95}

 36%|███▌      | 664/1850 [44:17<1:21:21,  4.12s/it]
 36%|███▌      | 665/1850 [44:20<1:17:54,  3.94s/it]
                                                    
{'loss': 0.0062, 'grad_norm': 0.9140625, 'learning_rate': 6.405405405405406e-05, 'epoch': 17.97}

 36%|███▌      | 665/1850 [44:20<1:17:54,  3.94s/it]
 36%|███▌      | 666/1850 [44:23<1:12:52,  3.69s/it]
                                                    
{'loss': 0.0033, 'grad_norm': 0.53125, 'learning_rate': 6.400000000000001e-05, 'epoch': 18.0}

 36%|███▌      | 666/1850 [44:23<1:12:52,  3.69s/it]
 36%|███▌      | 667/1850 [44:27<1:10:44,  3.59s/it]
                                                    
{'loss': 0.0035, 'grad_norm': 0.5390625, 'learning_rate': 6.394594594594595e-05, 'epoch': 18.03}

 36%|███▌      | 667/1850 [44:27<1:10:44,  3.59s/it]
 36%|███▌      | 668/1850 [44:30<1:11:43,  3.64s/it]
                                                    
{'loss': 0.0068, 'grad_norm': 1.234375, 'learning_rate': 6.38918918918919e-05, 'epoch': 18.05}

 36%|███▌      | 668/1850 [44:30<1:11:43,  3.64s/it]
 36%|███▌      | 669/1850 [44:34<1:14:16,  3.77s/it]
                                                    
{'loss': 0.0061, 'grad_norm': 0.93359375, 'learning_rate': 6.383783783783784e-05, 'epoch': 18.08}

 36%|███▌      | 669/1850 [44:34<1:14:16,  3.77s/it]
 36%|███▌      | 670/1850 [44:39<1:17:02,  3.92s/it]
                                                    
{'loss': 0.0032, 'grad_norm': 0.2109375, 'learning_rate': 6.378378378378379e-05, 'epoch': 18.11}

 36%|███▌      | 670/1850 [44:39<1:17:02,  3.92s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 36%|███▋      | 671/1850 [44:43<1:18:39,  4.00s/it]
                                                    
{'loss': 0.0053, 'grad_norm': 1.6640625, 'learning_rate': 6.372972972972974e-05, 'epoch': 18.14}

 36%|███▋      | 671/1850 [44:43<1:18:39,  4.00s/it]
 36%|███▋      | 672/1850 [44:47<1:19:16,  4.04s/it]
                                                    
{'loss': 0.0057, 'grad_norm': 1.3203125, 'learning_rate': 6.367567567567568e-05, 'epoch': 18.16}

 36%|███▋      | 672/1850 [44:47<1:19:16,  4.04s/it]
 36%|███▋      | 673/1850 [44:52<1:27:53,  4.48s/it]
                                                    
{'loss': 0.0054, 'grad_norm': 0.419921875, 'learning_rate': 6.362162162162163e-05, 'epoch': 18.19}

 36%|███▋      | 673/1850 [44:52<1:27:53,  4.48s/it]
 36%|███▋      | 674/1850 [44:56<1:23:56,  4.28s/it]
                                                    
{'loss': 0.0034, 'grad_norm': 0.5859375, 'learning_rate': 6.356756756756757e-05, 'epoch': 18.22}

 36%|███▋      | 674/1850 [44:56<1:23:56,  4.28s/it]
 36%|███▋      | 675/1850 [45:00<1:21:53,  4.18s/it]
                                                    
{'loss': 0.0052, 'grad_norm': 0.734375, 'learning_rate': 6.351351351351352e-05, 'epoch': 18.24}

 36%|███▋      | 675/1850 [45:00<1:21:53,  4.18s/it]
 37%|███▋      | 676/1850 [45:04<1:21:11,  4.15s/it]
                                                    
{'loss': 0.0038, 'grad_norm': 0.6328125, 'learning_rate': 6.345945945945946e-05, 'epoch': 18.27}

 37%|███▋      | 676/1850 [45:04<1:21:11,  4.15s/it]
 37%|███▋      | 677/1850 [45:09<1:21:47,  4.18s/it]
                                                    
{'loss': 0.0068, 'grad_norm': 0.8515625, 'learning_rate': 6.34054054054054e-05, 'epoch': 18.3}

 37%|███▋      | 677/1850 [45:09<1:21:47,  4.18s/it]
 37%|███▋      | 678/1850 [45:13<1:25:36,  4.38s/it]
                                                    
{'loss': 0.0051, 'grad_norm': 1.0078125, 'learning_rate': 6.335135135135136e-05, 'epoch': 18.32}

 37%|███▋      | 678/1850 [45:13<1:25:36,  4.38s/it]
 37%|███▋      | 679/1850 [45:17<1:22:48,  4.24s/it]
                                                    
{'loss': 0.0044, 'grad_norm': 0.61328125, 'learning_rate': 6.32972972972973e-05, 'epoch': 18.35}

 37%|███▋      | 679/1850 [45:17<1:22:48,  4.24s/it]
 37%|███▋      | 680/1850 [45:21<1:17:45,  3.99s/it]
                                                    
{'loss': 0.0029, 'grad_norm': 0.61328125, 'learning_rate': 6.324324324324325e-05, 'epoch': 18.38}

 37%|███▋      | 680/1850 [45:21<1:17:45,  3.99s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 37%|███▋      | 681/1850 [45:26<1:22:25,  4.23s/it]
                                                    
{'loss': 0.0056, 'grad_norm': 0.8828125, 'learning_rate': 6.318918918918918e-05, 'epoch': 18.41}

 37%|███▋      | 681/1850 [45:26<1:22:25,  4.23s/it]
 37%|███▋      | 682/1850 [45:29<1:20:31,  4.14s/it]
                                                    
{'loss': 0.0084, 'grad_norm': 1.515625, 'learning_rate': 6.313513513513514e-05, 'epoch': 18.43}

 37%|███▋      | 682/1850 [45:29<1:20:31,  4.14s/it]
 37%|███▋      | 683/1850 [45:33<1:15:37,  3.89s/it]
                                                    
{'loss': 0.0102, 'grad_norm': 1.796875, 'learning_rate': 6.308108108108109e-05, 'epoch': 18.46}

 37%|███▋      | 683/1850 [45:33<1:15:37,  3.89s/it]
 37%|███▋      | 684/1850 [45:37<1:16:41,  3.95s/it]
                                                    
{'loss': 0.0092, 'grad_norm': 0.92578125, 'learning_rate': 6.302702702702702e-05, 'epoch': 18.49}

 37%|███▋      | 684/1850 [45:37<1:16:41,  3.95s/it]
 37%|███▋      | 685/1850 [45:41<1:15:35,  3.89s/it]
                                                    
{'loss': 0.0038, 'grad_norm': 0.59375, 'learning_rate': 6.297297297297298e-05, 'epoch': 18.51}

 37%|███▋      | 685/1850 [45:41<1:15:35,  3.89s/it]
 37%|███▋      | 686/1850 [45:45<1:17:50,  4.01s/it]
                                                    
{'loss': 0.0084, 'grad_norm': 1.359375, 'learning_rate': 6.291891891891893e-05, 'epoch': 18.54}

 37%|███▋      | 686/1850 [45:45<1:17:50,  4.01s/it]
 37%|███▋      | 687/1850 [45:49<1:17:11,  3.98s/it]
                                                    
{'loss': 0.0116, 'grad_norm': 2.203125, 'learning_rate': 6.286486486486486e-05, 'epoch': 18.57}

 37%|███▋      | 687/1850 [45:49<1:17:11,  3.98s/it]
 37%|███▋      | 688/1850 [45:52<1:14:51,  3.87s/it]
                                                    
{'loss': 0.0133, 'grad_norm': 0.75, 'learning_rate': 6.281081081081082e-05, 'epoch': 18.59}

 37%|███▋      | 688/1850 [45:52<1:14:51,  3.87s/it]
 37%|███▋      | 689/1850 [45:57<1:19:49,  4.12s/it]
                                                    
{'loss': 0.0061, 'grad_norm': 1.0859375, 'learning_rate': 6.275675675675677e-05, 'epoch': 18.62}

 37%|███▋      | 689/1850 [45:57<1:19:49,  4.12s/it]
 37%|███▋      | 690/1850 [46:01<1:18:09,  4.04s/it]
                                                    
{'loss': 0.0058, 'grad_norm': 0.98046875, 'learning_rate': 6.27027027027027e-05, 'epoch': 18.65}

 37%|███▋      | 690/1850 [46:01<1:18:09,  4.04s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 37%|███▋      | 691/1850 [46:06<1:24:43,  4.39s/it]
                                                    
{'loss': 0.0045, 'grad_norm': 0.5859375, 'learning_rate': 6.264864864864866e-05, 'epoch': 18.68}

 37%|███▋      | 691/1850 [46:06<1:24:43,  4.39s/it]
 37%|███▋      | 692/1850 [46:10<1:20:08,  4.15s/it]
                                                    
{'loss': 0.0042, 'grad_norm': 1.40625, 'learning_rate': 6.259459459459461e-05, 'epoch': 18.7}

 37%|███▋      | 692/1850 [46:10<1:20:08,  4.15s/it]
 37%|███▋      | 693/1850 [46:15<1:23:48,  4.35s/it]
                                                    
{'loss': 0.012, 'grad_norm': 1.6484375, 'learning_rate': 6.254054054054054e-05, 'epoch': 18.73}

 37%|███▋      | 693/1850 [46:15<1:23:48,  4.35s/it]
 38%|███▊      | 694/1850 [46:19<1:24:23,  4.38s/it]
                                                    
{'loss': 0.0058, 'grad_norm': 1.1015625, 'learning_rate': 6.248648648648648e-05, 'epoch': 18.76}

 38%|███▊      | 694/1850 [46:19<1:24:23,  4.38s/it]
 38%|███▊      | 695/1850 [46:23<1:21:23,  4.23s/it]
                                                    
{'loss': 0.0037, 'grad_norm': 0.357421875, 'learning_rate': 6.243243243243243e-05, 'epoch': 18.78}

 38%|███▊      | 695/1850 [46:23<1:21:23,  4.23s/it]
 38%|███▊      | 696/1850 [46:28<1:24:16,  4.38s/it]
                                                    
{'loss': 0.0057, 'grad_norm': 1.1640625, 'learning_rate': 6.237837837837837e-05, 'epoch': 18.81}

 38%|███▊      | 696/1850 [46:28<1:24:16,  4.38s/it]
 38%|███▊      | 697/1850 [46:31<1:20:40,  4.20s/it]
                                                    
{'loss': 0.0045, 'grad_norm': 0.94140625, 'learning_rate': 6.232432432432432e-05, 'epoch': 18.84}

 38%|███▊      | 697/1850 [46:31<1:20:40,  4.20s/it]
 38%|███▊      | 698/1850 [46:35<1:19:25,  4.14s/it]
                                                    
{'loss': 0.005, 'grad_norm': 1.890625, 'learning_rate': 6.227027027027027e-05, 'epoch': 18.86}

 38%|███▊      | 698/1850 [46:35<1:19:25,  4.14s/it]
 38%|███▊      | 699/1850 [46:39<1:14:09,  3.87s/it]
                                                    
{'loss': 0.0032, 'grad_norm': 0.58203125, 'learning_rate': 6.221621621621621e-05, 'epoch': 18.89}

 38%|███▊      | 699/1850 [46:39<1:14:09,  3.87s/it]
 38%|███▊      | 700/1850 [46:42<1:13:28,  3.83s/it]
                                                    
{'loss': 0.0032, 'grad_norm': 0.419921875, 'learning_rate': 6.216216216216216e-05, 'epoch': 18.92}

 38%|███▊      | 700/1850 [46:42<1:13:28,  3.83s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 38%|███▊      | 701/1850 [46:47<1:16:37,  4.00s/it]
                                                    
{'loss': 0.0058, 'grad_norm': 0.84375, 'learning_rate': 6.210810810810811e-05, 'epoch': 18.95}

 38%|███▊      | 701/1850 [46:47<1:16:37,  4.00s/it]
 38%|███▊      | 702/1850 [46:50<1:14:21,  3.89s/it]
                                                    
{'loss': 0.0044, 'grad_norm': 0.42578125, 'learning_rate': 6.205405405405405e-05, 'epoch': 18.97}

 38%|███▊      | 702/1850 [46:50<1:14:21,  3.89s/it]
 38%|███▊      | 703/1850 [46:53<1:09:14,  3.62s/it]
                                                    
{'loss': 0.0061, 'grad_norm': 1.03125, 'learning_rate': 6.2e-05, 'epoch': 19.0}

 38%|███▊      | 703/1850 [46:53<1:09:14,  3.62s/it]
 38%|███▊      | 704/1850 [46:58<1:16:39,  4.01s/it]
                                                    
{'loss': 0.0044, 'grad_norm': 0.68359375, 'learning_rate': 6.194594594594595e-05, 'epoch': 19.03}

 38%|███▊      | 704/1850 [46:58<1:16:39,  4.01s/it]
 38%|███▊      | 705/1850 [47:02<1:14:13,  3.89s/it]
                                                    
{'loss': 0.0045, 'grad_norm': 0.7578125, 'learning_rate': 6.189189189189189e-05, 'epoch': 19.05}

 38%|███▊      | 705/1850 [47:02<1:14:13,  3.89s/it]
 38%|███▊      | 706/1850 [47:06<1:13:59,  3.88s/it]
                                                    
{'loss': 0.0073, 'grad_norm': 1.8359375, 'learning_rate': 6.183783783783784e-05, 'epoch': 19.08}

 38%|███▊      | 706/1850 [47:06<1:13:59,  3.88s/it]
 38%|███▊      | 707/1850 [47:10<1:18:29,  4.12s/it]
                                                    
{'loss': 0.0076, 'grad_norm': 1.0859375, 'learning_rate': 6.17837837837838e-05, 'epoch': 19.11}

 38%|███▊      | 707/1850 [47:10<1:18:29,  4.12s/it]
 38%|███▊      | 708/1850 [47:14<1:15:42,  3.98s/it]
                                                    
{'loss': 0.0028, 'grad_norm': 0.53125, 'learning_rate': 6.172972972972973e-05, 'epoch': 19.14}

 38%|███▊      | 708/1850 [47:14<1:15:42,  3.98s/it]
 38%|███▊      | 709/1850 [47:18<1:17:06,  4.05s/it]
                                                    
{'loss': 0.0033, 'grad_norm': 0.6015625, 'learning_rate': 6.167567567567568e-05, 'epoch': 19.16}

 38%|███▊      | 709/1850 [47:18<1:17:06,  4.05s/it]
 38%|███▊      | 710/1850 [47:23<1:20:50,  4.26s/it]
                                                    
{'loss': 0.0035, 'grad_norm': 0.341796875, 'learning_rate': 6.162162162162163e-05, 'epoch': 19.19}

 38%|███▊      | 710/1850 [47:23<1:20:50,  4.26s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 38%|███▊      | 711/1850 [47:27<1:20:22,  4.23s/it]
                                                    
{'loss': 0.0033, 'grad_norm': 0.375, 'learning_rate': 6.156756756756757e-05, 'epoch': 19.22}

 38%|███▊      | 711/1850 [47:27<1:20:22,  4.23s/it]
 38%|███▊      | 712/1850 [47:32<1:23:15,  4.39s/it]
                                                    
{'loss': 0.0069, 'grad_norm': 0.99609375, 'learning_rate': 6.151351351351352e-05, 'epoch': 19.24}

 38%|███▊      | 712/1850 [47:32<1:23:15,  4.39s/it]
 39%|███▊      | 713/1850 [47:37<1:25:01,  4.49s/it]
                                                    
{'loss': 0.0035, 'grad_norm': 0.416015625, 'learning_rate': 6.145945945945946e-05, 'epoch': 19.27}

 39%|███▊      | 713/1850 [47:37<1:25:01,  4.49s/it]
 39%|███▊      | 714/1850 [47:41<1:24:18,  4.45s/it]
                                                    
{'loss': 0.0042, 'grad_norm': 0.80859375, 'learning_rate': 6.14054054054054e-05, 'epoch': 19.3}

 39%|███▊      | 714/1850 [47:41<1:24:18,  4.45s/it]
 39%|███▊      | 715/1850 [47:45<1:19:42,  4.21s/it]
                                                    
{'loss': 0.0018, 'grad_norm': 0.1259765625, 'learning_rate': 6.135135135135135e-05, 'epoch': 19.32}

 39%|███▊      | 715/1850 [47:45<1:19:42,  4.21s/it]
 39%|███▊      | 716/1850 [47:49<1:20:40,  4.27s/it]
                                                    
{'loss': 0.0027, 'grad_norm': 0.2138671875, 'learning_rate': 6.12972972972973e-05, 'epoch': 19.35}

 39%|███▊      | 716/1850 [47:49<1:20:40,  4.27s/it]
 39%|███▉      | 717/1850 [47:53<1:16:42,  4.06s/it]
                                                    
{'loss': 0.0068, 'grad_norm': 0.80078125, 'learning_rate': 6.124324324324324e-05, 'epoch': 19.38}

 39%|███▉      | 717/1850 [47:53<1:16:42,  4.06s/it]
 39%|███▉      | 718/1850 [47:56<1:12:25,  3.84s/it]
                                                    
{'loss': 0.0083, 'grad_norm': 0.85546875, 'learning_rate': 6.118918918918919e-05, 'epoch': 19.41}

 39%|███▉      | 718/1850 [47:56<1:12:25,  3.84s/it]
 39%|███▉      | 719/1850 [48:00<1:13:58,  3.92s/it]
                                                    
{'loss': 0.0049, 'grad_norm': 1.03125, 'learning_rate': 6.113513513513514e-05, 'epoch': 19.43}

 39%|███▉      | 719/1850 [48:00<1:13:58,  3.92s/it]
 39%|███▉      | 720/1850 [48:04<1:11:59,  3.82s/it]
                                                    
{'loss': 0.0038, 'grad_norm': 1.140625, 'learning_rate': 6.108108108108108e-05, 'epoch': 19.46}

 39%|███▉      | 720/1850 [48:04<1:11:59,  3.82s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 39%|███▉      | 721/1850 [48:08<1:13:11,  3.89s/it]
                                                    
{'loss': 0.0018, 'grad_norm': 0.12158203125, 'learning_rate': 6.102702702702703e-05, 'epoch': 19.49}

 39%|███▉      | 721/1850 [48:08<1:13:11,  3.89s/it]
 39%|███▉      | 722/1850 [48:12<1:13:11,  3.89s/it]
                                                    
{'loss': 0.0032, 'grad_norm': 0.3828125, 'learning_rate': 6.0972972972972974e-05, 'epoch': 19.51}

 39%|███▉      | 722/1850 [48:12<1:13:11,  3.89s/it]
 39%|███▉      | 723/1850 [48:16<1:15:22,  4.01s/it]
                                                    
{'loss': 0.0049, 'grad_norm': 0.96875, 'learning_rate': 6.091891891891892e-05, 'epoch': 19.54}

 39%|███▉      | 723/1850 [48:16<1:15:22,  4.01s/it]
 39%|███▉      | 724/1850 [48:20<1:15:27,  4.02s/it]
                                                    
{'loss': 0.0034, 'grad_norm': 0.57421875, 'learning_rate': 6.086486486486487e-05, 'epoch': 19.57}

 39%|███▉      | 724/1850 [48:20<1:15:27,  4.02s/it]
 39%|███▉      | 725/1850 [48:24<1:16:40,  4.09s/it]
                                                    
{'loss': 0.0068, 'grad_norm': 0.4765625, 'learning_rate': 6.0810810810810814e-05, 'epoch': 19.59}

 39%|███▉      | 725/1850 [48:24<1:16:40,  4.09s/it]
 39%|███▉      | 726/1850 [48:28<1:16:03,  4.06s/it]
                                                    
{'loss': 0.0048, 'grad_norm': 1.1875, 'learning_rate': 6.075675675675676e-05, 'epoch': 19.62}

 39%|███▉      | 726/1850 [48:28<1:16:03,  4.06s/it]
 39%|███▉      | 727/1850 [48:32<1:13:19,  3.92s/it]
                                                    
{'loss': 0.004, 'grad_norm': 0.62890625, 'learning_rate': 6.070270270270271e-05, 'epoch': 19.65}

 39%|███▉      | 727/1850 [48:32<1:13:19,  3.92s/it]
 39%|███▉      | 728/1850 [48:35<1:11:35,  3.83s/it]
                                                    
{'loss': 0.0048, 'grad_norm': 0.6796875, 'learning_rate': 6.0648648648648655e-05, 'epoch': 19.68}

 39%|███▉      | 728/1850 [48:35<1:11:35,  3.83s/it]
 39%|███▉      | 729/1850 [48:39<1:11:12,  3.81s/it]
                                                    
{'loss': 0.0032, 'grad_norm': 0.3203125, 'learning_rate': 6.05945945945946e-05, 'epoch': 19.7}

 39%|███▉      | 729/1850 [48:39<1:11:12,  3.81s/it]
 39%|███▉      | 730/1850 [48:43<1:09:24,  3.72s/it]
                                                    
{'loss': 0.002, 'grad_norm': 0.298828125, 'learning_rate': 6.0540540540540543e-05, 'epoch': 19.73}

 39%|███▉      | 730/1850 [48:43<1:09:24,  3.72s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 40%|███▉      | 731/1850 [48:47<1:10:53,  3.80s/it]
                                                    
{'loss': 0.0028, 'grad_norm': 0.291015625, 'learning_rate': 6.0486486486486495e-05, 'epoch': 19.76}

 40%|███▉      | 731/1850 [48:47<1:10:53,  3.80s/it]
 40%|███▉      | 732/1850 [48:51<1:14:52,  4.02s/it]
                                                    
{'loss': 0.0101, 'grad_norm': 2.1875, 'learning_rate': 6.043243243243244e-05, 'epoch': 19.78}

 40%|███▉      | 732/1850 [48:51<1:14:52,  4.02s/it]
 40%|███▉      | 733/1850 [48:55<1:10:41,  3.80s/it]
                                                    
{'loss': 0.0023, 'grad_norm': 0.220703125, 'learning_rate': 6.0378378378378384e-05, 'epoch': 19.81}

 40%|███▉      | 733/1850 [48:55<1:10:41,  3.80s/it]
 40%|███▉      | 734/1850 [48:59<1:11:55,  3.87s/it]
                                                    
{'loss': 0.0026, 'grad_norm': 0.34375, 'learning_rate': 6.032432432432432e-05, 'epoch': 19.84}

 40%|███▉      | 734/1850 [48:59<1:11:55,  3.87s/it]
 40%|███▉      | 735/1850 [49:03<1:13:53,  3.98s/it]
                                                    
{'loss': 0.0091, 'grad_norm': 1.4296875, 'learning_rate': 6.0270270270270266e-05, 'epoch': 19.86}

 40%|███▉      | 735/1850 [49:03<1:13:53,  3.98s/it]
 40%|███▉      | 736/1850 [49:07<1:12:34,  3.91s/it]
                                                    
{'loss': 0.0169, 'grad_norm': 2.765625, 'learning_rate': 6.021621621621622e-05, 'epoch': 19.89}

 40%|███▉      | 736/1850 [49:07<1:12:34,  3.91s/it]
 40%|███▉      | 737/1850 [49:11<1:13:05,  3.94s/it]
                                                    
{'loss': 0.0026, 'grad_norm': 0.39453125, 'learning_rate': 6.016216216216216e-05, 'epoch': 19.92}

 40%|███▉      | 737/1850 [49:11<1:13:05,  3.94s/it]
 40%|███▉      | 738/1850 [49:14<1:09:15,  3.74s/it]
                                                    
{'loss': 0.0071, 'grad_norm': 1.8671875, 'learning_rate': 6.0108108108108106e-05, 'epoch': 19.95}

 40%|███▉      | 738/1850 [49:14<1:09:15,  3.74s/it]
 40%|███▉      | 739/1850 [49:18<1:14:14,  4.01s/it]
                                                    
{'loss': 0.0221, 'grad_norm': 1.6015625, 'learning_rate': 6.005405405405406e-05, 'epoch': 19.97}

 40%|███▉      | 739/1850 [49:18<1:14:14,  4.01s/it]
 40%|████      | 740/1850 [49:22<1:09:19,  3.75s/it]
                                                    
{'loss': 0.0044, 'grad_norm': 0.5859375, 'learning_rate': 6e-05, 'epoch': 20.0}

 40%|████      | 740/1850 [49:22<1:09:19,  3.75s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 40%|████      | 741/1850 [49:26<1:13:23,  3.97s/it]
                                                    
{'loss': 0.0072, 'grad_norm': 1.1796875, 'learning_rate': 5.9945945945945946e-05, 'epoch': 20.03}

 40%|████      | 741/1850 [49:26<1:13:23,  3.97s/it]
 40%|████      | 742/1850 [49:30<1:10:26,  3.81s/it]
                                                    
{'loss': 0.0015, 'grad_norm': 0.0751953125, 'learning_rate': 5.989189189189189e-05, 'epoch': 20.05}

 40%|████      | 742/1850 [49:30<1:10:26,  3.81s/it]
 40%|████      | 743/1850 [49:34<1:12:16,  3.92s/it]
                                                    
{'loss': 0.0023, 'grad_norm': 0.318359375, 'learning_rate': 5.983783783783784e-05, 'epoch': 20.08}

 40%|████      | 743/1850 [49:34<1:12:16,  3.92s/it]
 40%|████      | 744/1850 [49:37<1:08:47,  3.73s/it]
                                                    
{'loss': 0.0021, 'grad_norm': 0.216796875, 'learning_rate': 5.9783783783783786e-05, 'epoch': 20.11}

 40%|████      | 744/1850 [49:37<1:08:47,  3.73s/it]
 40%|████      | 745/1850 [49:40<1:06:58,  3.64s/it]
                                                    
{'loss': 0.0143, 'grad_norm': 1.4296875, 'learning_rate': 5.972972972972973e-05, 'epoch': 20.14}

 40%|████      | 745/1850 [49:40<1:06:58,  3.64s/it]
 40%|████      | 746/1850 [49:44<1:07:58,  3.69s/it]
                                                    
{'loss': 0.0022, 'grad_norm': 0.349609375, 'learning_rate': 5.967567567567568e-05, 'epoch': 20.16}

 40%|████      | 746/1850 [49:44<1:07:58,  3.69s/it]
 40%|████      | 747/1850 [49:49<1:11:21,  3.88s/it]
                                                    
{'loss': 0.005, 'grad_norm': 0.298828125, 'learning_rate': 5.9621621621621626e-05, 'epoch': 20.19}

 40%|████      | 747/1850 [49:49<1:11:21,  3.88s/it]
 40%|████      | 748/1850 [49:53<1:13:16,  3.99s/it]
                                                    
{'loss': 0.0025, 'grad_norm': 0.32421875, 'learning_rate': 5.956756756756757e-05, 'epoch': 20.22}

 40%|████      | 748/1850 [49:53<1:13:16,  3.99s/it]
 40%|████      | 749/1850 [49:56<1:10:45,  3.86s/it]
                                                    
{'loss': 0.0018, 'grad_norm': 0.091796875, 'learning_rate': 5.951351351351352e-05, 'epoch': 20.24}

 40%|████      | 749/1850 [49:56<1:10:45,  3.86s/it]
 41%|████      | 750/1850 [50:01<1:13:33,  4.01s/it]
                                                    
{'loss': 0.0029, 'grad_norm': 0.33984375, 'learning_rate': 5.9459459459459466e-05, 'epoch': 20.27}

 41%|████      | 750/1850 [50:01<1:13:33,  4.01s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 41%|████      | 751/1850 [50:05<1:16:13,  4.16s/it]
                                                    
{'loss': 0.007, 'grad_norm': 1.1484375, 'learning_rate': 5.940540540540541e-05, 'epoch': 20.3}

 41%|████      | 751/1850 [50:05<1:16:13,  4.16s/it]
 41%|████      | 752/1850 [50:10<1:17:57,  4.26s/it]
                                                    
{'loss': 0.0019, 'grad_norm': 0.16015625, 'learning_rate': 5.9351351351351355e-05, 'epoch': 20.32}

 41%|████      | 752/1850 [50:10<1:17:57,  4.26s/it]
 41%|████      | 753/1850 [50:14<1:19:09,  4.33s/it]
                                                    
{'loss': 0.0028, 'grad_norm': 0.3046875, 'learning_rate': 5.9297297297297306e-05, 'epoch': 20.35}

 41%|████      | 753/1850 [50:14<1:19:09,  4.33s/it]
 41%|████      | 754/1850 [50:19<1:18:43,  4.31s/it]
                                                    
{'loss': 0.0043, 'grad_norm': 1.0546875, 'learning_rate': 5.924324324324325e-05, 'epoch': 20.38}

 41%|████      | 754/1850 [50:19<1:18:43,  4.31s/it]
 41%|████      | 755/1850 [50:23<1:17:04,  4.22s/it]
                                                    
{'loss': 0.003, 'grad_norm': 0.298828125, 'learning_rate': 5.918918918918919e-05, 'epoch': 20.41}

 41%|████      | 755/1850 [50:23<1:17:04,  4.22s/it]
 41%|████      | 756/1850 [50:27<1:20:13,  4.40s/it]
                                                    
{'loss': 0.0103, 'grad_norm': 2.5, 'learning_rate': 5.913513513513513e-05, 'epoch': 20.43}

 41%|████      | 756/1850 [50:27<1:20:13,  4.40s/it]
 41%|████      | 757/1850 [50:31<1:18:21,  4.30s/it]
                                                    
{'loss': 0.0019, 'grad_norm': 0.1484375, 'learning_rate': 5.908108108108108e-05, 'epoch': 20.46}

 41%|████      | 757/1850 [50:31<1:18:21,  4.30s/it]
 41%|████      | 758/1850 [50:36<1:18:07,  4.29s/it]
                                                    
{'loss': 0.0049, 'grad_norm': 0.91015625, 'learning_rate': 5.902702702702703e-05, 'epoch': 20.49}

 41%|████      | 758/1850 [50:36<1:18:07,  4.29s/it]
 41%|████      | 759/1850 [50:39<1:14:00,  4.07s/it]
                                                    
{'loss': 0.0375, 'grad_norm': 1.7265625, 'learning_rate': 5.897297297297297e-05, 'epoch': 20.51}

 41%|████      | 759/1850 [50:39<1:14:00,  4.07s/it]
 41%|████      | 760/1850 [50:43<1:12:57,  4.02s/it]
                                                    
{'loss': 0.0019, 'grad_norm': 0.2255859375, 'learning_rate': 5.891891891891892e-05, 'epoch': 20.54}

 41%|████      | 760/1850 [50:43<1:12:57,  4.02s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 41%|████      | 761/1850 [50:47<1:12:32,  4.00s/it]
                                                    
{'loss': 0.0022, 'grad_norm': 0.439453125, 'learning_rate': 5.886486486486487e-05, 'epoch': 20.57}

 41%|████      | 761/1850 [50:47<1:12:32,  4.00s/it]
 41%|████      | 762/1850 [50:51<1:11:01,  3.92s/it]
                                                    
{'loss': 0.0023, 'grad_norm': 0.357421875, 'learning_rate': 5.881081081081081e-05, 'epoch': 20.59}

 41%|████      | 762/1850 [50:51<1:11:01,  3.92s/it]
 41%|████      | 763/1850 [50:55<1:10:25,  3.89s/it]
                                                    
{'loss': 0.0019, 'grad_norm': 0.1162109375, 'learning_rate': 5.875675675675676e-05, 'epoch': 20.62}

 41%|████      | 763/1850 [50:55<1:10:25,  3.89s/it]
 41%|████▏     | 764/1850 [50:59<1:12:18,  4.00s/it]
                                                    
{'loss': 0.0022, 'grad_norm': 0.125, 'learning_rate': 5.87027027027027e-05, 'epoch': 20.65}

 41%|████▏     | 764/1850 [50:59<1:12:18,  4.00s/it]
 41%|████▏     | 765/1850 [51:03<1:12:52,  4.03s/it]
                                                    
{'loss': 0.0025, 'grad_norm': 1.3046875, 'learning_rate': 5.8648648648648653e-05, 'epoch': 20.68}

 41%|████▏     | 765/1850 [51:03<1:12:52,  4.03s/it]
 41%|████▏     | 766/1850 [51:07<1:11:09,  3.94s/it]
                                                    
{'loss': 0.0045, 'grad_norm': 0.7421875, 'learning_rate': 5.85945945945946e-05, 'epoch': 20.7}

 41%|████▏     | 766/1850 [51:07<1:11:09,  3.94s/it]
 41%|████▏     | 767/1850 [51:11<1:12:17,  4.00s/it]
                                                    
{'loss': 0.0035, 'grad_norm': 0.40625, 'learning_rate': 5.854054054054054e-05, 'epoch': 20.73}

 41%|████▏     | 767/1850 [51:11<1:12:17,  4.00s/it]
 42%|████▏     | 768/1850 [51:15<1:11:37,  3.97s/it]
                                                    
{'loss': 0.0081, 'grad_norm': 0.6484375, 'learning_rate': 5.8486486486486494e-05, 'epoch': 20.76}

 42%|████▏     | 768/1850 [51:15<1:11:37,  3.97s/it]
 42%|████▏     | 769/1850 [51:19<1:12:22,  4.02s/it]
                                                    
{'loss': 0.0022, 'grad_norm': 0.1171875, 'learning_rate': 5.843243243243244e-05, 'epoch': 20.78}

 42%|████▏     | 769/1850 [51:19<1:12:22,  4.02s/it]
 42%|████▏     | 770/1850 [51:23<1:10:56,  3.94s/it]
                                                    
{'loss': 0.0021, 'grad_norm': 0.140625, 'learning_rate': 5.837837837837838e-05, 'epoch': 20.81}

 42%|████▏     | 770/1850 [51:23<1:10:56,  3.94s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 42%|████▏     | 771/1850 [51:27<1:13:11,  4.07s/it]
                                                    
{'loss': 0.0023, 'grad_norm': 0.11767578125, 'learning_rate': 5.8324324324324334e-05, 'epoch': 20.84}

 42%|████▏     | 771/1850 [51:27<1:13:11,  4.07s/it]
 42%|████▏     | 772/1850 [51:31<1:10:34,  3.93s/it]
                                                    
{'loss': 0.0022, 'grad_norm': 0.1806640625, 'learning_rate': 5.827027027027028e-05, 'epoch': 20.86}

 42%|████▏     | 772/1850 [51:31<1:10:34,  3.93s/it]
 42%|████▏     | 773/1850 [51:34<1:09:01,  3.85s/it]
                                                    
{'loss': 0.0027, 'grad_norm': 0.259765625, 'learning_rate': 5.821621621621622e-05, 'epoch': 20.89}

 42%|████▏     | 773/1850 [51:34<1:09:01,  3.85s/it]
 42%|████▏     | 774/1850 [51:38<1:10:27,  3.93s/it]
                                                    
{'loss': 0.0096, 'grad_norm': 1.78125, 'learning_rate': 5.8162162162162174e-05, 'epoch': 20.92}

 42%|████▏     | 774/1850 [51:38<1:10:27,  3.93s/it]
 42%|████▏     | 775/1850 [51:42<1:09:10,  3.86s/it]
                                                    
{'loss': 0.0058, 'grad_norm': 2.171875, 'learning_rate': 5.8108108108108105e-05, 'epoch': 20.95}

 42%|████▏     | 775/1850 [51:42<1:09:10,  3.86s/it]
 42%|████▏     | 776/1850 [51:45<1:06:20,  3.71s/it]
                                                    
{'loss': 0.003, 'grad_norm': 0.73828125, 'learning_rate': 5.805405405405405e-05, 'epoch': 20.97}

 42%|████▏     | 776/1850 [51:45<1:06:20,  3.71s/it]
 42%|████▏     | 777/1850 [51:49<1:06:44,  3.73s/it]
                                                    
{'loss': 0.0069, 'grad_norm': 1.21875, 'learning_rate': 5.8e-05, 'epoch': 21.0}

 42%|████▏     | 777/1850 [51:49<1:06:44,  3.73s/it]
 42%|████▏     | 778/1850 [51:53<1:07:46,  3.79s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.08837890625, 'learning_rate': 5.7945945945945945e-05, 'epoch': 21.03}

 42%|████▏     | 778/1850 [51:53<1:07:46,  3.79s/it]
 42%|████▏     | 779/1850 [51:58<1:12:49,  4.08s/it]
                                                    
{'loss': 0.0021, 'grad_norm': 0.1787109375, 'learning_rate': 5.789189189189189e-05, 'epoch': 21.05}

 42%|████▏     | 779/1850 [51:58<1:12:49,  4.08s/it]
 42%|████▏     | 780/1850 [52:02<1:12:09,  4.05s/it]
                                                    
{'loss': 0.0019, 'grad_norm': 0.1328125, 'learning_rate': 5.783783783783784e-05, 'epoch': 21.08}

 42%|████▏     | 780/1850 [52:02<1:12:09,  4.05s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 42%|████▏     | 781/1850 [52:06<1:14:27,  4.18s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.06640625, 'learning_rate': 5.7783783783783785e-05, 'epoch': 21.11}

 42%|████▏     | 781/1850 [52:06<1:14:27,  4.18s/it]
 42%|████▏     | 782/1850 [52:10<1:10:41,  3.97s/it]
                                                    
{'loss': 0.0014, 'grad_norm': 0.087890625, 'learning_rate': 5.772972972972973e-05, 'epoch': 21.14}

 42%|████▏     | 782/1850 [52:10<1:10:41,  3.97s/it]
 42%|████▏     | 783/1850 [52:14<1:10:47,  3.98s/it]
                                                    
{'loss': 0.0019, 'grad_norm': 0.06884765625, 'learning_rate': 5.767567567567568e-05, 'epoch': 21.16}

 42%|████▏     | 783/1850 [52:14<1:10:47,  3.98s/it]
 42%|████▏     | 784/1850 [52:18<1:12:32,  4.08s/it]
                                                    
{'loss': 0.0025, 'grad_norm': 0.166015625, 'learning_rate': 5.7621621621621625e-05, 'epoch': 21.19}

 42%|████▏     | 784/1850 [52:18<1:12:32,  4.08s/it]
 42%|████▏     | 785/1850 [52:22<1:12:24,  4.08s/it]
                                                    
{'loss': 0.0027, 'grad_norm': 0.1826171875, 'learning_rate': 5.756756756756757e-05, 'epoch': 21.22}

 42%|████▏     | 785/1850 [52:22<1:12:24,  4.08s/it]
 42%|████▏     | 786/1850 [52:26<1:11:49,  4.05s/it]
                                                    
{'loss': 0.0072, 'grad_norm': 1.3046875, 'learning_rate': 5.7513513513513514e-05, 'epoch': 21.24}

 42%|████▏     | 786/1850 [52:26<1:11:49,  4.05s/it]
 43%|████▎     | 787/1850 [52:31<1:13:49,  4.17s/it]
                                                    
{'loss': 0.0028, 'grad_norm': 0.267578125, 'learning_rate': 5.7459459459459465e-05, 'epoch': 21.27}

 43%|████▎     | 787/1850 [52:31<1:13:49,  4.17s/it]
 43%|████▎     | 788/1850 [52:35<1:13:06,  4.13s/it]
                                                    
{'loss': 0.0026, 'grad_norm': 0.76953125, 'learning_rate': 5.740540540540541e-05, 'epoch': 21.3}

 43%|████▎     | 788/1850 [52:35<1:13:06,  4.13s/it]
 43%|████▎     | 789/1850 [52:39<1:11:47,  4.06s/it]
                                                    
{'loss': 0.0022, 'grad_norm': 0.158203125, 'learning_rate': 5.7351351351351354e-05, 'epoch': 21.32}

 43%|████▎     | 789/1850 [52:39<1:11:47,  4.06s/it]
 43%|████▎     | 790/1850 [52:42<1:09:11,  3.92s/it]
                                                    
{'loss': 0.0066, 'grad_norm': 1.625, 'learning_rate': 5.7297297297297305e-05, 'epoch': 21.35}

 43%|████▎     | 790/1850 [52:42<1:09:11,  3.92s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 43%|████▎     | 791/1850 [52:47<1:11:59,  4.08s/it]
                                                    
{'loss': 0.0015, 'grad_norm': 0.119140625, 'learning_rate': 5.724324324324325e-05, 'epoch': 21.38}

 43%|████▎     | 791/1850 [52:47<1:11:59,  4.08s/it]
 43%|████▎     | 792/1850 [52:51<1:12:38,  4.12s/it]
                                                    
{'loss': 0.0023, 'grad_norm': 0.212890625, 'learning_rate': 5.7189189189189194e-05, 'epoch': 21.41}

 43%|████▎     | 792/1850 [52:51<1:12:38,  4.12s/it]
 43%|████▎     | 793/1850 [52:55<1:11:53,  4.08s/it]
                                                    
{'loss': 0.0019, 'grad_norm': 0.08984375, 'learning_rate': 5.7135135135135145e-05, 'epoch': 21.43}

 43%|████▎     | 793/1850 [52:55<1:11:53,  4.08s/it]
 43%|████▎     | 794/1850 [52:59<1:10:00,  3.98s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.302734375, 'learning_rate': 5.708108108108109e-05, 'epoch': 21.46}

 43%|████▎     | 794/1850 [52:59<1:10:00,  3.98s/it]
 43%|████▎     | 795/1850 [53:03<1:09:46,  3.97s/it]
                                                    
{'loss': 0.0049, 'grad_norm': 0.83984375, 'learning_rate': 5.702702702702703e-05, 'epoch': 21.49}

 43%|████▎     | 795/1850 [53:03<1:09:46,  3.97s/it]
 43%|████▎     | 796/1850 [53:06<1:07:34,  3.85s/it]
                                                    
{'loss': 0.0014, 'grad_norm': 0.080078125, 'learning_rate': 5.697297297297297e-05, 'epoch': 21.51}

 43%|████▎     | 796/1850 [53:06<1:07:34,  3.85s/it]
 43%|████▎     | 797/1850 [53:10<1:08:18,  3.89s/it]
                                                    
{'loss': 0.0025, 'grad_norm': 0.703125, 'learning_rate': 5.6918918918918916e-05, 'epoch': 21.54}

 43%|████▎     | 797/1850 [53:10<1:08:18,  3.89s/it]
 43%|████▎     | 798/1850 [53:14<1:06:55,  3.82s/it]
                                                    
{'loss': 0.0013, 'grad_norm': 0.0625, 'learning_rate': 5.686486486486486e-05, 'epoch': 21.57}

 43%|████▎     | 798/1850 [53:14<1:06:55,  3.82s/it]
 43%|████▎     | 799/1850 [53:18<1:09:08,  3.95s/it]
                                                    
{'loss': 0.0022, 'grad_norm': 0.3125, 'learning_rate': 5.681081081081081e-05, 'epoch': 21.59}

 43%|████▎     | 799/1850 [53:18<1:09:08,  3.95s/it]
 43%|████▎     | 800/1850 [53:22<1:09:12,  3.95s/it]
                                                    
{'loss': 0.0105, 'grad_norm': 1.96875, 'learning_rate': 5.6756756756756757e-05, 'epoch': 21.62}

 43%|████▎     | 800/1850 [53:22<1:09:12,  3.95s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 43%|████▎     | 801/1850 [53:26<1:08:49,  3.94s/it]
                                                    
{'loss': 0.0017, 'grad_norm': 0.150390625, 'learning_rate': 5.67027027027027e-05, 'epoch': 21.65}

 43%|████▎     | 801/1850 [53:26<1:08:49,  3.94s/it]
 43%|████▎     | 802/1850 [53:30<1:09:59,  4.01s/it]
                                                    
{'loss': 0.0063, 'grad_norm': 0.75, 'learning_rate': 5.664864864864865e-05, 'epoch': 21.68}

 43%|████▎     | 802/1850 [53:30<1:09:59,  4.01s/it]
 43%|████▎     | 803/1850 [53:35<1:12:25,  4.15s/it]
                                                    
{'loss': 0.0037, 'grad_norm': 1.3203125, 'learning_rate': 5.65945945945946e-05, 'epoch': 21.7}

 43%|████▎     | 803/1850 [53:35<1:12:25,  4.15s/it]
 43%|████▎     | 804/1850 [53:39<1:12:54,  4.18s/it]
                                                    
{'loss': 0.0086, 'grad_norm': 0.3828125, 'learning_rate': 5.654054054054054e-05, 'epoch': 21.73}

 43%|████▎     | 804/1850 [53:39<1:12:54,  4.18s/it]
 44%|████▎     | 805/1850 [53:43<1:12:10,  4.14s/it]
                                                    
{'loss': 0.0077, 'grad_norm': 1.3359375, 'learning_rate': 5.648648648648649e-05, 'epoch': 21.76}

 44%|████▎     | 805/1850 [53:43<1:12:10,  4.14s/it]
 44%|████▎     | 806/1850 [53:46<1:09:18,  3.98s/it]
                                                    
{'loss': 0.0015, 'grad_norm': 0.08984375, 'learning_rate': 5.643243243243244e-05, 'epoch': 21.78}

 44%|████▎     | 806/1850 [53:46<1:09:18,  3.98s/it]
 44%|████▎     | 807/1850 [53:51<1:10:20,  4.05s/it]
                                                    
{'loss': 0.0056, 'grad_norm': 1.1640625, 'learning_rate': 5.637837837837838e-05, 'epoch': 21.81}

 44%|████▎     | 807/1850 [53:51<1:10:20,  4.05s/it]
 44%|████▎     | 808/1850 [53:54<1:06:59,  3.86s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.08740234375, 'learning_rate': 5.6324324324324326e-05, 'epoch': 21.84}

 44%|████▎     | 808/1850 [53:54<1:06:59,  3.86s/it]
 44%|████▎     | 809/1850 [53:58<1:07:08,  3.87s/it]
                                                    
{'loss': 0.002, 'grad_norm': 0.228515625, 'learning_rate': 5.627027027027028e-05, 'epoch': 21.86}

 44%|████▎     | 809/1850 [53:58<1:07:08,  3.87s/it]
 44%|████▍     | 810/1850 [54:01<1:02:39,  3.61s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.0751953125, 'learning_rate': 5.621621621621622e-05, 'epoch': 21.89}

 44%|████▍     | 810/1850 [54:01<1:02:39,  3.61s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 44%|████▍     | 811/1850 [54:06<1:07:48,  3.92s/it]
                                                    
{'loss': 0.0035, 'grad_norm': 0.66015625, 'learning_rate': 5.6162162162162166e-05, 'epoch': 21.92}

 44%|████▍     | 811/1850 [54:06<1:07:48,  3.92s/it]
 44%|████▍     | 812/1850 [54:09<1:05:50,  3.81s/it]
                                                    
{'loss': 0.0021, 'grad_norm': 0.39453125, 'learning_rate': 5.610810810810812e-05, 'epoch': 21.95}

 44%|████▍     | 812/1850 [54:09<1:05:50,  3.81s/it]
 44%|████▍     | 813/1850 [54:13<1:06:20,  3.84s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.0693359375, 'learning_rate': 5.605405405405406e-05, 'epoch': 21.97}

 44%|████▍     | 813/1850 [54:13<1:06:20,  3.84s/it]
 44%|████▍     | 814/1850 [54:17<1:08:20,  3.96s/it]
                                                    
{'loss': 0.0072, 'grad_norm': 2.875, 'learning_rate': 5.6000000000000006e-05, 'epoch': 22.0}

 44%|████▍     | 814/1850 [54:17<1:08:20,  3.96s/it]
 44%|████▍     | 815/1850 [54:21<1:09:25,  4.02s/it]
                                                    
{'loss': 0.0021, 'grad_norm': 0.138671875, 'learning_rate': 5.5945945945945944e-05, 'epoch': 22.03}

 44%|████▍     | 815/1850 [54:21<1:09:25,  4.02s/it]
 44%|████▍     | 816/1850 [54:26<1:11:17,  4.14s/it]
                                                    
{'loss': 0.0162, 'grad_norm': 2.390625, 'learning_rate': 5.589189189189189e-05, 'epoch': 22.05}

 44%|████▍     | 816/1850 [54:26<1:11:17,  4.14s/it]
 44%|████▍     | 817/1850 [54:29<1:07:10,  3.90s/it]
                                                    
{'loss': 0.0017, 'grad_norm': 0.3984375, 'learning_rate': 5.583783783783784e-05, 'epoch': 22.08}

 44%|████▍     | 817/1850 [54:29<1:07:10,  3.90s/it]
 44%|████▍     | 818/1850 [54:33<1:07:57,  3.95s/it]
                                                    
{'loss': 0.002, 'grad_norm': 0.1279296875, 'learning_rate': 5.5783783783783784e-05, 'epoch': 22.11}

 44%|████▍     | 818/1850 [54:33<1:07:57,  3.95s/it]
 44%|████▍     | 819/1850 [54:37<1:05:02,  3.79s/it]
                                                    
{'loss': 0.0017, 'grad_norm': 0.447265625, 'learning_rate': 5.572972972972973e-05, 'epoch': 22.14}

 44%|████▍     | 819/1850 [54:37<1:05:02,  3.79s/it]
 44%|████▍     | 820/1850 [54:41<1:06:46,  3.89s/it]
                                                    
{'loss': 0.0025, 'grad_norm': 0.55078125, 'learning_rate': 5.567567567567567e-05, 'epoch': 22.16}

 44%|████▍     | 820/1850 [54:41<1:06:46,  3.89s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 44%|████▍     | 821/1850 [54:45<1:07:33,  3.94s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.1787109375, 'learning_rate': 5.5621621621621624e-05, 'epoch': 22.19}

 44%|████▍     | 821/1850 [54:45<1:07:33,  3.94s/it]
 44%|████▍     | 822/1850 [54:49<1:08:42,  4.01s/it]
                                                    
{'loss': 0.0018, 'grad_norm': 0.2177734375, 'learning_rate': 5.556756756756757e-05, 'epoch': 22.22}

 44%|████▍     | 822/1850 [54:49<1:08:42,  4.01s/it]
 44%|████▍     | 823/1850 [54:53<1:06:36,  3.89s/it]
                                                    
{'loss': 0.0013, 'grad_norm': 0.0693359375, 'learning_rate': 5.551351351351351e-05, 'epoch': 22.24}

 44%|████▍     | 823/1850 [54:53<1:06:36,  3.89s/it]
 45%|████▍     | 824/1850 [54:57<1:07:49,  3.97s/it]
                                                    
{'loss': 0.0032, 'grad_norm': 2.25, 'learning_rate': 5.5459459459459464e-05, 'epoch': 22.27}

 45%|████▍     | 824/1850 [54:57<1:07:49,  3.97s/it]
 45%|████▍     | 825/1850 [55:00<1:05:52,  3.86s/it]
                                                    
{'loss': 0.0013, 'grad_norm': 0.068359375, 'learning_rate': 5.540540540540541e-05, 'epoch': 22.3}

 45%|████▍     | 825/1850 [55:00<1:05:52,  3.86s/it]
 45%|████▍     | 826/1850 [55:04<1:05:31,  3.84s/it]
                                                    
{'loss': 0.0018, 'grad_norm': 0.1513671875, 'learning_rate': 5.535135135135135e-05, 'epoch': 22.32}

 45%|████▍     | 826/1850 [55:04<1:05:31,  3.84s/it]
 45%|████▍     | 827/1850 [55:09<1:09:21,  4.07s/it]
                                                    
{'loss': 0.0061, 'grad_norm': 1.0234375, 'learning_rate': 5.5297297297297304e-05, 'epoch': 22.35}

 45%|████▍     | 827/1850 [55:09<1:09:21,  4.07s/it]
 45%|████▍     | 828/1850 [55:12<1:05:17,  3.83s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.09716796875, 'learning_rate': 5.524324324324325e-05, 'epoch': 22.38}

 45%|████▍     | 828/1850 [55:12<1:05:17,  3.83s/it]
 45%|████▍     | 829/1850 [55:16<1:03:33,  3.74s/it]
                                                    
{'loss': 0.0015, 'grad_norm': 0.4296875, 'learning_rate': 5.518918918918919e-05, 'epoch': 22.41}

 45%|████▍     | 829/1850 [55:16<1:03:33,  3.74s/it]
 45%|████▍     | 830/1850 [55:20<1:06:28,  3.91s/it]
                                                    
{'loss': 0.0021, 'grad_norm': 0.2578125, 'learning_rate': 5.5135135135135144e-05, 'epoch': 22.43}

 45%|████▍     | 830/1850 [55:20<1:06:28,  3.91s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 45%|████▍     | 831/1850 [55:24<1:07:23,  3.97s/it]
                                                    
{'loss': 0.0014, 'grad_norm': 0.0986328125, 'learning_rate': 5.508108108108109e-05, 'epoch': 22.46}

 45%|████▍     | 831/1850 [55:24<1:07:23,  3.97s/it]
 45%|████▍     | 832/1850 [55:29<1:12:01,  4.24s/it]
                                                    
{'loss': 0.0023, 'grad_norm': 0.09619140625, 'learning_rate': 5.502702702702703e-05, 'epoch': 22.49}

 45%|████▍     | 832/1850 [55:29<1:12:01,  4.24s/it]
 45%|████▌     | 833/1850 [55:33<1:11:59,  4.25s/it]
                                                    
{'loss': 0.0084, 'grad_norm': 1.2890625, 'learning_rate': 5.497297297297298e-05, 'epoch': 22.51}

 45%|████▌     | 833/1850 [55:33<1:11:59,  4.25s/it]
 45%|████▌     | 834/1850 [55:37<1:08:46,  4.06s/it]
                                                    
{'loss': 0.0014, 'grad_norm': 0.12890625, 'learning_rate': 5.491891891891893e-05, 'epoch': 22.54}

 45%|████▌     | 834/1850 [55:37<1:08:46,  4.06s/it]
 45%|████▌     | 835/1850 [55:41<1:10:08,  4.15s/it]
                                                    
{'loss': 0.0022, 'grad_norm': 0.3359375, 'learning_rate': 5.486486486486486e-05, 'epoch': 22.57}

 45%|████▌     | 835/1850 [55:41<1:10:08,  4.15s/it]
 45%|████▌     | 836/1850 [55:46<1:11:33,  4.23s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.0908203125, 'learning_rate': 5.481081081081081e-05, 'epoch': 22.59}

 45%|████▌     | 836/1850 [55:46<1:11:33,  4.23s/it]
 45%|████▌     | 837/1850 [55:49<1:08:30,  4.06s/it]
                                                    
{'loss': 0.003, 'grad_norm': 0.75, 'learning_rate': 5.4756756756756755e-05, 'epoch': 22.62}

 45%|████▌     | 837/1850 [55:49<1:08:30,  4.06s/it]
 45%|████▌     | 838/1850 [55:53<1:08:43,  4.07s/it]
                                                    
{'loss': 0.0025, 'grad_norm': 0.75390625, 'learning_rate': 5.47027027027027e-05, 'epoch': 22.65}

 45%|████▌     | 838/1850 [55:53<1:08:43,  4.07s/it]
 45%|████▌     | 839/1850 [55:57<1:08:56,  4.09s/it]
                                                    
{'loss': 0.0017, 'grad_norm': 0.1748046875, 'learning_rate': 5.464864864864865e-05, 'epoch': 22.68}

 45%|████▌     | 839/1850 [55:57<1:08:56,  4.09s/it]
 45%|████▌     | 840/1850 [56:01<1:08:04,  4.04s/it]
                                                    
{'loss': 0.0013, 'grad_norm': 0.126953125, 'learning_rate': 5.4594594594594595e-05, 'epoch': 22.7}

 45%|████▌     | 840/1850 [56:01<1:08:04,  4.04s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 45%|████▌     | 841/1850 [56:06<1:12:03,  4.29s/it]
                                                    
{'loss': 0.0036, 'grad_norm': 0.64453125, 'learning_rate': 5.454054054054054e-05, 'epoch': 22.73}

 45%|████▌     | 841/1850 [56:06<1:12:03,  4.29s/it]
 46%|████▌     | 842/1850 [56:10<1:10:16,  4.18s/it]
                                                    
{'loss': 0.0022, 'grad_norm': 0.6484375, 'learning_rate': 5.4486486486486484e-05, 'epoch': 22.76}

 46%|████▌     | 842/1850 [56:10<1:10:16,  4.18s/it]
 46%|████▌     | 843/1850 [56:14<1:09:46,  4.16s/it]
                                                    
{'loss': 0.0036, 'grad_norm': 0.66015625, 'learning_rate': 5.4432432432432436e-05, 'epoch': 22.78}

 46%|████▌     | 843/1850 [56:14<1:09:46,  4.16s/it]
 46%|████▌     | 844/1850 [56:18<1:05:38,  3.91s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.091796875, 'learning_rate': 5.437837837837838e-05, 'epoch': 22.81}

 46%|████▌     | 844/1850 [56:18<1:05:38,  3.91s/it]
 46%|████▌     | 845/1850 [56:22<1:07:14,  4.01s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.1083984375, 'learning_rate': 5.4324324324324325e-05, 'epoch': 22.84}

 46%|████▌     | 845/1850 [56:22<1:07:14,  4.01s/it]
 46%|████▌     | 846/1850 [56:26<1:08:08,  4.07s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.087890625, 'learning_rate': 5.4270270270270276e-05, 'epoch': 22.86}

 46%|████▌     | 846/1850 [56:26<1:08:08,  4.07s/it]
 46%|████▌     | 847/1850 [56:30<1:08:29,  4.10s/it]
                                                    
{'loss': 0.0036, 'grad_norm': 2.125, 'learning_rate': 5.421621621621622e-05, 'epoch': 22.89}

 46%|████▌     | 847/1850 [56:30<1:08:29,  4.10s/it]
 46%|████▌     | 848/1850 [56:34<1:04:56,  3.89s/it]
                                                    
{'loss': 0.0012, 'grad_norm': 0.07666015625, 'learning_rate': 5.4162162162162165e-05, 'epoch': 22.92}

 46%|████▌     | 848/1850 [56:34<1:04:56,  3.89s/it]
 46%|████▌     | 849/1850 [56:38<1:07:07,  4.02s/it]
                                                    
{'loss': 0.0015, 'grad_norm': 0.06103515625, 'learning_rate': 5.4108108108108116e-05, 'epoch': 22.95}

 46%|████▌     | 849/1850 [56:38<1:07:07,  4.02s/it]
 46%|████▌     | 850/1850 [56:42<1:05:53,  3.95s/it]
                                                    
{'loss': 0.002, 'grad_norm': 0.3203125, 'learning_rate': 5.405405405405406e-05, 'epoch': 22.97}

 46%|████▌     | 850/1850 [56:42<1:05:53,  3.95s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 46%|████▌     | 851/1850 [56:46<1:06:22,  3.99s/it]
                                                    
{'loss': 0.0036, 'grad_norm': 0.91796875, 'learning_rate': 5.4000000000000005e-05, 'epoch': 23.0}

 46%|████▌     | 851/1850 [56:46<1:06:22,  3.99s/it]
 46%|████▌     | 852/1850 [56:51<1:12:03,  4.33s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.060302734375, 'learning_rate': 5.3945945945945956e-05, 'epoch': 23.03}

 46%|████▌     | 852/1850 [56:51<1:12:03,  4.33s/it]
 46%|████▌     | 853/1850 [56:55<1:11:09,  4.28s/it]
                                                    
{'loss': 0.0014, 'grad_norm': 0.06787109375, 'learning_rate': 5.38918918918919e-05, 'epoch': 23.05}

 46%|████▌     | 853/1850 [56:55<1:11:09,  4.28s/it]
 46%|████▌     | 854/1850 [56:59<1:08:18,  4.12s/it]
                                                    
{'loss': 0.0012, 'grad_norm': 0.060791015625, 'learning_rate': 5.3837837837837845e-05, 'epoch': 23.08}

 46%|████▌     | 854/1850 [56:59<1:08:18,  4.12s/it]
 46%|████▌     | 855/1850 [57:03<1:08:46,  4.15s/it]
                                                    
{'loss': 0.0013, 'grad_norm': 0.047119140625, 'learning_rate': 5.378378378378378e-05, 'epoch': 23.11}

 46%|████▌     | 855/1850 [57:03<1:08:46,  4.15s/it]
 46%|████▋     | 856/1850 [57:07<1:08:19,  4.12s/it]
                                                    
{'loss': 0.0012, 'grad_norm': 0.052978515625, 'learning_rate': 5.372972972972973e-05, 'epoch': 23.14}

 46%|████▋     | 856/1850 [57:07<1:08:19,  4.12s/it]
 46%|████▋     | 857/1850 [57:11<1:07:54,  4.10s/it]
                                                    
{'loss': 0.0012, 'grad_norm': 0.06298828125, 'learning_rate': 5.367567567567567e-05, 'epoch': 23.16}

 46%|████▋     | 857/1850 [57:11<1:07:54,  4.10s/it]
 46%|████▋     | 858/1850 [57:15<1:06:31,  4.02s/it]
                                                    
{'loss': 0.0095, 'grad_norm': 1.3359375, 'learning_rate': 5.362162162162162e-05, 'epoch': 23.19}

 46%|████▋     | 858/1850 [57:15<1:06:31,  4.02s/it]
 46%|████▋     | 859/1850 [57:20<1:10:13,  4.25s/it]
                                                    
{'loss': 0.0019, 'grad_norm': 0.10888671875, 'learning_rate': 5.356756756756757e-05, 'epoch': 23.22}

 46%|████▋     | 859/1850 [57:20<1:10:13,  4.25s/it]
 46%|████▋     | 860/1850 [57:23<1:06:53,  4.05s/it]
                                                    
{'loss': 0.0015, 'grad_norm': 0.0712890625, 'learning_rate': 5.351351351351351e-05, 'epoch': 23.24}

 46%|████▋     | 860/1850 [57:23<1:06:53,  4.05s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 47%|████▋     | 861/1850 [57:27<1:05:26,  3.97s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.0498046875, 'learning_rate': 5.345945945945946e-05, 'epoch': 23.27}

 47%|████▋     | 861/1850 [57:27<1:05:26,  3.97s/it]
 47%|████▋     | 862/1850 [57:31<1:05:42,  3.99s/it]
                                                    
{'loss': 0.0013, 'grad_norm': 0.049560546875, 'learning_rate': 5.340540540540541e-05, 'epoch': 23.3}

 47%|████▋     | 862/1850 [57:31<1:05:42,  3.99s/it]
 47%|████▋     | 863/1850 [57:34<1:01:13,  3.72s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.06884765625, 'learning_rate': 5.335135135135135e-05, 'epoch': 23.32}

 47%|████▋     | 863/1850 [57:34<1:01:13,  3.72s/it]
 47%|████▋     | 864/1850 [57:39<1:03:41,  3.88s/it]
                                                    
{'loss': 0.0015, 'grad_norm': 0.087890625, 'learning_rate': 5.3297297297297296e-05, 'epoch': 23.35}

 47%|████▋     | 864/1850 [57:39<1:03:41,  3.88s/it]
 47%|████▋     | 865/1850 [57:43<1:06:10,  4.03s/it]
                                                    
{'loss': 0.0017, 'grad_norm': 0.11181640625, 'learning_rate': 5.324324324324325e-05, 'epoch': 23.38}

 47%|████▋     | 865/1850 [57:43<1:06:10,  4.03s/it]
 47%|████▋     | 866/1850 [57:47<1:04:25,  3.93s/it]
                                                    
{'loss': 0.0012, 'grad_norm': 0.07568359375, 'learning_rate': 5.318918918918919e-05, 'epoch': 23.41}

 47%|████▋     | 866/1850 [57:47<1:04:25,  3.93s/it]
 47%|████▋     | 867/1850 [57:50<1:03:35,  3.88s/it]
                                                    
{'loss': 0.0038, 'grad_norm': 1.390625, 'learning_rate': 5.3135135135135136e-05, 'epoch': 23.43}

 47%|████▋     | 867/1850 [57:50<1:03:35,  3.88s/it]
 47%|████▋     | 868/1850 [57:55<1:04:44,  3.96s/it]
                                                    
{'loss': 0.0012, 'grad_norm': 0.08154296875, 'learning_rate': 5.308108108108109e-05, 'epoch': 23.46}

 47%|████▋     | 868/1850 [57:55<1:04:44,  3.96s/it]
 47%|████▋     | 869/1850 [57:59<1:04:57,  3.97s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.1171875, 'learning_rate': 5.302702702702703e-05, 'epoch': 23.49}

 47%|████▋     | 869/1850 [57:59<1:04:57,  3.97s/it]
 47%|████▋     | 870/1850 [58:02<1:04:05,  3.92s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.0625, 'learning_rate': 5.2972972972972976e-05, 'epoch': 23.51}

 47%|████▋     | 870/1850 [58:02<1:04:05,  3.92s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 47%|████▋     | 871/1850 [58:07<1:05:46,  4.03s/it]
                                                    
{'loss': 0.0014, 'grad_norm': 0.11083984375, 'learning_rate': 5.291891891891893e-05, 'epoch': 23.54}

 47%|████▋     | 871/1850 [58:07<1:05:46,  4.03s/it]
 47%|████▋     | 872/1850 [58:11<1:05:13,  4.00s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.0458984375, 'learning_rate': 5.286486486486487e-05, 'epoch': 23.57}

 47%|████▋     | 872/1850 [58:11<1:05:13,  4.00s/it]
 47%|████▋     | 873/1850 [58:15<1:05:22,  4.01s/it]
                                                    
{'loss': 0.0033, 'grad_norm': 0.6796875, 'learning_rate': 5.2810810810810817e-05, 'epoch': 23.59}

 47%|████▋     | 873/1850 [58:15<1:05:22,  4.01s/it]
 47%|████▋     | 874/1850 [58:18<1:03:36,  3.91s/it]
                                                    
{'loss': 0.0023, 'grad_norm': 0.578125, 'learning_rate': 5.275675675675677e-05, 'epoch': 23.62}

 47%|████▋     | 874/1850 [58:18<1:03:36,  3.91s/it]
 47%|████▋     | 875/1850 [58:22<1:02:57,  3.87s/it]
                                                    
{'loss': 0.0014, 'grad_norm': 0.1103515625, 'learning_rate': 5.27027027027027e-05, 'epoch': 23.65}

 47%|████▋     | 875/1850 [58:22<1:02:57,  3.87s/it]
 47%|████▋     | 876/1850 [58:25<1:00:17,  3.71s/it]
                                                    
{'loss': 0.0009, 'grad_norm': 0.046875, 'learning_rate': 5.264864864864864e-05, 'epoch': 23.68}

 47%|████▋     | 876/1850 [58:25<1:00:17,  3.71s/it]
 47%|████▋     | 877/1850 [58:29<59:46,  3.69s/it]  
                                                  
{'loss': 0.0012, 'grad_norm': 0.06591796875, 'learning_rate': 5.2594594594594594e-05, 'epoch': 23.7}

 47%|████▋     | 877/1850 [58:29<59:46,  3.69s/it]
 47%|████▋     | 878/1850 [58:33<1:02:54,  3.88s/it]
                                                    
{'loss': 0.003, 'grad_norm': 0.67578125, 'learning_rate': 5.254054054054054e-05, 'epoch': 23.73}

 47%|████▋     | 878/1850 [58:33<1:02:54,  3.88s/it]
 48%|████▊     | 879/1850 [58:38<1:04:07,  3.96s/it]
                                                    
{'loss': 0.0021, 'grad_norm': 0.44140625, 'learning_rate': 5.248648648648648e-05, 'epoch': 23.76}

 48%|████▊     | 879/1850 [58:38<1:04:07,  3.96s/it]
 48%|████▊     | 880/1850 [58:42<1:05:13,  4.03s/it]
                                                    
{'loss': 0.0017, 'grad_norm': 0.1845703125, 'learning_rate': 5.2432432432432434e-05, 'epoch': 23.78}

 48%|████▊     | 880/1850 [58:42<1:05:13,  4.03s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 48%|████▊     | 881/1850 [58:46<1:04:12,  3.98s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.056396484375, 'learning_rate': 5.237837837837838e-05, 'epoch': 23.81}

 48%|████▊     | 881/1850 [58:46<1:04:12,  3.98s/it]
 48%|████▊     | 882/1850 [58:49<1:02:58,  3.90s/it]
                                                    
{'loss': 0.0054, 'grad_norm': 0.470703125, 'learning_rate': 5.232432432432432e-05, 'epoch': 23.84}

 48%|████▊     | 882/1850 [58:49<1:02:58,  3.90s/it]
 48%|████▊     | 883/1850 [58:54<1:05:36,  4.07s/it]
                                                    
{'loss': 0.0014, 'grad_norm': 0.1328125, 'learning_rate': 5.2270270270270275e-05, 'epoch': 23.86}

 48%|████▊     | 883/1850 [58:54<1:05:36,  4.07s/it]
 48%|████▊     | 884/1850 [58:57<1:03:40,  3.95s/it]
                                                    
{'loss': 0.0014, 'grad_norm': 0.1884765625, 'learning_rate': 5.221621621621622e-05, 'epoch': 23.89}

 48%|████▊     | 884/1850 [58:57<1:03:40,  3.95s/it]
 48%|████▊     | 885/1850 [59:02<1:04:28,  4.01s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.06689453125, 'learning_rate': 5.2162162162162163e-05, 'epoch': 23.92}

 48%|████▊     | 885/1850 [59:02<1:04:28,  4.01s/it]
 48%|████▊     | 886/1850 [59:05<1:01:33,  3.83s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.458984375, 'learning_rate': 5.210810810810811e-05, 'epoch': 23.95}

 48%|████▊     | 886/1850 [59:05<1:01:33,  3.83s/it]
 48%|████▊     | 887/1850 [59:09<1:03:10,  3.94s/it]
                                                    
{'loss': 0.0015, 'grad_norm': 0.07470703125, 'learning_rate': 5.205405405405406e-05, 'epoch': 23.97}

 48%|████▊     | 887/1850 [59:09<1:03:10,  3.94s/it]
 48%|████▊     | 888/1850 [59:13<1:02:53,  3.92s/it]
                                                    
{'loss': 0.0015, 'grad_norm': 0.10888671875, 'learning_rate': 5.2000000000000004e-05, 'epoch': 24.0}

 48%|████▊     | 888/1850 [59:13<1:02:53,  3.92s/it]
 48%|████▊     | 889/1850 [59:17<1:01:54,  3.87s/it]
                                                    
{'loss': 0.0026, 'grad_norm': 0.23046875, 'learning_rate': 5.194594594594595e-05, 'epoch': 24.03}

 48%|████▊     | 889/1850 [59:17<1:01:54,  3.87s/it]
 48%|████▊     | 890/1850 [59:21<1:01:59,  3.87s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.060546875, 'learning_rate': 5.18918918918919e-05, 'epoch': 24.05}

 48%|████▊     | 890/1850 [59:21<1:01:59,  3.87s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 48%|████▊     | 891/1850 [59:25<1:02:23,  3.90s/it]
                                                    
{'loss': 0.0008, 'grad_norm': 0.047119140625, 'learning_rate': 5.1837837837837844e-05, 'epoch': 24.08}

 48%|████▊     | 891/1850 [59:25<1:02:23,  3.90s/it]
 48%|████▊     | 892/1850 [59:28<1:00:13,  3.77s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.10205078125, 'learning_rate': 5.178378378378379e-05, 'epoch': 24.11}

 48%|████▊     | 892/1850 [59:28<1:00:13,  3.77s/it]
 48%|████▊     | 893/1850 [59:32<1:00:45,  3.81s/it]
                                                    
{'loss': 0.0016, 'grad_norm': 0.376953125, 'learning_rate': 5.172972972972974e-05, 'epoch': 24.14}

 48%|████▊     | 893/1850 [59:32<1:00:45,  3.81s/it]
 48%|████▊     | 894/1850 [59:36<1:01:06,  3.84s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.042724609375, 'learning_rate': 5.1675675675675684e-05, 'epoch': 24.16}

 48%|████▊     | 894/1850 [59:36<1:01:06,  3.84s/it]
 48%|████▊     | 895/1850 [59:40<1:02:05,  3.90s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.056884765625, 'learning_rate': 5.162162162162162e-05, 'epoch': 24.19}

 48%|████▊     | 895/1850 [59:40<1:02:05,  3.90s/it]
 48%|████▊     | 896/1850 [59:44<1:00:09,  3.78s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.050048828125, 'learning_rate': 5.1567567567567566e-05, 'epoch': 24.22}

 48%|████▊     | 896/1850 [59:44<1:00:09,  3.78s/it]
 48%|████▊     | 897/1850 [59:47<1:00:11,  3.79s/it]
                                                    
{'loss': 0.0009, 'grad_norm': 0.0400390625, 'learning_rate': 5.151351351351351e-05, 'epoch': 24.24}

 48%|████▊     | 897/1850 [59:47<1:00:11,  3.79s/it]
 49%|████▊     | 898/1850 [59:52<1:04:21,  4.06s/it]
                                                    
{'loss': 0.0014, 'grad_norm': 0.072265625, 'learning_rate': 5.1459459459459455e-05, 'epoch': 24.27}

 49%|████▊     | 898/1850 [59:52<1:04:21,  4.06s/it]
 49%|████▊     | 899/1850 [59:56<1:02:56,  3.97s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.052978515625, 'learning_rate': 5.1405405405405406e-05, 'epoch': 24.3}

 49%|████▊     | 899/1850 [59:56<1:02:56,  3.97s/it]
 49%|████▊     | 900/1850 [1:00:00<1:02:41,  3.96s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.051025390625, 'learning_rate': 5.135135135135135e-05, 'epoch': 24.32}

 49%|████▊     | 900/1850 [1:00:00<1:02:41,  3.96s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 49%|████▊     | 901/1850 [1:00:04<1:04:22,  4.07s/it]
                                                      
{'loss': 0.0013, 'grad_norm': 0.05615234375, 'learning_rate': 5.1297297297297295e-05, 'epoch': 24.35}

 49%|████▊     | 901/1850 [1:00:04<1:04:22,  4.07s/it]
 49%|████▉     | 902/1850 [1:00:09<1:07:26,  4.27s/it]
                                                      
{'loss': 0.0015, 'grad_norm': 0.0556640625, 'learning_rate': 5.1243243243243246e-05, 'epoch': 24.38}

 49%|████▉     | 902/1850 [1:00:09<1:07:26,  4.27s/it]
 49%|████▉     | 903/1850 [1:00:13<1:05:25,  4.15s/it]
                                                      
{'loss': 0.0016, 'grad_norm': 0.130859375, 'learning_rate': 5.118918918918919e-05, 'epoch': 24.41}

 49%|████▉     | 903/1850 [1:00:13<1:05:25,  4.15s/it]
 49%|████▉     | 904/1850 [1:00:17<1:06:24,  4.21s/it]
                                                      
{'loss': 0.0012, 'grad_norm': 0.04931640625, 'learning_rate': 5.1135135135135135e-05, 'epoch': 24.43}

 49%|████▉     | 904/1850 [1:00:17<1:06:24,  4.21s/it]
 49%|████▉     | 905/1850 [1:00:21<1:05:47,  4.18s/it]
                                                      
{'loss': 0.0012, 'grad_norm': 0.040771484375, 'learning_rate': 5.1081081081081086e-05, 'epoch': 24.46}

 49%|████▉     | 905/1850 [1:00:21<1:05:47,  4.18s/it]
 49%|████▉     | 906/1850 [1:00:26<1:09:48,  4.44s/it]
                                                      
{'loss': 0.0016, 'grad_norm': 0.052734375, 'learning_rate': 5.102702702702703e-05, 'epoch': 24.49}

 49%|████▉     | 906/1850 [1:00:26<1:09:48,  4.44s/it]
 49%|████▉     | 907/1850 [1:00:30<1:08:25,  4.35s/it]
                                                      
{'loss': 0.0013, 'grad_norm': 0.0810546875, 'learning_rate': 5.0972972972972975e-05, 'epoch': 24.51}

 49%|████▉     | 907/1850 [1:00:30<1:08:25,  4.35s/it]
 49%|████▉     | 908/1850 [1:00:34<1:04:46,  4.13s/it]
                                                      
{'loss': 0.0018, 'grad_norm': 0.54296875, 'learning_rate': 5.0918918918918926e-05, 'epoch': 24.54}

 49%|████▉     | 908/1850 [1:00:34<1:04:46,  4.13s/it]
 49%|████▉     | 909/1850 [1:00:37<1:01:54,  3.95s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.04052734375, 'learning_rate': 5.086486486486487e-05, 'epoch': 24.57}

 49%|████▉     | 909/1850 [1:00:37<1:01:54,  3.95s/it]
 49%|████▉     | 910/1850 [1:00:41<1:01:20,  3.92s/it]
                                                      
{'loss': 0.0012, 'grad_norm': 0.052001953125, 'learning_rate': 5.0810810810810815e-05, 'epoch': 24.59}

 49%|████▉     | 910/1850 [1:00:41<1:01:20,  3.92s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 49%|████▉     | 911/1850 [1:00:45<1:00:20,  3.86s/it]
                                                      
{'loss': 0.0009, 'grad_norm': 0.064453125, 'learning_rate': 5.075675675675676e-05, 'epoch': 24.62}

 49%|████▉     | 911/1850 [1:00:45<1:00:20,  3.86s/it]
 49%|████▉     | 912/1850 [1:00:49<59:37,  3.81s/it]  
                                                    
{'loss': 0.001, 'grad_norm': 0.03955078125, 'learning_rate': 5.070270270270271e-05, 'epoch': 24.65}

 49%|████▉     | 912/1850 [1:00:49<59:37,  3.81s/it]
 49%|████▉     | 913/1850 [1:00:52<57:23,  3.68s/it]
                                                    
{'loss': 0.0014, 'grad_norm': 0.050048828125, 'learning_rate': 5.0648648648648655e-05, 'epoch': 24.68}

 49%|████▉     | 913/1850 [1:00:52<57:23,  3.68s/it]
 49%|████▉     | 914/1850 [1:00:56<1:00:19,  3.87s/it]
                                                      
{'loss': 0.0012, 'grad_norm': 0.039306640625, 'learning_rate': 5.05945945945946e-05, 'epoch': 24.7}

 49%|████▉     | 914/1850 [1:00:56<1:00:19,  3.87s/it]
 49%|████▉     | 915/1850 [1:01:00<1:01:01,  3.92s/it]
                                                      
{'loss': 0.0009, 'grad_norm': 0.037353515625, 'learning_rate': 5.054054054054054e-05, 'epoch': 24.73}

 49%|████▉     | 915/1850 [1:01:00<1:01:01,  3.92s/it]
 50%|████▉     | 916/1850 [1:01:04<1:01:04,  3.92s/it]
                                                      
{'loss': 0.0013, 'grad_norm': 0.0888671875, 'learning_rate': 5.048648648648648e-05, 'epoch': 24.76}

 50%|████▉     | 916/1850 [1:01:04<1:01:04,  3.92s/it]
 50%|████▉     | 917/1850 [1:01:08<1:00:55,  3.92s/it]
                                                      
{'loss': 0.0009, 'grad_norm': 0.046142578125, 'learning_rate': 5.043243243243243e-05, 'epoch': 24.78}

 50%|████▉     | 917/1850 [1:01:08<1:00:55,  3.92s/it]
 50%|████▉     | 918/1850 [1:01:12<58:47,  3.78s/it]  
                                                    
{'loss': 0.0012, 'grad_norm': 0.107421875, 'learning_rate': 5.037837837837838e-05, 'epoch': 24.81}

 50%|████▉     | 918/1850 [1:01:12<58:47,  3.78s/it]
 50%|████▉     | 919/1850 [1:01:15<58:34,  3.77s/it]
                                                    
{'loss': 0.0013, 'grad_norm': 0.08740234375, 'learning_rate': 5.032432432432432e-05, 'epoch': 24.84}

 50%|████▉     | 919/1850 [1:01:15<58:34,  3.77s/it]
 50%|████▉     | 920/1850 [1:01:19<57:42,  3.72s/it]
                                                    
{'loss': 0.0028, 'grad_norm': 1.015625, 'learning_rate': 5.0270270270270267e-05, 'epoch': 24.86}

 50%|████▉     | 920/1850 [1:01:19<57:42,  3.72s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 50%|████▉     | 921/1850 [1:01:23<57:37,  3.72s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.040283203125, 'learning_rate': 5.021621621621622e-05, 'epoch': 24.89}

 50%|████▉     | 921/1850 [1:01:23<57:37,  3.72s/it]
 50%|████▉     | 922/1850 [1:01:28<1:02:40,  4.05s/it]
                                                      
{'loss': 0.0014, 'grad_norm': 0.0498046875, 'learning_rate': 5.016216216216216e-05, 'epoch': 24.92}

 50%|████▉     | 922/1850 [1:01:28<1:02:40,  4.05s/it]
 50%|████▉     | 923/1850 [1:01:31<58:22,  3.78s/it]  
                                                    
{'loss': 0.0008, 'grad_norm': 0.044189453125, 'learning_rate': 5.010810810810811e-05, 'epoch': 24.95}

 50%|████▉     | 923/1850 [1:01:31<58:22,  3.78s/it]
 50%|████▉     | 924/1850 [1:01:36<1:05:14,  4.23s/it]
                                                      
{'loss': 0.0019, 'grad_norm': 0.048828125, 'learning_rate': 5.005405405405406e-05, 'epoch': 24.97}

 50%|████▉     | 924/1850 [1:01:36<1:05:14,  4.23s/it]
 50%|█████     | 925/1850 [1:01:40<1:03:52,  4.14s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.0400390625, 'learning_rate': 5e-05, 'epoch': 25.0}

 50%|█████     | 925/1850 [1:01:40<1:03:52,  4.14s/it]
 50%|█████     | 926/1850 [1:01:43<59:36,  3.87s/it]  
                                                    
{'loss': 0.0007, 'grad_norm': 0.0291748046875, 'learning_rate': 4.994594594594595e-05, 'epoch': 25.03}

 50%|█████     | 926/1850 [1:01:43<59:36,  3.87s/it]
 50%|█████     | 927/1850 [1:01:47<1:00:05,  3.91s/it]
                                                      
{'loss': 0.0013, 'grad_norm': 0.04931640625, 'learning_rate': 4.98918918918919e-05, 'epoch': 25.05}

 50%|█████     | 927/1850 [1:01:47<1:00:05,  3.91s/it]
 50%|█████     | 928/1850 [1:01:51<59:17,  3.86s/it]  
                                                    
{'loss': 0.001, 'grad_norm': 0.033447265625, 'learning_rate': 4.983783783783784e-05, 'epoch': 25.08}

 50%|█████     | 928/1850 [1:01:51<59:17,  3.86s/it]
 50%|█████     | 929/1850 [1:01:55<59:48,  3.90s/it]
                                                    
{'loss': 0.0012, 'grad_norm': 0.045166015625, 'learning_rate': 4.978378378378379e-05, 'epoch': 25.11}

 50%|█████     | 929/1850 [1:01:55<59:48,  3.90s/it]
 50%|█████     | 930/1850 [1:01:59<59:59,  3.91s/it]
                                                    
{'loss': 0.0009, 'grad_norm': 0.03955078125, 'learning_rate': 4.972972972972974e-05, 'epoch': 25.14}

 50%|█████     | 930/1850 [1:01:59<59:59,  3.91s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 50%|█████     | 931/1850 [1:02:03<1:02:43,  4.09s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.043701171875, 'learning_rate': 4.9675675675675676e-05, 'epoch': 25.16}

 50%|█████     | 931/1850 [1:02:03<1:02:43,  4.09s/it]
 50%|█████     | 932/1850 [1:02:07<1:02:29,  4.08s/it]
                                                      
{'loss': 0.0008, 'grad_norm': 0.0341796875, 'learning_rate': 4.962162162162162e-05, 'epoch': 25.19}

 50%|█████     | 932/1850 [1:02:07<1:02:29,  4.08s/it]
 50%|█████     | 933/1850 [1:02:12<1:04:15,  4.20s/it]
                                                      
{'loss': 0.0013, 'grad_norm': 0.06689453125, 'learning_rate': 4.956756756756757e-05, 'epoch': 25.22}

 50%|█████     | 933/1850 [1:02:12<1:04:15,  4.20s/it]
 50%|█████     | 934/1850 [1:02:16<1:04:01,  4.19s/it]
                                                      
{'loss': 0.003, 'grad_norm': 0.21484375, 'learning_rate': 4.9513513513513516e-05, 'epoch': 25.24}

 50%|█████     | 934/1850 [1:02:16<1:04:01,  4.19s/it]
 51%|█████     | 935/1850 [1:02:20<1:02:32,  4.10s/it]
                                                      
{'loss': 0.001, 'grad_norm': 0.0380859375, 'learning_rate': 4.945945945945946e-05, 'epoch': 25.27}

 51%|█████     | 935/1850 [1:02:20<1:02:32,  4.10s/it]
 51%|█████     | 936/1850 [1:02:24<1:02:27,  4.10s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.056884765625, 'learning_rate': 4.940540540540541e-05, 'epoch': 25.3}

 51%|█████     | 936/1850 [1:02:24<1:02:27,  4.10s/it]
 51%|█████     | 937/1850 [1:02:28<1:01:31,  4.04s/it]
                                                      
{'loss': 0.001, 'grad_norm': 0.041015625, 'learning_rate': 4.9351351351351356e-05, 'epoch': 25.32}

 51%|█████     | 937/1850 [1:02:28<1:01:31,  4.04s/it]
 51%|█████     | 938/1850 [1:02:31<58:45,  3.87s/it]  
                                                    
{'loss': 0.0009, 'grad_norm': 0.03662109375, 'learning_rate': 4.92972972972973e-05, 'epoch': 25.35}

 51%|█████     | 938/1850 [1:02:31<58:45,  3.87s/it]
 51%|█████     | 939/1850 [1:02:35<59:30,  3.92s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.03955078125, 'learning_rate': 4.9243243243243245e-05, 'epoch': 25.38}

 51%|█████     | 939/1850 [1:02:35<59:30,  3.92s/it]
 51%|█████     | 940/1850 [1:02:39<59:10,  3.90s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.049560546875, 'learning_rate': 4.9189189189189196e-05, 'epoch': 25.41}

 51%|█████     | 940/1850 [1:02:39<59:10,  3.90s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 51%|█████     | 941/1850 [1:02:44<1:00:50,  4.02s/it]
                                                      
{'loss': 0.0009, 'grad_norm': 0.04052734375, 'learning_rate': 4.9135135135135134e-05, 'epoch': 25.43}

 51%|█████     | 941/1850 [1:02:44<1:00:50,  4.02s/it]
 51%|█████     | 942/1850 [1:02:48<1:03:06,  4.17s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.043212890625, 'learning_rate': 4.908108108108108e-05, 'epoch': 25.46}

 51%|█████     | 942/1850 [1:02:48<1:03:06,  4.17s/it]
 51%|█████     | 943/1850 [1:02:52<1:02:47,  4.15s/it]
                                                      
{'loss': 0.0012, 'grad_norm': 0.047119140625, 'learning_rate': 4.902702702702703e-05, 'epoch': 25.49}

 51%|█████     | 943/1850 [1:02:52<1:02:47,  4.15s/it]
 51%|█████     | 944/1850 [1:02:57<1:03:12,  4.19s/it]
                                                      
{'loss': 0.0012, 'grad_norm': 0.0380859375, 'learning_rate': 4.8972972972972974e-05, 'epoch': 25.51}

 51%|█████     | 944/1850 [1:02:57<1:03:12,  4.19s/it]
 51%|█████     | 945/1850 [1:03:01<1:04:22,  4.27s/it]
                                                      
{'loss': 0.0014, 'grad_norm': 0.05126953125, 'learning_rate': 4.891891891891892e-05, 'epoch': 25.54}

 51%|█████     | 945/1850 [1:03:01<1:04:22,  4.27s/it]
 51%|█████     | 946/1850 [1:03:05<1:01:48,  4.10s/it]
                                                      
{'loss': 0.0012, 'grad_norm': 0.0439453125, 'learning_rate': 4.886486486486487e-05, 'epoch': 25.57}

 51%|█████     | 946/1850 [1:03:05<1:01:48,  4.10s/it]
 51%|█████     | 947/1850 [1:03:09<1:02:00,  4.12s/it]
                                                      
{'loss': 0.0012, 'grad_norm': 0.044189453125, 'learning_rate': 4.8810810810810814e-05, 'epoch': 25.59}

 51%|█████     | 947/1850 [1:03:09<1:02:00,  4.12s/it]
 51%|█████     | 948/1850 [1:03:13<1:00:59,  4.06s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.035888671875, 'learning_rate': 4.875675675675676e-05, 'epoch': 25.62}

 51%|█████     | 948/1850 [1:03:13<1:00:59,  4.06s/it]
 51%|█████▏    | 949/1850 [1:03:17<1:00:42,  4.04s/it]
                                                      
{'loss': 0.0012, 'grad_norm': 0.08203125, 'learning_rate': 4.870270270270271e-05, 'epoch': 25.65}

 51%|█████▏    | 949/1850 [1:03:17<1:00:42,  4.04s/it]
 51%|█████▏    | 950/1850 [1:03:21<1:00:03,  4.00s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.03955078125, 'learning_rate': 4.8648648648648654e-05, 'epoch': 25.68}

 51%|█████▏    | 950/1850 [1:03:21<1:00:03,  4.00s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 51%|█████▏    | 951/1850 [1:03:25<1:01:42,  4.12s/it]
                                                      
{'loss': 0.0017, 'grad_norm': 0.46875, 'learning_rate': 4.859459459459459e-05, 'epoch': 25.7}

 51%|█████▏    | 951/1850 [1:03:25<1:01:42,  4.12s/it]
 51%|█████▏    | 952/1850 [1:03:29<1:00:44,  4.06s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.03759765625, 'learning_rate': 4.854054054054054e-05, 'epoch': 25.73}

 51%|█████▏    | 952/1850 [1:03:29<1:00:44,  4.06s/it]
 52%|█████▏    | 953/1850 [1:03:32<56:21,  3.77s/it]  
                                                    
{'loss': 0.0008, 'grad_norm': 0.03271484375, 'learning_rate': 4.848648648648649e-05, 'epoch': 25.76}

 52%|█████▏    | 953/1850 [1:03:32<56:21,  3.77s/it]
 52%|█████▏    | 954/1850 [1:03:36<58:07,  3.89s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.037353515625, 'learning_rate': 4.843243243243243e-05, 'epoch': 25.78}

 52%|█████▏    | 954/1850 [1:03:36<58:07,  3.89s/it]
 52%|█████▏    | 955/1850 [1:03:40<57:21,  3.85s/it]
                                                    
{'loss': 0.0008, 'grad_norm': 0.0341796875, 'learning_rate': 4.837837837837838e-05, 'epoch': 25.81}

 52%|█████▏    | 955/1850 [1:03:40<57:21,  3.85s/it]
 52%|█████▏    | 956/1850 [1:03:44<58:02,  3.90s/it]
                                                    
{'loss': 0.0008, 'grad_norm': 0.040771484375, 'learning_rate': 4.832432432432433e-05, 'epoch': 25.84}

 52%|█████▏    | 956/1850 [1:03:44<58:02,  3.90s/it]
 52%|█████▏    | 957/1850 [1:03:48<59:30,  4.00s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.046630859375, 'learning_rate': 4.827027027027027e-05, 'epoch': 25.86}

 52%|█████▏    | 957/1850 [1:03:48<59:30,  4.00s/it]
 52%|█████▏    | 958/1850 [1:03:52<58:41,  3.95s/it]
                                                    
{'loss': 0.0012, 'grad_norm': 0.04052734375, 'learning_rate': 4.8216216216216223e-05, 'epoch': 25.89}

 52%|█████▏    | 958/1850 [1:03:52<58:41,  3.95s/it]
 52%|█████▏    | 959/1850 [1:03:57<1:02:21,  4.20s/it]
                                                      
{'loss': 0.0016, 'grad_norm': 0.0439453125, 'learning_rate': 4.816216216216217e-05, 'epoch': 25.92}

 52%|█████▏    | 959/1850 [1:03:57<1:02:21,  4.20s/it]
 52%|█████▏    | 960/1850 [1:04:02<1:05:16,  4.40s/it]
                                                      
{'loss': 0.0012, 'grad_norm': 0.039306640625, 'learning_rate': 4.810810810810811e-05, 'epoch': 25.95}

 52%|█████▏    | 960/1850 [1:04:02<1:05:16,  4.40s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 52%|█████▏    | 961/1850 [1:04:06<1:03:17,  4.27s/it]
                                                      
{'loss': 0.001, 'grad_norm': 0.03955078125, 'learning_rate': 4.805405405405406e-05, 'epoch': 25.97}

 52%|█████▏    | 961/1850 [1:04:06<1:03:17,  4.27s/it]
 52%|█████▏    | 962/1850 [1:04:09<1:00:45,  4.11s/it]
                                                      
{'loss': 0.0009, 'grad_norm': 0.038330078125, 'learning_rate': 4.8e-05, 'epoch': 26.0}

 52%|█████▏    | 962/1850 [1:04:09<1:00:45,  4.11s/it]
 52%|█████▏    | 963/1850 [1:04:14<1:02:30,  4.23s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.03857421875, 'learning_rate': 4.7945945945945946e-05, 'epoch': 26.03}

 52%|█████▏    | 963/1850 [1:04:14<1:02:30,  4.23s/it]
 52%|█████▏    | 964/1850 [1:04:18<1:00:18,  4.08s/it]
                                                      
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 4.78918918918919e-05, 'epoch': 26.05}

 52%|█████▏    | 964/1850 [1:04:18<1:00:18,  4.08s/it]
 52%|█████▏    | 965/1850 [1:04:21<56:55,  3.86s/it]  
                                                    
{'loss': 0.0006, 'grad_norm': 0.0283203125, 'learning_rate': 4.783783783783784e-05, 'epoch': 26.08}

 52%|█████▏    | 965/1850 [1:04:21<56:55,  3.86s/it]
 52%|█████▏    | 966/1850 [1:04:25<57:44,  3.92s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.044921875, 'learning_rate': 4.7783783783783786e-05, 'epoch': 26.11}

 52%|█████▏    | 966/1850 [1:04:25<57:44,  3.92s/it]
 52%|█████▏    | 967/1850 [1:04:29<55:58,  3.80s/it]
                                                    
{'loss': 0.0007, 'grad_norm': 0.02783203125, 'learning_rate': 4.772972972972973e-05, 'epoch': 26.14}

 52%|█████▏    | 967/1850 [1:04:29<55:58,  3.80s/it]
 52%|█████▏    | 968/1850 [1:04:33<56:25,  3.84s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.03466796875, 'learning_rate': 4.767567567567568e-05, 'epoch': 26.16}

 52%|█████▏    | 968/1850 [1:04:33<56:25,  3.84s/it]
 52%|█████▏    | 969/1850 [1:04:37<58:19,  3.97s/it]
                                                    
{'loss': 0.0012, 'grad_norm': 0.034423828125, 'learning_rate': 4.7621621621621626e-05, 'epoch': 26.19}

 52%|█████▏    | 969/1850 [1:04:37<58:19,  3.97s/it]
 52%|█████▏    | 970/1850 [1:04:41<59:00,  4.02s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.031982421875, 'learning_rate': 4.756756756756757e-05, 'epoch': 26.22}

 52%|█████▏    | 970/1850 [1:04:41<59:00,  4.02s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 52%|█████▏    | 971/1850 [1:04:45<59:33,  4.07s/it]
                                                    
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 4.7513513513513515e-05, 'epoch': 26.24}

 52%|█████▏    | 971/1850 [1:04:45<59:33,  4.07s/it]
 53%|█████▎    | 972/1850 [1:04:49<56:56,  3.89s/it]
                                                    
{'loss': 0.0007, 'grad_norm': 0.030517578125, 'learning_rate': 4.745945945945946e-05, 'epoch': 26.27}

 53%|█████▎    | 972/1850 [1:04:49<56:56,  3.89s/it]
 53%|█████▎    | 973/1850 [1:04:53<1:00:04,  4.11s/it]
                                                      
{'loss': 0.0013, 'grad_norm': 0.040283203125, 'learning_rate': 4.7405405405405404e-05, 'epoch': 26.3}

 53%|█████▎    | 973/1850 [1:04:53<1:00:04,  4.11s/it]
 53%|█████▎    | 974/1850 [1:04:57<59:15,  4.06s/it]  
                                                    
{'loss': 0.0008, 'grad_norm': 0.0322265625, 'learning_rate': 4.7351351351351355e-05, 'epoch': 26.32}

 53%|█████▎    | 974/1850 [1:04:57<59:15,  4.06s/it]
 53%|█████▎    | 975/1850 [1:05:02<1:00:24,  4.14s/it]
                                                      
{'loss': 0.0014, 'grad_norm': 0.03857421875, 'learning_rate': 4.72972972972973e-05, 'epoch': 26.35}

 53%|█████▎    | 975/1850 [1:05:02<1:00:24,  4.14s/it]
 53%|█████▎    | 976/1850 [1:05:06<1:01:24,  4.22s/it]
                                                      
{'loss': 0.0013, 'grad_norm': 0.04296875, 'learning_rate': 4.7243243243243244e-05, 'epoch': 26.38}

 53%|█████▎    | 976/1850 [1:05:06<1:01:24,  4.22s/it]
 53%|█████▎    | 977/1850 [1:05:10<1:00:21,  4.15s/it]
                                                      
{'loss': 0.001, 'grad_norm': 0.031494140625, 'learning_rate': 4.7189189189189195e-05, 'epoch': 26.41}

 53%|█████▎    | 977/1850 [1:05:10<1:00:21,  4.15s/it]
 53%|█████▎    | 978/1850 [1:05:14<58:56,  4.06s/it]  
                                                    
{'loss': 0.0007, 'grad_norm': 0.0279541015625, 'learning_rate': 4.713513513513514e-05, 'epoch': 26.43}

 53%|█████▎    | 978/1850 [1:05:14<58:56,  4.06s/it]
 53%|█████▎    | 979/1850 [1:05:18<1:00:41,  4.18s/it]
                                                      
{'loss': 0.0011, 'grad_norm': 0.038818359375, 'learning_rate': 4.7081081081081084e-05, 'epoch': 26.46}

 53%|█████▎    | 979/1850 [1:05:18<1:00:41,  4.18s/it]
 53%|█████▎    | 980/1850 [1:05:22<58:40,  4.05s/it]  
                                                    
{'loss': 0.001, 'grad_norm': 0.033203125, 'learning_rate': 4.7027027027027035e-05, 'epoch': 26.49}

 53%|█████▎    | 980/1850 [1:05:22<58:40,  4.05s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 53%|█████▎    | 981/1850 [1:05:27<1:01:36,  4.25s/it]
                                                      
{'loss': 0.001, 'grad_norm': 0.03466796875, 'learning_rate': 4.697297297297297e-05, 'epoch': 26.51}

 53%|█████▎    | 981/1850 [1:05:27<1:01:36,  4.25s/it]
 53%|█████▎    | 982/1850 [1:05:31<59:43,  4.13s/it]  
                                                    
{'loss': 0.0009, 'grad_norm': 0.0341796875, 'learning_rate': 4.691891891891892e-05, 'epoch': 26.54}

 53%|█████▎    | 982/1850 [1:05:31<59:43,  4.13s/it]
 53%|█████▎    | 983/1850 [1:05:34<58:55,  4.08s/it]
                                                    
{'loss': 0.0012, 'grad_norm': 0.037841796875, 'learning_rate': 4.686486486486487e-05, 'epoch': 26.57}

 53%|█████▎    | 983/1850 [1:05:34<58:55,  4.08s/it]
 53%|█████▎    | 984/1850 [1:05:38<57:03,  3.95s/it]
                                                    
{'loss': 0.0009, 'grad_norm': 0.03466796875, 'learning_rate': 4.681081081081081e-05, 'epoch': 26.59}

 53%|█████▎    | 984/1850 [1:05:38<57:03,  3.95s/it]
 53%|█████▎    | 985/1850 [1:05:42<55:28,  3.85s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.0361328125, 'learning_rate': 4.675675675675676e-05, 'epoch': 26.62}

 53%|█████▎    | 985/1850 [1:05:42<55:28,  3.85s/it]
 53%|█████▎    | 986/1850 [1:05:46<55:47,  3.87s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.033203125, 'learning_rate': 4.670270270270271e-05, 'epoch': 26.65}

 53%|█████▎    | 986/1850 [1:05:46<55:47,  3.87s/it]
 53%|█████▎    | 987/1850 [1:05:49<55:17,  3.84s/it]
                                                    
{'loss': 0.0017, 'grad_norm': 0.1904296875, 'learning_rate': 4.664864864864865e-05, 'epoch': 26.68}

 53%|█████▎    | 987/1850 [1:05:49<55:17,  3.84s/it]
 53%|█████▎    | 988/1850 [1:05:54<57:54,  4.03s/it]
                                                    
{'loss': 0.002, 'grad_norm': 0.1474609375, 'learning_rate': 4.65945945945946e-05, 'epoch': 26.7}

 53%|█████▎    | 988/1850 [1:05:54<57:54,  4.03s/it]
 53%|█████▎    | 989/1850 [1:05:58<56:27,  3.93s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.035400390625, 'learning_rate': 4.654054054054054e-05, 'epoch': 26.73}

 53%|█████▎    | 989/1850 [1:05:58<56:27,  3.93s/it]
 54%|█████▎    | 990/1850 [1:06:02<56:49,  3.97s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.03515625, 'learning_rate': 4.648648648648649e-05, 'epoch': 26.76}

 54%|█████▎    | 990/1850 [1:06:02<56:49,  3.97s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 54%|█████▎    | 991/1850 [1:06:06<59:24,  4.15s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.039794921875, 'learning_rate': 4.643243243243243e-05, 'epoch': 26.78}

 54%|█████▎    | 991/1850 [1:06:06<59:24,  4.15s/it]
 54%|█████▎    | 992/1850 [1:06:10<59:45,  4.18s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.029541015625, 'learning_rate': 4.637837837837838e-05, 'epoch': 26.81}

 54%|█████▎    | 992/1850 [1:06:10<59:45,  4.18s/it]
 54%|█████▎    | 993/1850 [1:06:15<59:12,  4.15s/it]
                                                    
{'loss': 0.0009, 'grad_norm': 0.0311279296875, 'learning_rate': 4.6324324324324327e-05, 'epoch': 26.84}

 54%|█████▎    | 993/1850 [1:06:15<59:12,  4.15s/it]
 54%|█████▎    | 994/1850 [1:06:18<57:34,  4.04s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.037353515625, 'learning_rate': 4.627027027027027e-05, 'epoch': 26.86}

 54%|█████▎    | 994/1850 [1:06:18<57:34,  4.04s/it]
 54%|█████▍    | 995/1850 [1:06:22<56:15,  3.95s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.038330078125, 'learning_rate': 4.6216216216216215e-05, 'epoch': 26.89}

 54%|█████▍    | 995/1850 [1:06:22<56:15,  3.95s/it]
 54%|█████▍    | 996/1850 [1:06:26<57:37,  4.05s/it]
                                                    
{'loss': 0.001, 'grad_norm': 0.039306640625, 'learning_rate': 4.616216216216217e-05, 'epoch': 26.92}

 54%|█████▍    | 996/1850 [1:06:26<57:37,  4.05s/it]
 54%|█████▍    | 997/1850 [1:06:31<58:31,  4.12s/it]
                                                    
{'loss': 0.0012, 'grad_norm': 0.04248046875, 'learning_rate': 4.610810810810811e-05, 'epoch': 26.95}

 54%|█████▍    | 997/1850 [1:06:31<58:31,  4.12s/it]
 54%|█████▍    | 998/1850 [1:06:33<52:30,  3.70s/it]
                                                    
{'loss': 0.0007, 'grad_norm': 0.0341796875, 'learning_rate': 4.6054054054054056e-05, 'epoch': 26.97}

 54%|█████▍    | 998/1850 [1:06:33<52:30,  3.70s/it]
 54%|█████▍    | 999/1850 [1:06:38<54:57,  3.88s/it]
                                                    
{'loss': 0.0011, 'grad_norm': 0.040283203125, 'learning_rate': 4.600000000000001e-05, 'epoch': 27.0}

 54%|█████▍    | 999/1850 [1:06:38<54:57,  3.88s/it]
 54%|█████▍    | 1000/1850 [1:06:42<55:15,  3.90s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03125, 'learning_rate': 4.594594594594595e-05, 'epoch': 27.03}

 54%|█████▍    | 1000/1850 [1:06:42<55:15,  3.90s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 54%|█████▍    | 1001/1850 [1:06:46<56:10,  3.97s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.037109375, 'learning_rate': 4.589189189189189e-05, 'epoch': 27.05}

 54%|█████▍    | 1001/1850 [1:06:46<56:10,  3.97s/it]
 54%|█████▍    | 1002/1850 [1:06:50<57:22,  4.06s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.035888671875, 'learning_rate': 4.583783783783784e-05, 'epoch': 27.08}

 54%|█████▍    | 1002/1850 [1:06:50<57:22,  4.06s/it]
 54%|█████▍    | 1003/1850 [1:06:54<58:28,  4.14s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.038330078125, 'learning_rate': 4.5783783783783785e-05, 'epoch': 27.11}

 54%|█████▍    | 1003/1850 [1:06:54<58:28,  4.14s/it]
 54%|█████▍    | 1004/1850 [1:06:59<58:50,  4.17s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03466796875, 'learning_rate': 4.572972972972973e-05, 'epoch': 27.14}

 54%|█████▍    | 1004/1850 [1:06:59<58:50,  4.17s/it]
 54%|█████▍    | 1005/1850 [1:07:03<57:47,  4.10s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033935546875, 'learning_rate': 4.567567567567568e-05, 'epoch': 27.16}

 54%|█████▍    | 1005/1850 [1:07:03<57:47,  4.10s/it]
 54%|█████▍    | 1006/1850 [1:07:07<57:37,  4.10s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0341796875, 'learning_rate': 4.5621621621621625e-05, 'epoch': 27.19}

 54%|█████▍    | 1006/1850 [1:07:07<57:37,  4.10s/it]
 54%|█████▍    | 1007/1850 [1:07:11<56:42,  4.04s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033447265625, 'learning_rate': 4.556756756756757e-05, 'epoch': 27.22}

 54%|█████▍    | 1007/1850 [1:07:11<56:42,  4.04s/it]
 54%|█████▍    | 1008/1850 [1:07:15<57:43,  4.11s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.0390625, 'learning_rate': 4.551351351351352e-05, 'epoch': 27.24}

 54%|█████▍    | 1008/1850 [1:07:15<57:43,  4.11s/it]
 55%|█████▍    | 1009/1850 [1:07:19<57:35,  4.11s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03369140625, 'learning_rate': 4.5459459459459465e-05, 'epoch': 27.27}

 55%|█████▍    | 1009/1850 [1:07:19<57:35,  4.11s/it]
 55%|█████▍    | 1010/1850 [1:07:23<57:28,  4.11s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0361328125, 'learning_rate': 4.540540540540541e-05, 'epoch': 27.3}

 55%|█████▍    | 1010/1850 [1:07:23<57:28,  4.11s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 55%|█████▍    | 1011/1850 [1:07:27<56:37,  4.05s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03759765625, 'learning_rate': 4.5351351351351354e-05, 'epoch': 27.32}

 55%|█████▍    | 1011/1850 [1:07:27<56:37,  4.05s/it]
 55%|█████▍    | 1012/1850 [1:07:32<1:00:23,  4.32s/it]
                                                       
{'loss': 0.0013, 'grad_norm': 0.041015625, 'learning_rate': 4.52972972972973e-05, 'epoch': 27.35}

 55%|█████▍    | 1012/1850 [1:07:32<1:00:23,  4.32s/it]
 55%|█████▍    | 1013/1850 [1:07:35<56:02,  4.02s/it]  
                                                     
{'loss': 0.0007, 'grad_norm': 0.036376953125, 'learning_rate': 4.524324324324324e-05, 'epoch': 27.38}

 55%|█████▍    | 1013/1850 [1:07:35<56:02,  4.02s/it]
 55%|█████▍    | 1014/1850 [1:07:39<56:44,  4.07s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.04052734375, 'learning_rate': 4.5189189189189194e-05, 'epoch': 27.41}

 55%|█████▍    | 1014/1850 [1:07:39<56:44,  4.07s/it]
 55%|█████▍    | 1015/1850 [1:07:43<55:01,  3.95s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03369140625, 'learning_rate': 4.513513513513514e-05, 'epoch': 27.43}

 55%|█████▍    | 1015/1850 [1:07:43<55:01,  3.95s/it]
 55%|█████▍    | 1016/1850 [1:07:47<53:20,  3.84s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.11962890625, 'learning_rate': 4.508108108108108e-05, 'epoch': 27.46}

 55%|█████▍    | 1016/1850 [1:07:47<53:20,  3.84s/it]
 55%|█████▍    | 1017/1850 [1:07:50<52:57,  3.81s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.032470703125, 'learning_rate': 4.502702702702703e-05, 'epoch': 27.49}

 55%|█████▍    | 1017/1850 [1:07:50<52:57,  3.81s/it]
 55%|█████▌    | 1018/1850 [1:07:54<53:45,  3.88s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.033935546875, 'learning_rate': 4.497297297297298e-05, 'epoch': 27.51}

 55%|█████▌    | 1018/1850 [1:07:54<53:45,  3.88s/it]
 55%|█████▌    | 1019/1850 [1:07:59<56:05,  4.05s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0341796875, 'learning_rate': 4.491891891891892e-05, 'epoch': 27.54}

 55%|█████▌    | 1019/1850 [1:07:59<56:05,  4.05s/it]
 55%|█████▌    | 1020/1850 [1:08:02<52:40,  3.81s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03271484375, 'learning_rate': 4.486486486486487e-05, 'epoch': 27.57}

 55%|█████▌    | 1020/1850 [1:08:02<52:40,  3.81s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 55%|█████▌    | 1021/1850 [1:08:06<54:43,  3.96s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0341796875, 'learning_rate': 4.481081081081081e-05, 'epoch': 27.59}

 55%|█████▌    | 1021/1850 [1:08:06<54:43,  3.96s/it]
 55%|█████▌    | 1022/1850 [1:08:10<52:59,  3.84s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.032470703125, 'learning_rate': 4.4756756756756756e-05, 'epoch': 27.62}

 55%|█████▌    | 1022/1850 [1:08:10<52:59,  3.84s/it]
 55%|█████▌    | 1023/1850 [1:08:13<51:25,  3.73s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.032958984375, 'learning_rate': 4.47027027027027e-05, 'epoch': 27.65}

 55%|█████▌    | 1023/1850 [1:08:13<51:25,  3.73s/it]
 55%|█████▌    | 1024/1850 [1:08:17<51:19,  3.73s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.035888671875, 'learning_rate': 4.464864864864865e-05, 'epoch': 27.68}

 55%|█████▌    | 1024/1850 [1:08:17<51:19,  3.73s/it]
 55%|█████▌    | 1025/1850 [1:08:22<53:58,  3.93s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.037353515625, 'learning_rate': 4.4594594594594596e-05, 'epoch': 27.7}

 55%|█████▌    | 1025/1850 [1:08:22<53:58,  3.93s/it]
 55%|█████▌    | 1026/1850 [1:08:25<53:47,  3.92s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0306396484375, 'learning_rate': 4.454054054054054e-05, 'epoch': 27.73}

 55%|█████▌    | 1026/1850 [1:08:25<53:47,  3.92s/it]
 56%|█████▌    | 1027/1850 [1:08:29<53:32,  3.90s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 4.448648648648649e-05, 'epoch': 27.76}

 56%|█████▌    | 1027/1850 [1:08:29<53:32,  3.90s/it]
 56%|█████▌    | 1028/1850 [1:08:33<52:18,  3.82s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.034423828125, 'learning_rate': 4.4432432432432436e-05, 'epoch': 27.78}

 56%|█████▌    | 1028/1850 [1:08:33<52:18,  3.82s/it]
 56%|█████▌    | 1029/1850 [1:08:37<51:10,  3.74s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03076171875, 'learning_rate': 4.437837837837838e-05, 'epoch': 27.81}

 56%|█████▌    | 1029/1850 [1:08:37<51:10,  3.74s/it]
 56%|█████▌    | 1030/1850 [1:08:41<52:45,  3.86s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03369140625, 'learning_rate': 4.432432432432433e-05, 'epoch': 27.84}

 56%|█████▌    | 1030/1850 [1:08:41<52:45,  3.86s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 56%|█████▌    | 1031/1850 [1:08:44<51:39,  3.78s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.029296875, 'learning_rate': 4.427027027027027e-05, 'epoch': 27.86}

 56%|█████▌    | 1031/1850 [1:08:44<51:39,  3.78s/it]
 56%|█████▌    | 1032/1850 [1:08:49<53:50,  3.95s/it]
                                                     
{'loss': 0.0021, 'grad_norm': 0.1953125, 'learning_rate': 4.4216216216216214e-05, 'epoch': 27.89}

 56%|█████▌    | 1032/1850 [1:08:49<53:50,  3.95s/it]
 56%|█████▌    | 1033/1850 [1:08:53<53:57,  3.96s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03369140625, 'learning_rate': 4.4162162162162166e-05, 'epoch': 27.92}

 56%|█████▌    | 1033/1850 [1:08:53<53:57,  3.96s/it]
 56%|█████▌    | 1034/1850 [1:08:57<55:56,  4.11s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031494140625, 'learning_rate': 4.410810810810811e-05, 'epoch': 27.95}

 56%|█████▌    | 1034/1850 [1:08:57<55:56,  4.11s/it]
 56%|█████▌    | 1035/1850 [1:09:01<55:23,  4.08s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0380859375, 'learning_rate': 4.4054054054054054e-05, 'epoch': 27.97}

 56%|█████▌    | 1035/1850 [1:09:01<55:23,  4.08s/it]
 56%|█████▌    | 1036/1850 [1:09:05<55:56,  4.12s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.031982421875, 'learning_rate': 4.4000000000000006e-05, 'epoch': 28.0}

 56%|█████▌    | 1036/1850 [1:09:05<55:56,  4.12s/it]
 56%|█████▌    | 1037/1850 [1:09:09<55:41,  4.11s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0341796875, 'learning_rate': 4.394594594594595e-05, 'epoch': 28.03}

 56%|█████▌    | 1037/1850 [1:09:09<55:41,  4.11s/it]
 56%|█████▌    | 1038/1850 [1:09:13<55:38,  4.11s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0322265625, 'learning_rate': 4.3891891891891895e-05, 'epoch': 28.05}

 56%|█████▌    | 1038/1850 [1:09:13<55:38,  4.11s/it]
 56%|█████▌    | 1039/1850 [1:09:17<54:05,  4.00s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03564453125, 'learning_rate': 4.383783783783784e-05, 'epoch': 28.08}

 56%|█████▌    | 1039/1850 [1:09:17<54:05,  4.00s/it]
 56%|█████▌    | 1040/1850 [1:09:22<55:46,  4.13s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.037353515625, 'learning_rate': 4.378378378378379e-05, 'epoch': 28.11}

 56%|█████▌    | 1040/1850 [1:09:22<55:46,  4.13s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 56%|█████▋    | 1041/1850 [1:09:26<58:29,  4.34s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.0361328125, 'learning_rate': 4.372972972972973e-05, 'epoch': 28.14}

 56%|█████▋    | 1041/1850 [1:09:26<58:29,  4.34s/it]
 56%|█████▋    | 1042/1850 [1:09:31<57:15,  4.25s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 4.367567567567568e-05, 'epoch': 28.16}

 56%|█████▋    | 1042/1850 [1:09:31<57:15,  4.25s/it]
 56%|█████▋    | 1043/1850 [1:09:35<57:34,  4.28s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.034423828125, 'learning_rate': 4.3621621621621624e-05, 'epoch': 28.19}

 56%|█████▋    | 1043/1850 [1:09:35<57:34,  4.28s/it]
 56%|█████▋    | 1044/1850 [1:09:38<53:50,  4.01s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0296630859375, 'learning_rate': 4.356756756756757e-05, 'epoch': 28.22}

 56%|█████▋    | 1044/1850 [1:09:38<53:50,  4.01s/it]
 56%|█████▋    | 1045/1850 [1:09:42<51:32,  3.84s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02880859375, 'learning_rate': 4.351351351351351e-05, 'epoch': 28.24}

 56%|█████▋    | 1045/1850 [1:09:42<51:32,  3.84s/it]
 57%|█████▋    | 1046/1850 [1:09:46<51:58,  3.88s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033447265625, 'learning_rate': 4.3459459459459464e-05, 'epoch': 28.27}

 57%|█████▋    | 1046/1850 [1:09:46<51:58,  3.88s/it]
 57%|█████▋    | 1047/1850 [1:09:50<52:59,  3.96s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031494140625, 'learning_rate': 4.340540540540541e-05, 'epoch': 28.3}

 57%|█████▋    | 1047/1850 [1:09:50<52:59,  3.96s/it]
 57%|█████▋    | 1048/1850 [1:09:53<51:32,  3.86s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0299072265625, 'learning_rate': 4.335135135135135e-05, 'epoch': 28.32}

 57%|█████▋    | 1048/1850 [1:09:53<51:32,  3.86s/it]
 57%|█████▋    | 1049/1850 [1:09:57<50:31,  3.79s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.036376953125, 'learning_rate': 4.3297297297297304e-05, 'epoch': 28.35}

 57%|█████▋    | 1049/1850 [1:09:57<50:31,  3.79s/it]
 57%|█████▋    | 1050/1850 [1:10:01<51:19,  3.85s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03662109375, 'learning_rate': 4.324324324324325e-05, 'epoch': 28.38}

 57%|█████▋    | 1050/1850 [1:10:01<51:19,  3.85s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 57%|█████▋    | 1051/1850 [1:10:06<57:06,  4.29s/it]
                                                     
{'loss': 0.0016, 'grad_norm': 0.041748046875, 'learning_rate': 4.3189189189189186e-05, 'epoch': 28.41}

 57%|█████▋    | 1051/1850 [1:10:06<57:06,  4.29s/it]
 57%|█████▋    | 1052/1850 [1:10:11<57:00,  4.29s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03759765625, 'learning_rate': 4.313513513513514e-05, 'epoch': 28.43}

 57%|█████▋    | 1052/1850 [1:10:11<57:00,  4.29s/it]
 57%|█████▋    | 1053/1850 [1:10:14<54:48,  4.13s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.032958984375, 'learning_rate': 4.308108108108108e-05, 'epoch': 28.46}

 57%|█████▋    | 1053/1850 [1:10:14<54:48,  4.13s/it]
 57%|█████▋    | 1054/1850 [1:10:19<55:23,  4.18s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03515625, 'learning_rate': 4.3027027027027026e-05, 'epoch': 28.49}

 57%|█████▋    | 1054/1850 [1:10:19<55:23,  4.18s/it]
 57%|█████▋    | 1055/1850 [1:10:23<54:29,  4.11s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0712890625, 'learning_rate': 4.297297297297298e-05, 'epoch': 28.51}

 57%|█████▋    | 1055/1850 [1:10:23<54:29,  4.11s/it]
 57%|█████▋    | 1056/1850 [1:10:27<55:17,  4.18s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.037353515625, 'learning_rate': 4.291891891891892e-05, 'epoch': 28.54}

 57%|█████▋    | 1056/1850 [1:10:27<55:17,  4.18s/it]
 57%|█████▋    | 1057/1850 [1:10:31<53:02,  4.01s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031982421875, 'learning_rate': 4.2864864864864866e-05, 'epoch': 28.57}

 57%|█████▋    | 1057/1850 [1:10:31<53:02,  4.01s/it]
 57%|█████▋    | 1058/1850 [1:10:35<54:28,  4.13s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.037109375, 'learning_rate': 4.281081081081082e-05, 'epoch': 28.59}

 57%|█████▋    | 1058/1850 [1:10:35<54:28,  4.13s/it]
 57%|█████▋    | 1059/1850 [1:10:39<52:12,  3.96s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.033203125, 'learning_rate': 4.275675675675676e-05, 'epoch': 28.62}

 57%|█████▋    | 1059/1850 [1:10:39<52:12,  3.96s/it]
 57%|█████▋    | 1060/1850 [1:10:42<51:48,  3.94s/it]
                                                     
{'loss': 0.0022, 'grad_norm': 0.208984375, 'learning_rate': 4.2702702702702706e-05, 'epoch': 28.65}

 57%|█████▋    | 1060/1850 [1:10:42<51:48,  3.94s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 57%|█████▋    | 1061/1850 [1:10:46<51:34,  3.92s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03369140625, 'learning_rate': 4.264864864864865e-05, 'epoch': 28.68}

 57%|█████▋    | 1061/1850 [1:10:46<51:34,  3.92s/it]
 57%|█████▋    | 1062/1850 [1:10:51<52:28,  4.00s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.036376953125, 'learning_rate': 4.2594594594594595e-05, 'epoch': 28.7}

 57%|█████▋    | 1062/1850 [1:10:51<52:28,  4.00s/it]
 57%|█████▋    | 1063/1850 [1:10:55<53:24,  4.07s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0322265625, 'learning_rate': 4.254054054054054e-05, 'epoch': 28.73}

 57%|█████▋    | 1063/1850 [1:10:55<53:24,  4.07s/it]
 58%|█████▊    | 1064/1850 [1:10:59<53:25,  4.08s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.034912109375, 'learning_rate': 4.248648648648649e-05, 'epoch': 28.76}

 58%|█████▊    | 1064/1850 [1:10:59<53:25,  4.08s/it]
 58%|█████▊    | 1065/1850 [1:11:03<52:00,  3.98s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03564453125, 'learning_rate': 4.2432432432432435e-05, 'epoch': 28.78}

 58%|█████▊    | 1065/1850 [1:11:03<52:00,  3.98s/it]
 58%|█████▊    | 1066/1850 [1:11:06<51:09,  3.92s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.041015625, 'learning_rate': 4.237837837837838e-05, 'epoch': 28.81}

 58%|█████▊    | 1066/1850 [1:11:06<51:09,  3.92s/it]
 58%|█████▊    | 1067/1850 [1:11:10<49:04,  3.76s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0296630859375, 'learning_rate': 4.2324324324324324e-05, 'epoch': 28.84}

 58%|█████▊    | 1067/1850 [1:11:10<49:04,  3.76s/it]
 58%|█████▊    | 1068/1850 [1:11:14<50:38,  3.89s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0308837890625, 'learning_rate': 4.2270270270270275e-05, 'epoch': 28.86}

 58%|█████▊    | 1068/1850 [1:11:14<50:38,  3.89s/it]
 58%|█████▊    | 1069/1850 [1:11:18<51:23,  3.95s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0303955078125, 'learning_rate': 4.221621621621622e-05, 'epoch': 28.89}

 58%|█████▊    | 1069/1850 [1:11:18<51:23,  3.95s/it]
 58%|█████▊    | 1070/1850 [1:11:22<51:34,  3.97s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.041015625, 'learning_rate': 4.2162162162162164e-05, 'epoch': 28.92}

 58%|█████▊    | 1070/1850 [1:11:22<51:34,  3.97s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 58%|█████▊    | 1071/1850 [1:11:26<50:52,  3.92s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.025390625, 'learning_rate': 4.210810810810811e-05, 'epoch': 28.95}

 58%|█████▊    | 1071/1850 [1:11:26<50:52,  3.92s/it]
 58%|█████▊    | 1072/1850 [1:11:29<49:07,  3.79s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.044189453125, 'learning_rate': 4.205405405405405e-05, 'epoch': 28.97}

 58%|█████▊    | 1072/1850 [1:11:29<49:07,  3.79s/it]
 58%|█████▊    | 1073/1850 [1:11:33<49:57,  3.86s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0294189453125, 'learning_rate': 4.2e-05, 'epoch': 29.0}

 58%|█████▊    | 1073/1850 [1:11:33<49:57,  3.86s/it]
 58%|█████▊    | 1074/1850 [1:11:37<49:47,  3.85s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.03125, 'learning_rate': 4.194594594594595e-05, 'epoch': 29.03}

 58%|█████▊    | 1074/1850 [1:11:37<49:47,  3.85s/it]
 58%|█████▊    | 1075/1850 [1:11:42<51:36,  4.00s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.037353515625, 'learning_rate': 4.189189189189189e-05, 'epoch': 29.05}

 58%|█████▊    | 1075/1850 [1:11:42<51:36,  4.00s/it]
 58%|█████▊    | 1076/1850 [1:11:45<51:21,  3.98s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02978515625, 'learning_rate': 4.183783783783784e-05, 'epoch': 29.08}

 58%|█████▊    | 1076/1850 [1:11:45<51:21,  3.98s/it]
 58%|█████▊    | 1077/1850 [1:11:50<51:55,  4.03s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0308837890625, 'learning_rate': 4.178378378378379e-05, 'epoch': 29.11}

 58%|█████▊    | 1077/1850 [1:11:50<51:55,  4.03s/it]
 58%|█████▊    | 1078/1850 [1:11:54<52:31,  4.08s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 4.1729729729729733e-05, 'epoch': 29.14}

 58%|█████▊    | 1078/1850 [1:11:54<52:31,  4.08s/it]
 58%|█████▊    | 1079/1850 [1:11:57<49:56,  3.89s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.03173828125, 'learning_rate': 4.167567567567568e-05, 'epoch': 29.16}

 58%|█████▊    | 1079/1850 [1:11:57<49:56,  3.89s/it]
 58%|█████▊    | 1080/1850 [1:12:02<52:09,  4.06s/it]
                                                     
{'loss': 0.0016, 'grad_norm': 0.039306640625, 'learning_rate': 4.162162162162163e-05, 'epoch': 29.19}

 58%|█████▊    | 1080/1850 [1:12:02<52:09,  4.06s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 58%|█████▊    | 1081/1850 [1:12:06<51:04,  3.99s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.035400390625, 'learning_rate': 4.156756756756757e-05, 'epoch': 29.22}

 58%|█████▊    | 1081/1850 [1:12:06<51:04,  3.99s/it]
 58%|█████▊    | 1082/1850 [1:12:09<49:03,  3.83s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.036865234375, 'learning_rate': 4.151351351351351e-05, 'epoch': 29.24}

 58%|█████▊    | 1082/1850 [1:12:09<49:03,  3.83s/it]
 59%|█████▊    | 1083/1850 [1:12:13<48:33,  3.80s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033203125, 'learning_rate': 4.145945945945946e-05, 'epoch': 29.27}

 59%|█████▊    | 1083/1850 [1:12:13<48:33,  3.80s/it]
 59%|█████▊    | 1084/1850 [1:12:17<49:26,  3.87s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.035400390625, 'learning_rate': 4.140540540540541e-05, 'epoch': 29.3}

 59%|█████▊    | 1084/1850 [1:12:17<49:26,  3.87s/it]
 59%|█████▊    | 1085/1850 [1:12:21<50:08,  3.93s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.026611328125, 'learning_rate': 4.135135135135135e-05, 'epoch': 29.32}

 59%|█████▊    | 1085/1850 [1:12:21<50:08,  3.93s/it]
 59%|█████▊    | 1086/1850 [1:12:25<50:27,  3.96s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03466796875, 'learning_rate': 4.12972972972973e-05, 'epoch': 29.35}

 59%|█████▊    | 1086/1850 [1:12:25<50:27,  3.96s/it]
 59%|█████▉    | 1087/1850 [1:12:29<50:09,  3.94s/it]
                                                     
{'loss': 0.0016, 'grad_norm': 0.12109375, 'learning_rate': 4.124324324324325e-05, 'epoch': 29.38}

 59%|█████▉    | 1087/1850 [1:12:29<50:09,  3.94s/it]
 59%|█████▉    | 1088/1850 [1:12:32<48:34,  3.82s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03076171875, 'learning_rate': 4.118918918918919e-05, 'epoch': 29.41}

 59%|█████▉    | 1088/1850 [1:12:32<48:34,  3.82s/it]
 59%|█████▉    | 1089/1850 [1:12:35<45:50,  3.61s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03173828125, 'learning_rate': 4.1135135135135136e-05, 'epoch': 29.43}

 59%|█████▉    | 1089/1850 [1:12:35<45:50,  3.61s/it]
 59%|█████▉    | 1090/1850 [1:12:40<47:36,  3.76s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03076171875, 'learning_rate': 4.108108108108109e-05, 'epoch': 29.46}

 59%|█████▉    | 1090/1850 [1:12:40<47:36,  3.76s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 59%|█████▉    | 1091/1850 [1:12:44<48:23,  3.83s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0341796875, 'learning_rate': 4.1027027027027025e-05, 'epoch': 29.49}

 59%|█████▉    | 1091/1850 [1:12:44<48:23,  3.83s/it]
 59%|█████▉    | 1092/1850 [1:12:48<52:29,  4.16s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.033935546875, 'learning_rate': 4.0972972972972976e-05, 'epoch': 29.51}

 59%|█████▉    | 1092/1850 [1:12:48<52:29,  4.16s/it]
 59%|█████▉    | 1093/1850 [1:12:53<53:54,  4.27s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0322265625, 'learning_rate': 4.091891891891892e-05, 'epoch': 29.54}

 59%|█████▉    | 1093/1850 [1:12:53<53:54,  4.27s/it]
 59%|█████▉    | 1094/1850 [1:12:57<51:25,  4.08s/it]
                                                     
{'loss': 0.0017, 'grad_norm': 0.2236328125, 'learning_rate': 4.0864864864864865e-05, 'epoch': 29.57}

 59%|█████▉    | 1094/1850 [1:12:57<51:25,  4.08s/it]
 59%|█████▉    | 1095/1850 [1:13:00<50:05,  3.98s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033447265625, 'learning_rate': 4.081081081081081e-05, 'epoch': 29.59}

 59%|█████▉    | 1095/1850 [1:13:00<50:05,  3.98s/it]
 59%|█████▉    | 1096/1850 [1:13:05<52:40,  4.19s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.03662109375, 'learning_rate': 4.075675675675676e-05, 'epoch': 29.62}

 59%|█████▉    | 1096/1850 [1:13:05<52:40,  4.19s/it]
 59%|█████▉    | 1097/1850 [1:13:10<55:06,  4.39s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.0390625, 'learning_rate': 4.0702702702702705e-05, 'epoch': 29.65}

 59%|█████▉    | 1097/1850 [1:13:10<55:06,  4.39s/it]
 59%|█████▉    | 1098/1850 [1:13:14<54:20,  4.34s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03369140625, 'learning_rate': 4.064864864864865e-05, 'epoch': 29.68}

 59%|█████▉    | 1098/1850 [1:13:14<54:20,  4.34s/it]
 59%|█████▉    | 1099/1850 [1:13:18<53:20,  4.26s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.031494140625, 'learning_rate': 4.05945945945946e-05, 'epoch': 29.7}

 59%|█████▉    | 1099/1850 [1:13:18<53:20,  4.26s/it]
 59%|█████▉    | 1100/1850 [1:13:22<51:46,  4.14s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.034423828125, 'learning_rate': 4.0540540540540545e-05, 'epoch': 29.73}

 59%|█████▉    | 1100/1850 [1:13:22<51:46,  4.14s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 60%|█████▉    | 1101/1850 [1:13:26<52:42,  4.22s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0279541015625, 'learning_rate': 4.048648648648648e-05, 'epoch': 29.76}

 60%|█████▉    | 1101/1850 [1:13:26<52:42,  4.22s/it]
 60%|█████▉    | 1102/1850 [1:13:30<51:26,  4.13s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0341796875, 'learning_rate': 4.0432432432432434e-05, 'epoch': 29.78}

 60%|█████▉    | 1102/1850 [1:13:30<51:26,  4.13s/it]
 60%|█████▉    | 1103/1850 [1:13:34<50:05,  4.02s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033935546875, 'learning_rate': 4.037837837837838e-05, 'epoch': 29.81}

 60%|█████▉    | 1103/1850 [1:13:34<50:05,  4.02s/it]
 60%|█████▉    | 1104/1850 [1:13:38<49:48,  4.01s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03271484375, 'learning_rate': 4.032432432432432e-05, 'epoch': 29.84}

 60%|█████▉    | 1104/1850 [1:13:38<49:48,  4.01s/it]
 60%|█████▉    | 1105/1850 [1:13:42<49:38,  4.00s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029052734375, 'learning_rate': 4.0270270270270274e-05, 'epoch': 29.86}

 60%|█████▉    | 1105/1850 [1:13:42<49:38,  4.00s/it]
 60%|█████▉    | 1106/1850 [1:13:46<48:05,  3.88s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.032958984375, 'learning_rate': 4.021621621621622e-05, 'epoch': 29.89}

 60%|█████▉    | 1106/1850 [1:13:46<48:05,  3.88s/it]
 60%|█████▉    | 1107/1850 [1:13:50<49:05,  3.96s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.032470703125, 'learning_rate': 4.016216216216216e-05, 'epoch': 29.92}

 60%|█████▉    | 1107/1850 [1:13:50<49:05,  3.96s/it]
 60%|█████▉    | 1108/1850 [1:13:54<48:41,  3.94s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0272216796875, 'learning_rate': 4.0108108108108114e-05, 'epoch': 29.95}

 60%|█████▉    | 1108/1850 [1:13:54<48:41,  3.94s/it]
 60%|█████▉    | 1109/1850 [1:13:57<46:28,  3.76s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0322265625, 'learning_rate': 4.005405405405406e-05, 'epoch': 29.97}

 60%|█████▉    | 1109/1850 [1:13:57<46:28,  3.76s/it]
 60%|██████    | 1110/1850 [1:14:01<45:45,  3.71s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.031494140625, 'learning_rate': 4e-05, 'epoch': 30.0}

 60%|██████    | 1110/1850 [1:14:01<45:45,  3.71s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 60%|██████    | 1111/1850 [1:14:05<49:02,  3.98s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0341796875, 'learning_rate': 3.994594594594595e-05, 'epoch': 30.03}

 60%|██████    | 1111/1850 [1:14:05<49:02,  3.98s/it]
 60%|██████    | 1112/1850 [1:14:10<51:39,  4.20s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.09814453125, 'learning_rate': 3.989189189189189e-05, 'epoch': 30.05}

 60%|██████    | 1112/1850 [1:14:10<51:39,  4.20s/it]
 60%|██████    | 1113/1850 [1:14:13<48:32,  3.95s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.032470703125, 'learning_rate': 3.983783783783784e-05, 'epoch': 30.08}

 60%|██████    | 1113/1850 [1:14:13<48:32,  3.95s/it]
 60%|██████    | 1114/1850 [1:14:18<49:26,  4.03s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.032958984375, 'learning_rate': 3.978378378378379e-05, 'epoch': 30.11}

 60%|██████    | 1114/1850 [1:14:18<49:26,  4.03s/it]
 60%|██████    | 1115/1850 [1:14:22<50:39,  4.14s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033447265625, 'learning_rate': 3.972972972972973e-05, 'epoch': 30.14}

 60%|██████    | 1115/1850 [1:14:22<50:39,  4.14s/it]
 60%|██████    | 1116/1850 [1:14:26<49:56,  4.08s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03759765625, 'learning_rate': 3.967567567567568e-05, 'epoch': 30.16}

 60%|██████    | 1116/1850 [1:14:26<49:56,  4.08s/it]
 60%|██████    | 1117/1850 [1:14:30<49:21,  4.04s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0341796875, 'learning_rate': 3.962162162162162e-05, 'epoch': 30.19}

 60%|██████    | 1117/1850 [1:14:30<49:21,  4.04s/it]
 60%|██████    | 1118/1850 [1:14:33<46:48,  3.84s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0302734375, 'learning_rate': 3.956756756756757e-05, 'epoch': 30.22}

 60%|██████    | 1118/1850 [1:14:33<46:48,  3.84s/it]
 60%|██████    | 1119/1850 [1:14:38<49:32,  4.07s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.036865234375, 'learning_rate': 3.951351351351352e-05, 'epoch': 30.24}

 60%|██████    | 1119/1850 [1:14:38<49:32,  4.07s/it]
 61%|██████    | 1120/1850 [1:14:42<49:22,  4.06s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0341796875, 'learning_rate': 3.945945945945946e-05, 'epoch': 30.27}

 61%|██████    | 1120/1850 [1:14:42<49:22,  4.06s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 61%|██████    | 1121/1850 [1:14:46<49:30,  4.07s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031982421875, 'learning_rate': 3.940540540540541e-05, 'epoch': 30.3}

 61%|██████    | 1121/1850 [1:14:46<49:30,  4.07s/it]
 61%|██████    | 1122/1850 [1:14:50<49:07,  4.05s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.031494140625, 'learning_rate': 3.935135135135135e-05, 'epoch': 30.32}

 61%|██████    | 1122/1850 [1:14:50<49:07,  4.05s/it]
 61%|██████    | 1123/1850 [1:14:54<47:15,  3.90s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03076171875, 'learning_rate': 3.9297297297297295e-05, 'epoch': 30.35}

 61%|██████    | 1123/1850 [1:14:54<47:15,  3.90s/it]
 61%|██████    | 1124/1850 [1:14:58<47:46,  3.95s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02978515625, 'learning_rate': 3.9243243243243246e-05, 'epoch': 30.38}

 61%|██████    | 1124/1850 [1:14:58<47:46,  3.95s/it]
 61%|██████    | 1125/1850 [1:15:01<46:29,  3.85s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03466796875, 'learning_rate': 3.918918918918919e-05, 'epoch': 30.41}

 61%|██████    | 1125/1850 [1:15:01<46:29,  3.85s/it]
 61%|██████    | 1126/1850 [1:15:05<47:21,  3.92s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.033203125, 'learning_rate': 3.9135135135135135e-05, 'epoch': 30.43}

 61%|██████    | 1126/1850 [1:15:05<47:21,  3.92s/it]
 61%|██████    | 1127/1850 [1:15:09<45:00,  3.74s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0303955078125, 'learning_rate': 3.9081081081081086e-05, 'epoch': 30.46}

 61%|██████    | 1127/1850 [1:15:09<45:00,  3.74s/it]
 61%|██████    | 1128/1850 [1:15:13<45:50,  3.81s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03369140625, 'learning_rate': 3.902702702702703e-05, 'epoch': 30.49}

 61%|██████    | 1128/1850 [1:15:13<45:50,  3.81s/it]
 61%|██████    | 1129/1850 [1:15:16<43:52,  3.65s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.028564453125, 'learning_rate': 3.8972972972972975e-05, 'epoch': 30.51}

 61%|██████    | 1129/1850 [1:15:16<43:52,  3.65s/it]
 61%|██████    | 1130/1850 [1:15:20<47:04,  3.92s/it]
                                                     
{'loss': 0.0022, 'grad_norm': 0.216796875, 'learning_rate': 3.8918918918918926e-05, 'epoch': 30.54}

 61%|██████    | 1130/1850 [1:15:20<47:04,  3.92s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 61%|██████    | 1131/1850 [1:15:25<48:44,  4.07s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.032470703125, 'learning_rate': 3.886486486486487e-05, 'epoch': 30.57}

 61%|██████    | 1131/1850 [1:15:25<48:44,  4.07s/it]
 61%|██████    | 1132/1850 [1:15:28<46:14,  3.86s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03369140625, 'learning_rate': 3.881081081081081e-05, 'epoch': 30.59}

 61%|██████    | 1132/1850 [1:15:28<46:14,  3.86s/it]
 61%|██████    | 1133/1850 [1:15:33<49:19,  4.13s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0296630859375, 'learning_rate': 3.875675675675676e-05, 'epoch': 30.62}

 61%|██████    | 1133/1850 [1:15:33<49:19,  4.13s/it]
 61%|██████▏   | 1134/1850 [1:15:36<46:44,  3.92s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0311279296875, 'learning_rate': 3.8702702702702704e-05, 'epoch': 30.65}

 61%|██████▏   | 1134/1850 [1:15:36<46:44,  3.92s/it]
 61%|██████▏   | 1135/1850 [1:15:40<43:51,  3.68s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0277099609375, 'learning_rate': 3.864864864864865e-05, 'epoch': 30.68}

 61%|██████▏   | 1135/1850 [1:15:40<43:51,  3.68s/it]
 61%|██████▏   | 1136/1850 [1:15:44<47:55,  4.03s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.0361328125, 'learning_rate': 3.85945945945946e-05, 'epoch': 30.7}

 61%|██████▏   | 1136/1850 [1:15:44<47:55,  4.03s/it]
 61%|██████▏   | 1137/1850 [1:15:48<46:41,  3.93s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0322265625, 'learning_rate': 3.8540540540540544e-05, 'epoch': 30.73}

 61%|██████▏   | 1137/1850 [1:15:48<46:41,  3.93s/it]
 62%|██████▏   | 1138/1850 [1:15:52<45:28,  3.83s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0341796875, 'learning_rate': 3.848648648648649e-05, 'epoch': 30.76}

 62%|██████▏   | 1138/1850 [1:15:52<45:28,  3.83s/it]
 62%|██████▏   | 1139/1850 [1:15:56<47:15,  3.99s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031982421875, 'learning_rate': 3.843243243243243e-05, 'epoch': 30.78}

 62%|██████▏   | 1139/1850 [1:15:56<47:15,  3.99s/it]
 62%|██████▏   | 1140/1850 [1:16:00<46:21,  3.92s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029052734375, 'learning_rate': 3.8378378378378384e-05, 'epoch': 30.81}

 62%|██████▏   | 1140/1850 [1:16:00<46:21,  3.92s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 62%|██████▏   | 1141/1850 [1:16:04<47:30,  4.02s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0289306640625, 'learning_rate': 3.832432432432433e-05, 'epoch': 30.84}

 62%|██████▏   | 1141/1850 [1:16:04<47:30,  4.02s/it]
 62%|██████▏   | 1142/1850 [1:16:08<46:39,  3.95s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.030029296875, 'learning_rate': 3.827027027027027e-05, 'epoch': 30.86}

 62%|██████▏   | 1142/1850 [1:16:08<46:39,  3.95s/it]
 62%|██████▏   | 1143/1850 [1:16:12<46:21,  3.93s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02978515625, 'learning_rate': 3.821621621621622e-05, 'epoch': 30.89}

 62%|██████▏   | 1143/1850 [1:16:12<46:21,  3.93s/it]
 62%|██████▏   | 1144/1850 [1:16:15<45:03,  3.83s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03076171875, 'learning_rate': 3.816216216216216e-05, 'epoch': 30.92}

 62%|██████▏   | 1144/1850 [1:16:15<45:03,  3.83s/it]
 62%|██████▏   | 1145/1850 [1:16:20<46:52,  3.99s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.036376953125, 'learning_rate': 3.8108108108108106e-05, 'epoch': 30.95}

 62%|██████▏   | 1145/1850 [1:16:20<46:52,  3.99s/it]
 62%|██████▏   | 1146/1850 [1:16:24<46:44,  3.98s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0341796875, 'learning_rate': 3.805405405405406e-05, 'epoch': 30.97}

 62%|██████▏   | 1146/1850 [1:16:24<46:44,  3.98s/it]
 62%|██████▏   | 1147/1850 [1:16:27<45:45,  3.91s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03125, 'learning_rate': 3.8e-05, 'epoch': 31.0}

 62%|██████▏   | 1147/1850 [1:16:27<45:45,  3.91s/it]
 62%|██████▏   | 1148/1850 [1:16:32<48:01,  4.11s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.0771484375, 'learning_rate': 3.7945945945945947e-05, 'epoch': 31.03}

 62%|██████▏   | 1148/1850 [1:16:32<48:01,  4.11s/it]
 62%|██████▏   | 1149/1850 [1:16:35<46:01,  3.94s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.032958984375, 'learning_rate': 3.78918918918919e-05, 'epoch': 31.05}

 62%|██████▏   | 1149/1850 [1:16:36<46:01,  3.94s/it]
 62%|██████▏   | 1150/1850 [1:16:39<43:46,  3.75s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0306396484375, 'learning_rate': 3.783783783783784e-05, 'epoch': 31.08}

 62%|██████▏   | 1150/1850 [1:16:39<43:46,  3.75s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 62%|██████▏   | 1151/1850 [1:16:43<46:35,  4.00s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0303955078125, 'learning_rate': 3.778378378378379e-05, 'epoch': 31.11}

 62%|██████▏   | 1151/1850 [1:16:43<46:35,  4.00s/it]
 62%|██████▏   | 1152/1850 [1:16:47<45:56,  3.95s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0306396484375, 'learning_rate': 3.772972972972973e-05, 'epoch': 31.14}

 62%|██████▏   | 1152/1850 [1:16:47<45:56,  3.95s/it]
 62%|██████▏   | 1153/1850 [1:16:51<44:53,  3.86s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.032470703125, 'learning_rate': 3.7675675675675676e-05, 'epoch': 31.16}

 62%|██████▏   | 1153/1850 [1:16:51<44:53,  3.86s/it]
 62%|██████▏   | 1154/1850 [1:16:55<46:08,  3.98s/it]
                                                     
{'loss': 0.0015, 'grad_norm': 0.10693359375, 'learning_rate': 3.762162162162162e-05, 'epoch': 31.19}

 62%|██████▏   | 1154/1850 [1:16:55<46:08,  3.98s/it]
 62%|██████▏   | 1155/1850 [1:16:59<45:07,  3.90s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.034912109375, 'learning_rate': 3.756756756756757e-05, 'epoch': 31.22}

 62%|██████▏   | 1155/1850 [1:16:59<45:07,  3.90s/it]
 62%|██████▏   | 1156/1850 [1:17:03<47:04,  4.07s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.028564453125, 'learning_rate': 3.7513513513513516e-05, 'epoch': 31.24}

 62%|██████▏   | 1156/1850 [1:17:03<47:04,  4.07s/it]
 63%|██████▎   | 1157/1850 [1:17:07<47:08,  4.08s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033203125, 'learning_rate': 3.745945945945946e-05, 'epoch': 31.27}

 63%|██████▎   | 1157/1850 [1:17:07<47:08,  4.08s/it]
 63%|██████▎   | 1158/1850 [1:17:11<45:33,  3.95s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.03125, 'learning_rate': 3.740540540540541e-05, 'epoch': 31.3}

 63%|██████▎   | 1158/1850 [1:17:11<45:33,  3.95s/it]
 63%|██████▎   | 1159/1850 [1:17:14<43:24,  3.77s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.03173828125, 'learning_rate': 3.7351351351351356e-05, 'epoch': 31.32}

 63%|██████▎   | 1159/1850 [1:17:14<43:24,  3.77s/it]
 63%|██████▎   | 1160/1850 [1:17:18<44:07,  3.84s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03173828125, 'learning_rate': 3.72972972972973e-05, 'epoch': 31.35}

 63%|██████▎   | 1160/1850 [1:17:18<44:07,  3.84s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 63%|██████▎   | 1161/1850 [1:17:23<46:31,  4.05s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0301513671875, 'learning_rate': 3.7243243243243245e-05, 'epoch': 31.38}

 63%|██████▎   | 1161/1850 [1:17:23<46:31,  4.05s/it]
 63%|██████▎   | 1162/1850 [1:17:27<45:29,  3.97s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.033935546875, 'learning_rate': 3.718918918918919e-05, 'epoch': 31.41}

 63%|██████▎   | 1162/1850 [1:17:27<45:29,  3.97s/it]
 63%|██████▎   | 1163/1850 [1:17:30<44:03,  3.85s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03173828125, 'learning_rate': 3.7135135135135134e-05, 'epoch': 31.43}

 63%|██████▎   | 1163/1850 [1:17:30<44:03,  3.85s/it]
 63%|██████▎   | 1164/1850 [1:17:35<46:11,  4.04s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.037353515625, 'learning_rate': 3.7081081081081085e-05, 'epoch': 31.46}

 63%|██████▎   | 1164/1850 [1:17:35<46:11,  4.04s/it]
 63%|██████▎   | 1165/1850 [1:17:39<46:22,  4.06s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.032958984375, 'learning_rate': 3.702702702702703e-05, 'epoch': 31.49}

 63%|██████▎   | 1165/1850 [1:17:39<46:22,  4.06s/it]
 63%|██████▎   | 1166/1850 [1:17:42<43:49,  3.84s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0322265625, 'learning_rate': 3.6972972972972974e-05, 'epoch': 31.51}

 63%|██████▎   | 1166/1850 [1:17:42<43:49,  3.84s/it]
 63%|██████▎   | 1167/1850 [1:17:46<43:30,  3.82s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0284423828125, 'learning_rate': 3.691891891891892e-05, 'epoch': 31.54}

 63%|██████▎   | 1167/1850 [1:17:46<43:30,  3.82s/it]
 63%|██████▎   | 1168/1850 [1:17:50<42:37,  3.75s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.032958984375, 'learning_rate': 3.686486486486487e-05, 'epoch': 31.57}

 63%|██████▎   | 1168/1850 [1:17:50<42:37,  3.75s/it]
 63%|██████▎   | 1169/1850 [1:17:54<44:31,  3.92s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0257568359375, 'learning_rate': 3.6810810810810814e-05, 'epoch': 31.59}

 63%|██████▎   | 1169/1850 [1:17:54<44:31,  3.92s/it]
 63%|██████▎   | 1170/1850 [1:17:58<45:57,  4.05s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.032470703125, 'learning_rate': 3.675675675675676e-05, 'epoch': 31.62}

 63%|██████▎   | 1170/1850 [1:17:58<45:57,  4.05s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 63%|██████▎   | 1171/1850 [1:18:03<46:45,  4.13s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 3.670270270270271e-05, 'epoch': 31.65}

 63%|██████▎   | 1171/1850 [1:18:03<46:45,  4.13s/it]
 63%|██████▎   | 1172/1850 [1:18:07<45:57,  4.07s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0301513671875, 'learning_rate': 3.664864864864865e-05, 'epoch': 31.68}

 63%|██████▎   | 1172/1850 [1:18:07<45:57,  4.07s/it]
 63%|██████▎   | 1173/1850 [1:18:10<45:23,  4.02s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.028076171875, 'learning_rate': 3.659459459459459e-05, 'epoch': 31.7}

 63%|██████▎   | 1173/1850 [1:18:10<45:23,  4.02s/it]
 63%|██████▎   | 1174/1850 [1:18:15<46:25,  4.12s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.034912109375, 'learning_rate': 3.654054054054054e-05, 'epoch': 31.73}

 63%|██████▎   | 1174/1850 [1:18:15<46:25,  4.12s/it]
 64%|██████▎   | 1175/1850 [1:18:19<48:05,  4.28s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.031982421875, 'learning_rate': 3.648648648648649e-05, 'epoch': 31.76}

 64%|██████▎   | 1175/1850 [1:18:19<48:05,  4.28s/it]
 64%|██████▎   | 1176/1850 [1:18:24<48:37,  4.33s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.03515625, 'learning_rate': 3.643243243243243e-05, 'epoch': 31.78}

 64%|██████▎   | 1176/1850 [1:18:24<48:37,  4.33s/it]
 64%|██████▎   | 1177/1850 [1:18:28<47:17,  4.22s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03271484375, 'learning_rate': 3.637837837837838e-05, 'epoch': 31.81}

 64%|██████▎   | 1177/1850 [1:18:28<47:17,  4.22s/it]
 64%|██████▎   | 1178/1850 [1:18:31<44:52,  4.01s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0302734375, 'learning_rate': 3.632432432432433e-05, 'epoch': 31.84}

 64%|██████▎   | 1178/1850 [1:18:31<44:52,  4.01s/it]
 64%|██████▎   | 1179/1850 [1:18:35<44:18,  3.96s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029541015625, 'learning_rate': 3.627027027027027e-05, 'epoch': 31.86}

 64%|██████▎   | 1179/1850 [1:18:35<44:18,  3.96s/it]
 64%|██████▍   | 1180/1850 [1:18:39<43:55,  3.93s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029541015625, 'learning_rate': 3.621621621621622e-05, 'epoch': 31.89}

 64%|██████▍   | 1180/1850 [1:18:39<43:55,  3.93s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 64%|██████▍   | 1181/1850 [1:18:43<42:49,  3.84s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.032470703125, 'learning_rate': 3.616216216216217e-05, 'epoch': 31.92}

 64%|██████▍   | 1181/1850 [1:18:43<42:49,  3.84s/it]
 64%|██████▍   | 1182/1850 [1:18:47<42:56,  3.86s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033935546875, 'learning_rate': 3.6108108108108105e-05, 'epoch': 31.95}

 64%|██████▍   | 1182/1850 [1:18:47<42:56,  3.86s/it]
 64%|██████▍   | 1183/1850 [1:18:51<43:59,  3.96s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03564453125, 'learning_rate': 3.6054054054054056e-05, 'epoch': 31.97}

 64%|██████▍   | 1183/1850 [1:18:51<43:59,  3.96s/it]
 64%|██████▍   | 1184/1850 [1:18:55<44:10,  3.98s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0380859375, 'learning_rate': 3.6e-05, 'epoch': 32.0}

 64%|██████▍   | 1184/1850 [1:18:55<44:10,  3.98s/it]
 64%|██████▍   | 1185/1850 [1:18:59<43:25,  3.92s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0269775390625, 'learning_rate': 3.5945945945945945e-05, 'epoch': 32.03}

 64%|██████▍   | 1185/1850 [1:18:59<43:25,  3.92s/it]
 64%|██████▍   | 1186/1850 [1:19:03<44:37,  4.03s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.032470703125, 'learning_rate': 3.5891891891891897e-05, 'epoch': 32.05}

 64%|██████▍   | 1186/1850 [1:19:03<44:37,  4.03s/it]
 64%|██████▍   | 1187/1850 [1:19:07<44:46,  4.05s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0303955078125, 'learning_rate': 3.583783783783784e-05, 'epoch': 32.08}

 64%|██████▍   | 1187/1850 [1:19:07<44:46,  4.05s/it]
 64%|██████▍   | 1188/1850 [1:19:11<43:53,  3.98s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.030029296875, 'learning_rate': 3.5783783783783785e-05, 'epoch': 32.11}

 64%|██████▍   | 1188/1850 [1:19:11<43:53,  3.98s/it]
 64%|██████▍   | 1189/1850 [1:19:15<43:06,  3.91s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0284423828125, 'learning_rate': 3.572972972972973e-05, 'epoch': 32.14}

 64%|██████▍   | 1189/1850 [1:19:15<43:06,  3.91s/it]
 64%|██████▍   | 1190/1850 [1:19:18<42:25,  3.86s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03515625, 'learning_rate': 3.567567567567568e-05, 'epoch': 32.16}

 64%|██████▍   | 1190/1850 [1:19:18<42:25,  3.86s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 64%|██████▍   | 1191/1850 [1:19:22<42:33,  3.87s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0302734375, 'learning_rate': 3.5621621621621626e-05, 'epoch': 32.19}

 64%|██████▍   | 1191/1850 [1:19:22<42:33,  3.87s/it]
 64%|██████▍   | 1192/1850 [1:19:26<42:55,  3.91s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.032470703125, 'learning_rate': 3.556756756756757e-05, 'epoch': 32.22}

 64%|██████▍   | 1192/1850 [1:19:26<42:55,  3.91s/it]
 64%|██████▍   | 1193/1850 [1:19:30<43:16,  3.95s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.10791015625, 'learning_rate': 3.5513513513513515e-05, 'epoch': 32.24}

 64%|██████▍   | 1193/1850 [1:19:30<43:16,  3.95s/it]
 65%|██████▍   | 1194/1850 [1:19:35<44:39,  4.08s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0322265625, 'learning_rate': 3.545945945945946e-05, 'epoch': 32.27}

 65%|██████▍   | 1194/1850 [1:19:35<44:39,  4.08s/it]
 65%|██████▍   | 1195/1850 [1:19:39<45:40,  4.18s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 3.5405405405405403e-05, 'epoch': 32.3}

 65%|██████▍   | 1195/1850 [1:19:39<45:40,  4.18s/it]
 65%|██████▍   | 1196/1850 [1:19:43<44:51,  4.12s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0289306640625, 'learning_rate': 3.5351351351351355e-05, 'epoch': 32.32}

 65%|██████▍   | 1196/1850 [1:19:43<44:51,  4.12s/it]
 65%|██████▍   | 1197/1850 [1:19:48<46:59,  4.32s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0291748046875, 'learning_rate': 3.52972972972973e-05, 'epoch': 32.35}

 65%|██████▍   | 1197/1850 [1:19:48<46:59,  4.32s/it]
 65%|██████▍   | 1198/1850 [1:19:52<47:03,  4.33s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0303955078125, 'learning_rate': 3.5243243243243244e-05, 'epoch': 32.38}

 65%|██████▍   | 1198/1850 [1:19:52<47:03,  4.33s/it]
 65%|██████▍   | 1199/1850 [1:19:56<44:28,  4.10s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0322265625, 'learning_rate': 3.5189189189189195e-05, 'epoch': 32.41}

 65%|██████▍   | 1199/1850 [1:19:56<44:28,  4.10s/it]
 65%|██████▍   | 1200/1850 [1:20:00<44:09,  4.08s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.03271484375, 'learning_rate': 3.513513513513514e-05, 'epoch': 32.43}

 65%|██████▍   | 1200/1850 [1:20:00<44:09,  4.08s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 65%|██████▍   | 1201/1850 [1:20:04<45:12,  4.18s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0277099609375, 'learning_rate': 3.5081081081081084e-05, 'epoch': 32.46}

 65%|██████▍   | 1201/1850 [1:20:04<45:12,  4.18s/it]
 65%|██████▍   | 1202/1850 [1:20:08<45:02,  4.17s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033447265625, 'learning_rate': 3.502702702702703e-05, 'epoch': 32.49}

 65%|██████▍   | 1202/1850 [1:20:08<45:02,  4.17s/it]
 65%|██████▌   | 1203/1850 [1:20:12<44:55,  4.17s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0299072265625, 'learning_rate': 3.497297297297297e-05, 'epoch': 32.51}

 65%|██████▌   | 1203/1850 [1:20:12<44:55,  4.17s/it]
 65%|██████▌   | 1204/1850 [1:20:16<42:08,  3.91s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03173828125, 'learning_rate': 3.491891891891892e-05, 'epoch': 32.54}

 65%|██████▌   | 1204/1850 [1:20:16<42:08,  3.91s/it]
 65%|██████▌   | 1205/1850 [1:20:19<41:01,  3.82s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.03271484375, 'learning_rate': 3.486486486486487e-05, 'epoch': 32.57}

 65%|██████▌   | 1205/1850 [1:20:19<41:01,  3.82s/it]
 65%|██████▌   | 1206/1850 [1:20:24<42:16,  3.94s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03515625, 'learning_rate': 3.481081081081081e-05, 'epoch': 32.59}

 65%|██████▌   | 1206/1850 [1:20:24<42:16,  3.94s/it]
 65%|██████▌   | 1207/1850 [1:20:28<42:09,  3.93s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0289306640625, 'learning_rate': 3.475675675675676e-05, 'epoch': 32.62}

 65%|██████▌   | 1207/1850 [1:20:28<42:09,  3.93s/it]
 65%|██████▌   | 1208/1850 [1:20:31<40:52,  3.82s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03125, 'learning_rate': 3.470270270270271e-05, 'epoch': 32.65}

 65%|██████▌   | 1208/1850 [1:20:31<40:52,  3.82s/it]
 65%|██████▌   | 1209/1850 [1:20:36<42:49,  4.01s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03271484375, 'learning_rate': 3.464864864864865e-05, 'epoch': 32.68}

 65%|██████▌   | 1209/1850 [1:20:36<42:49,  4.01s/it]
 65%|██████▌   | 1210/1850 [1:20:40<42:43,  4.01s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03076171875, 'learning_rate': 3.45945945945946e-05, 'epoch': 32.7}

 65%|██████▌   | 1210/1850 [1:20:40<42:43,  4.01s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 65%|██████▌   | 1211/1850 [1:20:43<42:32,  3.99s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.027099609375, 'learning_rate': 3.454054054054054e-05, 'epoch': 32.73}

 65%|██████▌   | 1211/1850 [1:20:43<42:32,  3.99s/it]
 66%|██████▌   | 1212/1850 [1:20:48<45:26,  4.27s/it]
                                                     
{'loss': 0.0015, 'grad_norm': 0.03369140625, 'learning_rate': 3.4486486486486486e-05, 'epoch': 32.76}

 66%|██████▌   | 1212/1850 [1:20:48<45:26,  4.27s/it]
 66%|██████▌   | 1213/1850 [1:20:52<42:51,  4.04s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0294189453125, 'learning_rate': 3.443243243243243e-05, 'epoch': 32.78}

 66%|██████▌   | 1213/1850 [1:20:52<42:51,  4.04s/it]
 66%|██████▌   | 1214/1850 [1:20:56<41:55,  3.96s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0286865234375, 'learning_rate': 3.437837837837838e-05, 'epoch': 32.81}

 66%|██████▌   | 1214/1850 [1:20:56<41:55,  3.96s/it]
 66%|██████▌   | 1215/1850 [1:20:59<41:15,  3.90s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.030517578125, 'learning_rate': 3.4324324324324326e-05, 'epoch': 32.84}

 66%|██████▌   | 1215/1850 [1:20:59<41:15,  3.90s/it]
 66%|██████▌   | 1216/1850 [1:21:03<39:35,  3.75s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0296630859375, 'learning_rate': 3.427027027027027e-05, 'epoch': 32.86}

 66%|██████▌   | 1216/1850 [1:21:03<39:35,  3.75s/it]
 66%|██████▌   | 1217/1850 [1:21:07<42:09,  4.00s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.033935546875, 'learning_rate': 3.4216216216216215e-05, 'epoch': 32.89}

 66%|██████▌   | 1217/1850 [1:21:07<42:09,  4.00s/it]
 66%|██████▌   | 1218/1850 [1:21:11<41:40,  3.96s/it]
                                                     
{'loss': 0.0018, 'grad_norm': 0.154296875, 'learning_rate': 3.4162162162162166e-05, 'epoch': 32.92}

 66%|██████▌   | 1218/1850 [1:21:11<41:40,  3.96s/it]
 66%|██████▌   | 1219/1850 [1:21:15<41:15,  3.92s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0283203125, 'learning_rate': 3.410810810810811e-05, 'epoch': 32.95}

 66%|██████▌   | 1219/1850 [1:21:15<41:15,  3.92s/it]
 66%|██████▌   | 1220/1850 [1:21:18<39:24,  3.75s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.026611328125, 'learning_rate': 3.4054054054054055e-05, 'epoch': 32.97}

 66%|██████▌   | 1220/1850 [1:21:18<39:24,  3.75s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 66%|██████▌   | 1221/1850 [1:21:22<39:11,  3.74s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0341796875, 'learning_rate': 3.4000000000000007e-05, 'epoch': 33.0}

 66%|██████▌   | 1221/1850 [1:21:22<39:11,  3.74s/it]
 66%|██████▌   | 1222/1850 [1:21:26<40:18,  3.85s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02978515625, 'learning_rate': 3.3945945945945944e-05, 'epoch': 33.03}

 66%|██████▌   | 1222/1850 [1:21:26<40:18,  3.85s/it]
 66%|██████▌   | 1223/1850 [1:21:30<39:41,  3.80s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.032470703125, 'learning_rate': 3.389189189189189e-05, 'epoch': 33.05}

 66%|██████▌   | 1223/1850 [1:21:30<39:41,  3.80s/it]
 66%|██████▌   | 1224/1850 [1:21:34<41:21,  3.96s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0322265625, 'learning_rate': 3.383783783783784e-05, 'epoch': 33.08}

 66%|██████▌   | 1224/1850 [1:21:34<41:21,  3.96s/it]
 66%|██████▌   | 1225/1850 [1:21:39<42:24,  4.07s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03271484375, 'learning_rate': 3.3783783783783784e-05, 'epoch': 33.11}

 66%|██████▌   | 1225/1850 [1:21:39<42:24,  4.07s/it]
 66%|██████▋   | 1226/1850 [1:21:41<38:22,  3.69s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.028564453125, 'learning_rate': 3.372972972972973e-05, 'epoch': 33.14}

 66%|██████▋   | 1226/1850 [1:21:41<38:22,  3.69s/it]
 66%|██████▋   | 1227/1850 [1:21:45<38:29,  3.71s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0279541015625, 'learning_rate': 3.367567567567568e-05, 'epoch': 33.16}

 66%|██████▋   | 1227/1850 [1:21:45<38:29,  3.71s/it]
 66%|██████▋   | 1228/1850 [1:21:49<39:13,  3.78s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03125, 'learning_rate': 3.3621621621621624e-05, 'epoch': 33.19}

 66%|██████▋   | 1228/1850 [1:21:49<39:13,  3.78s/it]
 66%|██████▋   | 1229/1850 [1:21:53<40:37,  3.93s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03271484375, 'learning_rate': 3.356756756756757e-05, 'epoch': 33.22}

 66%|██████▋   | 1229/1850 [1:21:53<40:37,  3.93s/it]
 66%|██████▋   | 1230/1850 [1:21:58<42:09,  4.08s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.037841796875, 'learning_rate': 3.351351351351352e-05, 'epoch': 33.24}

 66%|██████▋   | 1230/1850 [1:21:58<42:09,  4.08s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 67%|██████▋   | 1231/1850 [1:22:02<42:19,  4.10s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0286865234375, 'learning_rate': 3.3459459459459465e-05, 'epoch': 33.27}

 67%|██████▋   | 1231/1850 [1:22:02<42:19,  4.10s/it]
 67%|██████▋   | 1232/1850 [1:22:06<42:55,  4.17s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 3.34054054054054e-05, 'epoch': 33.3}

 67%|██████▋   | 1232/1850 [1:22:06<42:55,  4.17s/it]
 67%|██████▋   | 1233/1850 [1:22:10<42:29,  4.13s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0257568359375, 'learning_rate': 3.3351351351351353e-05, 'epoch': 33.32}

 67%|██████▋   | 1233/1850 [1:22:10<42:29,  4.13s/it]
 67%|██████▋   | 1234/1850 [1:22:14<41:28,  4.04s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03173828125, 'learning_rate': 3.32972972972973e-05, 'epoch': 33.35}

 67%|██████▋   | 1234/1850 [1:22:14<41:28,  4.04s/it]
 67%|██████▋   | 1235/1850 [1:22:19<42:16,  4.12s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0277099609375, 'learning_rate': 3.324324324324324e-05, 'epoch': 33.38}

 67%|██████▋   | 1235/1850 [1:22:19<42:16,  4.12s/it]
 67%|██████▋   | 1236/1850 [1:22:22<39:45,  3.88s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.031494140625, 'learning_rate': 3.3189189189189194e-05, 'epoch': 33.41}

 67%|██████▋   | 1236/1850 [1:22:22<39:45,  3.88s/it]
 67%|██████▋   | 1237/1850 [1:22:26<40:52,  4.00s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03515625, 'learning_rate': 3.313513513513514e-05, 'epoch': 33.43}

 67%|██████▋   | 1237/1850 [1:22:26<40:52,  4.00s/it]
 67%|██████▋   | 1238/1850 [1:22:30<41:45,  4.09s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033203125, 'learning_rate': 3.308108108108108e-05, 'epoch': 33.46}

 67%|██████▋   | 1238/1850 [1:22:30<41:45,  4.09s/it]
 67%|██████▋   | 1239/1850 [1:22:35<41:58,  4.12s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031982421875, 'learning_rate': 3.302702702702703e-05, 'epoch': 33.49}

 67%|██████▋   | 1239/1850 [1:22:35<41:58,  4.12s/it]
 67%|██████▋   | 1240/1850 [1:22:38<40:27,  3.98s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03125, 'learning_rate': 3.297297297297298e-05, 'epoch': 33.51}

 67%|██████▋   | 1240/1850 [1:22:38<40:27,  3.98s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 67%|██████▋   | 1241/1850 [1:22:42<39:04,  3.85s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033447265625, 'learning_rate': 3.291891891891892e-05, 'epoch': 33.54}

 67%|██████▋   | 1241/1850 [1:22:42<39:04,  3.85s/it]
 67%|██████▋   | 1242/1850 [1:22:46<39:41,  3.92s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03515625, 'learning_rate': 3.286486486486487e-05, 'epoch': 33.57}

 67%|██████▋   | 1242/1850 [1:22:46<39:41,  3.92s/it]
 67%|██████▋   | 1243/1850 [1:22:50<39:40,  3.92s/it]
                                                     
{'loss': 0.0016, 'grad_norm': 0.1787109375, 'learning_rate': 3.281081081081081e-05, 'epoch': 33.59}

 67%|██████▋   | 1243/1850 [1:22:50<39:40,  3.92s/it]
 67%|██████▋   | 1244/1850 [1:22:54<39:27,  3.91s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.030029296875, 'learning_rate': 3.2756756756756756e-05, 'epoch': 33.62}

 67%|██████▋   | 1244/1850 [1:22:54<39:27,  3.91s/it]
 67%|██████▋   | 1245/1850 [1:22:58<40:17,  4.00s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0286865234375, 'learning_rate': 3.27027027027027e-05, 'epoch': 33.65}

 67%|██████▋   | 1245/1850 [1:22:58<40:17,  4.00s/it]
 67%|██████▋   | 1246/1850 [1:23:01<38:16,  3.80s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0283203125, 'learning_rate': 3.264864864864865e-05, 'epoch': 33.68}

 67%|██████▋   | 1246/1850 [1:23:01<38:16,  3.80s/it]
 67%|██████▋   | 1247/1850 [1:23:05<37:11,  3.70s/it]
                                                     
{'loss': 0.0017, 'grad_norm': 0.201171875, 'learning_rate': 3.2594594594594596e-05, 'epoch': 33.7}

 67%|██████▋   | 1247/1850 [1:23:05<37:11,  3.70s/it]
 67%|██████▋   | 1248/1850 [1:23:09<37:32,  3.74s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0322265625, 'learning_rate': 3.254054054054054e-05, 'epoch': 33.73}

 67%|██████▋   | 1248/1850 [1:23:09<37:32,  3.74s/it]
 68%|██████▊   | 1249/1850 [1:23:13<38:34,  3.85s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02978515625, 'learning_rate': 3.248648648648649e-05, 'epoch': 33.76}

 68%|██████▊   | 1249/1850 [1:23:13<38:34,  3.85s/it]
 68%|██████▊   | 1250/1850 [1:23:16<37:06,  3.71s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.027099609375, 'learning_rate': 3.2432432432432436e-05, 'epoch': 33.78}

 68%|██████▊   | 1250/1850 [1:23:16<37:06,  3.71s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 68%|██████▊   | 1251/1850 [1:23:21<39:39,  3.97s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029052734375, 'learning_rate': 3.237837837837838e-05, 'epoch': 33.81}

 68%|██████▊   | 1251/1850 [1:23:21<39:39,  3.97s/it]
 68%|██████▊   | 1252/1850 [1:23:24<37:02,  3.72s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.03076171875, 'learning_rate': 3.2324324324324325e-05, 'epoch': 33.84}

 68%|██████▊   | 1252/1850 [1:23:24<37:02,  3.72s/it]
 68%|██████▊   | 1253/1850 [1:23:28<38:57,  3.92s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.034912109375, 'learning_rate': 3.227027027027027e-05, 'epoch': 33.86}

 68%|██████▊   | 1253/1850 [1:23:28<38:57,  3.92s/it]
 68%|██████▊   | 1254/1850 [1:23:32<38:45,  3.90s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0341796875, 'learning_rate': 3.2216216216216214e-05, 'epoch': 33.89}

 68%|██████▊   | 1254/1850 [1:23:32<38:45,  3.90s/it]
 68%|██████▊   | 1255/1850 [1:23:35<37:23,  3.77s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0245361328125, 'learning_rate': 3.2162162162162165e-05, 'epoch': 33.92}

 68%|██████▊   | 1255/1850 [1:23:35<37:23,  3.77s/it]
 68%|██████▊   | 1256/1850 [1:23:39<37:33,  3.79s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031982421875, 'learning_rate': 3.210810810810811e-05, 'epoch': 33.95}

 68%|██████▊   | 1256/1850 [1:23:39<37:33,  3.79s/it]
 68%|██████▊   | 1257/1850 [1:23:44<39:56,  4.04s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.0341796875, 'learning_rate': 3.2054054054054054e-05, 'epoch': 33.97}

 68%|██████▊   | 1257/1850 [1:23:44<39:56,  4.04s/it]
 68%|██████▊   | 1258/1850 [1:23:48<40:16,  4.08s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0341796875, 'learning_rate': 3.2000000000000005e-05, 'epoch': 34.0}

 68%|██████▊   | 1258/1850 [1:23:48<40:16,  4.08s/it]
 68%|██████▊   | 1259/1850 [1:23:52<38:46,  3.94s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0267333984375, 'learning_rate': 3.194594594594595e-05, 'epoch': 34.03}

 68%|██████▊   | 1259/1850 [1:23:52<38:46,  3.94s/it]
 68%|██████▊   | 1260/1850 [1:23:55<38:09,  3.88s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0306396484375, 'learning_rate': 3.1891891891891894e-05, 'epoch': 34.05}

 68%|██████▊   | 1260/1850 [1:23:55<38:09,  3.88s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 68%|██████▊   | 1261/1850 [1:24:00<40:14,  4.10s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.037353515625, 'learning_rate': 3.183783783783784e-05, 'epoch': 34.08}

 68%|██████▊   | 1261/1850 [1:24:00<40:14,  4.10s/it]
 68%|██████▊   | 1262/1850 [1:24:04<39:27,  4.03s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.032470703125, 'learning_rate': 3.178378378378378e-05, 'epoch': 34.11}

 68%|██████▊   | 1262/1850 [1:24:04<39:27,  4.03s/it]
 68%|██████▊   | 1263/1850 [1:24:09<41:26,  4.24s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0284423828125, 'learning_rate': 3.172972972972973e-05, 'epoch': 34.14}

 68%|██████▊   | 1263/1850 [1:24:09<41:26,  4.24s/it]
 68%|██████▊   | 1264/1850 [1:24:13<41:29,  4.25s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03515625, 'learning_rate': 3.167567567567568e-05, 'epoch': 34.16}

 68%|██████▊   | 1264/1850 [1:24:13<41:29,  4.25s/it]
 68%|██████▊   | 1265/1850 [1:24:17<39:53,  4.09s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03271484375, 'learning_rate': 3.162162162162162e-05, 'epoch': 34.19}

 68%|██████▊   | 1265/1850 [1:24:17<39:53,  4.09s/it]
 68%|██████▊   | 1266/1850 [1:24:20<38:54,  4.00s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.14453125, 'learning_rate': 3.156756756756757e-05, 'epoch': 34.22}

 68%|██████▊   | 1266/1850 [1:24:20<38:54,  4.00s/it]
 68%|██████▊   | 1267/1850 [1:24:24<37:44,  3.88s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029541015625, 'learning_rate': 3.151351351351351e-05, 'epoch': 34.24}

 68%|██████▊   | 1267/1850 [1:24:24<37:44,  3.88s/it]
 69%|██████▊   | 1268/1850 [1:24:28<37:58,  3.92s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.036865234375, 'learning_rate': 3.145945945945946e-05, 'epoch': 34.27}

 69%|██████▊   | 1268/1850 [1:24:28<37:58,  3.92s/it]
 69%|██████▊   | 1269/1850 [1:24:32<38:25,  3.97s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0322265625, 'learning_rate': 3.140540540540541e-05, 'epoch': 34.3}

 69%|██████▊   | 1269/1850 [1:24:32<38:25,  3.97s/it]
 69%|██████▊   | 1270/1850 [1:24:35<36:32,  3.78s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0281982421875, 'learning_rate': 3.135135135135135e-05, 'epoch': 34.32}

 69%|██████▊   | 1270/1850 [1:24:35<36:32,  3.78s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 69%|██████▊   | 1271/1850 [1:24:40<39:38,  4.11s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0341796875, 'learning_rate': 3.1297297297297303e-05, 'epoch': 34.35}

 69%|██████▊   | 1271/1850 [1:24:40<39:38,  4.11s/it]
 69%|██████▉   | 1272/1850 [1:24:45<40:05,  4.16s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 3.124324324324324e-05, 'epoch': 34.38}

 69%|██████▉   | 1272/1850 [1:24:45<40:05,  4.16s/it]
 69%|██████▉   | 1273/1850 [1:24:49<40:42,  4.23s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.033447265625, 'learning_rate': 3.1189189189189186e-05, 'epoch': 34.41}

 69%|██████▉   | 1273/1850 [1:24:49<40:42,  4.23s/it]
 69%|██████▉   | 1274/1850 [1:24:52<37:28,  3.90s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0252685546875, 'learning_rate': 3.113513513513514e-05, 'epoch': 34.43}

 69%|██████▉   | 1274/1850 [1:24:52<37:28,  3.90s/it]
 69%|██████▉   | 1275/1850 [1:24:56<36:23,  3.80s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0264892578125, 'learning_rate': 3.108108108108108e-05, 'epoch': 34.46}

 69%|██████▉   | 1275/1850 [1:24:56<36:23,  3.80s/it]
 69%|██████▉   | 1276/1850 [1:25:00<36:47,  3.85s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0301513671875, 'learning_rate': 3.1027027027027026e-05, 'epoch': 34.49}

 69%|██████▉   | 1276/1850 [1:25:00<36:47,  3.85s/it]
 69%|██████▉   | 1277/1850 [1:25:03<36:25,  3.81s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.026611328125, 'learning_rate': 3.097297297297298e-05, 'epoch': 34.51}

 69%|██████▉   | 1277/1850 [1:25:03<36:25,  3.81s/it]
 69%|██████▉   | 1278/1850 [1:25:07<37:10,  3.90s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0281982421875, 'learning_rate': 3.091891891891892e-05, 'epoch': 34.54}

 69%|██████▉   | 1278/1850 [1:25:07<37:10,  3.90s/it]
 69%|██████▉   | 1279/1850 [1:25:12<37:40,  3.96s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0341796875, 'learning_rate': 3.0864864864864866e-05, 'epoch': 34.57}

 69%|██████▉   | 1279/1850 [1:25:12<37:40,  3.96s/it]
 69%|██████▉   | 1280/1850 [1:25:16<39:28,  4.16s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.034912109375, 'learning_rate': 3.081081081081082e-05, 'epoch': 34.59}

 69%|██████▉   | 1280/1850 [1:25:16<39:28,  4.16s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 69%|██████▉   | 1281/1850 [1:25:20<39:39,  4.18s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0291748046875, 'learning_rate': 3.075675675675676e-05, 'epoch': 34.62}

 69%|██████▉   | 1281/1850 [1:25:20<39:39,  4.18s/it]
 69%|██████▉   | 1282/1850 [1:25:25<40:21,  4.26s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03076171875, 'learning_rate': 3.07027027027027e-05, 'epoch': 34.65}

 69%|██████▉   | 1282/1850 [1:25:25<40:21,  4.26s/it]
 69%|██████▉   | 1283/1850 [1:25:29<40:35,  4.29s/it]
                                                     
{'loss': 0.0018, 'grad_norm': 0.15625, 'learning_rate': 3.064864864864865e-05, 'epoch': 34.68}

 69%|██████▉   | 1283/1850 [1:25:29<40:35,  4.29s/it]
 69%|██████▉   | 1284/1850 [1:25:33<39:07,  4.15s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0296630859375, 'learning_rate': 3.0594594594594595e-05, 'epoch': 34.7}

 69%|██████▉   | 1284/1850 [1:25:33<39:07,  4.15s/it]
 69%|██████▉   | 1285/1850 [1:25:37<39:08,  4.16s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0306396484375, 'learning_rate': 3.054054054054054e-05, 'epoch': 34.73}

 69%|██████▉   | 1285/1850 [1:25:37<39:08,  4.16s/it]
 70%|██████▉   | 1286/1850 [1:25:41<38:04,  4.05s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029296875, 'learning_rate': 3.0486486486486487e-05, 'epoch': 34.76}

 70%|██████▉   | 1286/1850 [1:25:41<38:04,  4.05s/it]
 70%|██████▉   | 1287/1850 [1:25:44<35:06,  3.74s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.034423828125, 'learning_rate': 3.0432432432432435e-05, 'epoch': 34.78}

 70%|██████▉   | 1287/1850 [1:25:44<35:06,  3.74s/it]
 70%|██████▉   | 1288/1850 [1:25:48<35:49,  3.82s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029296875, 'learning_rate': 3.037837837837838e-05, 'epoch': 34.81}

 70%|██████▉   | 1288/1850 [1:25:48<35:49,  3.82s/it]
 70%|██████▉   | 1289/1850 [1:25:53<38:08,  4.08s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.035400390625, 'learning_rate': 3.0324324324324327e-05, 'epoch': 34.84}

 70%|██████▉   | 1289/1850 [1:25:53<38:08,  4.08s/it]
 70%|██████▉   | 1290/1850 [1:25:57<38:16,  4.10s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.026611328125, 'learning_rate': 3.0270270270270272e-05, 'epoch': 34.86}

 70%|██████▉   | 1290/1850 [1:25:57<38:16,  4.10s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 70%|██████▉   | 1291/1850 [1:26:01<39:31,  4.24s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0301513671875, 'learning_rate': 3.021621621621622e-05, 'epoch': 34.89}

 70%|██████▉   | 1291/1850 [1:26:01<39:31,  4.24s/it]
 70%|██████▉   | 1292/1850 [1:26:05<36:51,  3.96s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0341796875, 'learning_rate': 3.016216216216216e-05, 'epoch': 34.92}

 70%|██████▉   | 1292/1850 [1:26:05<36:51,  3.96s/it]
 70%|██████▉   | 1293/1850 [1:26:09<36:12,  3.90s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.029541015625, 'learning_rate': 3.010810810810811e-05, 'epoch': 34.95}

 70%|██████▉   | 1293/1850 [1:26:09<36:12,  3.90s/it]
 70%|██████▉   | 1294/1850 [1:26:12<35:59,  3.88s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0284423828125, 'learning_rate': 3.0054054054054053e-05, 'epoch': 34.97}

 70%|██████▉   | 1294/1850 [1:26:12<35:59,  3.88s/it]
 70%|███████   | 1295/1850 [1:26:16<35:46,  3.87s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0283203125, 'learning_rate': 3e-05, 'epoch': 35.0}

 70%|███████   | 1295/1850 [1:26:16<35:46,  3.87s/it]
 70%|███████   | 1296/1850 [1:26:20<36:42,  3.98s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 2.9945945945945945e-05, 'epoch': 35.03}

 70%|███████   | 1296/1850 [1:26:20<36:42,  3.98s/it]
 70%|███████   | 1297/1850 [1:26:24<35:30,  3.85s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029296875, 'learning_rate': 2.9891891891891893e-05, 'epoch': 35.05}

 70%|███████   | 1297/1850 [1:26:24<35:30,  3.85s/it]
 70%|███████   | 1298/1850 [1:26:28<35:30,  3.86s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.027099609375, 'learning_rate': 2.983783783783784e-05, 'epoch': 35.08}

 70%|███████   | 1298/1850 [1:26:28<35:30,  3.86s/it]
 70%|███████   | 1299/1850 [1:26:32<35:30,  3.87s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0284423828125, 'learning_rate': 2.9783783783783785e-05, 'epoch': 35.11}

 70%|███████   | 1299/1850 [1:26:32<35:30,  3.87s/it]
 70%|███████   | 1300/1850 [1:26:36<35:40,  3.89s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0299072265625, 'learning_rate': 2.9729729729729733e-05, 'epoch': 35.14}

 70%|███████   | 1300/1850 [1:26:36<35:40,  3.89s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 70%|███████   | 1301/1850 [1:26:40<36:01,  3.94s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0283203125, 'learning_rate': 2.9675675675675678e-05, 'epoch': 35.16}

 70%|███████   | 1301/1850 [1:26:40<36:01,  3.94s/it]
 70%|███████   | 1302/1850 [1:26:43<34:32,  3.78s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0296630859375, 'learning_rate': 2.9621621621621625e-05, 'epoch': 35.19}

 70%|███████   | 1302/1850 [1:26:43<34:32,  3.78s/it]
 70%|███████   | 1303/1850 [1:26:48<36:41,  4.02s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033447265625, 'learning_rate': 2.9567567567567567e-05, 'epoch': 35.22}

 70%|███████   | 1303/1850 [1:26:48<36:41,  4.02s/it]
 70%|███████   | 1304/1850 [1:26:52<36:24,  4.00s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.032470703125, 'learning_rate': 2.9513513513513514e-05, 'epoch': 35.24}

 70%|███████   | 1304/1850 [1:26:52<36:24,  4.00s/it]
 71%|███████   | 1305/1850 [1:26:55<35:09,  3.87s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0274658203125, 'learning_rate': 2.945945945945946e-05, 'epoch': 35.27}

 71%|███████   | 1305/1850 [1:26:55<35:09,  3.87s/it]
 71%|███████   | 1306/1850 [1:26:59<34:55,  3.85s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0303955078125, 'learning_rate': 2.9405405405405407e-05, 'epoch': 35.3}

 71%|███████   | 1306/1850 [1:26:59<34:55,  3.85s/it]
 71%|███████   | 1307/1850 [1:27:03<35:26,  3.92s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029052734375, 'learning_rate': 2.935135135135135e-05, 'epoch': 35.32}

 71%|███████   | 1307/1850 [1:27:03<35:26,  3.92s/it]
 71%|███████   | 1308/1850 [1:27:07<33:52,  3.75s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.024658203125, 'learning_rate': 2.92972972972973e-05, 'epoch': 35.35}

 71%|███████   | 1308/1850 [1:27:07<33:52,  3.75s/it]
 71%|███████   | 1309/1850 [1:27:11<35:10,  3.90s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0283203125, 'learning_rate': 2.9243243243243247e-05, 'epoch': 35.38}

 71%|███████   | 1309/1850 [1:27:11<35:10,  3.90s/it]
 71%|███████   | 1310/1850 [1:27:16<38:23,  4.26s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.03466796875, 'learning_rate': 2.918918918918919e-05, 'epoch': 35.41}

 71%|███████   | 1310/1850 [1:27:16<38:23,  4.26s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 71%|███████   | 1311/1850 [1:27:20<38:28,  4.28s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.032470703125, 'learning_rate': 2.913513513513514e-05, 'epoch': 35.43}

 71%|███████   | 1311/1850 [1:27:20<38:28,  4.28s/it]
 71%|███████   | 1312/1850 [1:27:25<39:01,  4.35s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03125, 'learning_rate': 2.9081081081081087e-05, 'epoch': 35.46}

 71%|███████   | 1312/1850 [1:27:25<39:01,  4.35s/it]
 71%|███████   | 1313/1850 [1:27:29<38:43,  4.33s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03271484375, 'learning_rate': 2.9027027027027025e-05, 'epoch': 35.49}

 71%|███████   | 1313/1850 [1:27:29<38:43,  4.33s/it]
 71%|███████   | 1314/1850 [1:27:33<38:29,  4.31s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029052734375, 'learning_rate': 2.8972972972972972e-05, 'epoch': 35.51}

 71%|███████   | 1314/1850 [1:27:33<38:29,  4.31s/it]
 71%|███████   | 1315/1850 [1:27:38<38:21,  4.30s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03369140625, 'learning_rate': 2.891891891891892e-05, 'epoch': 35.54}

 71%|███████   | 1315/1850 [1:27:38<38:21,  4.30s/it]
 71%|███████   | 1316/1850 [1:27:42<37:29,  4.21s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.033203125, 'learning_rate': 2.8864864864864865e-05, 'epoch': 35.57}

 71%|███████   | 1316/1850 [1:27:42<37:29,  4.21s/it]
 71%|███████   | 1317/1850 [1:27:46<37:15,  4.19s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0322265625, 'learning_rate': 2.8810810810810813e-05, 'epoch': 35.59}

 71%|███████   | 1317/1850 [1:27:46<37:15,  4.19s/it]
 71%|███████   | 1318/1850 [1:27:49<35:46,  4.03s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0279541015625, 'learning_rate': 2.8756756756756757e-05, 'epoch': 35.62}

 71%|███████   | 1318/1850 [1:27:49<35:46,  4.03s/it]
 71%|███████▏  | 1319/1850 [1:27:53<34:41,  3.92s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.029541015625, 'learning_rate': 2.8702702702702705e-05, 'epoch': 35.65}

 71%|███████▏  | 1319/1850 [1:27:53<34:41,  3.92s/it]
 71%|███████▏  | 1320/1850 [1:27:57<35:22,  4.01s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.034423828125, 'learning_rate': 2.8648648648648653e-05, 'epoch': 35.68}

 71%|███████▏  | 1320/1850 [1:27:57<35:22,  4.01s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 71%|███████▏  | 1321/1850 [1:28:02<36:16,  4.11s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 2.8594594594594597e-05, 'epoch': 35.7}

 71%|███████▏  | 1321/1850 [1:28:02<36:16,  4.11s/it]
 71%|███████▏  | 1322/1850 [1:28:05<35:15,  4.01s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.1142578125, 'learning_rate': 2.8540540540540545e-05, 'epoch': 35.73}

 71%|███████▏  | 1322/1850 [1:28:05<35:15,  4.01s/it]
 72%|███████▏  | 1323/1850 [1:28:09<35:13,  4.01s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03125, 'learning_rate': 2.8486486486486486e-05, 'epoch': 35.76}

 72%|███████▏  | 1323/1850 [1:28:09<35:13,  4.01s/it]
 72%|███████▏  | 1324/1850 [1:28:13<33:59,  3.88s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031982421875, 'learning_rate': 2.843243243243243e-05, 'epoch': 35.78}

 72%|███████▏  | 1324/1850 [1:28:13<33:59,  3.88s/it]
 72%|███████▏  | 1325/1850 [1:28:17<33:20,  3.81s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0281982421875, 'learning_rate': 2.8378378378378378e-05, 'epoch': 35.81}

 72%|███████▏  | 1325/1850 [1:28:17<33:20,  3.81s/it]
 72%|███████▏  | 1326/1850 [1:28:21<33:51,  3.88s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0291748046875, 'learning_rate': 2.8324324324324326e-05, 'epoch': 35.84}

 72%|███████▏  | 1326/1850 [1:28:21<33:51,  3.88s/it]
 72%|███████▏  | 1327/1850 [1:28:26<36:33,  4.19s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.032958984375, 'learning_rate': 2.827027027027027e-05, 'epoch': 35.86}

 72%|███████▏  | 1327/1850 [1:28:26<36:33,  4.19s/it]
 72%|███████▏  | 1328/1850 [1:28:29<35:32,  4.08s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.030517578125, 'learning_rate': 2.821621621621622e-05, 'epoch': 35.89}

 72%|███████▏  | 1328/1850 [1:28:29<35:32,  4.08s/it]
 72%|███████▏  | 1329/1850 [1:28:33<33:56,  3.91s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0277099609375, 'learning_rate': 2.8162162162162163e-05, 'epoch': 35.92}

 72%|███████▏  | 1329/1850 [1:28:33<33:56,  3.91s/it]
 72%|███████▏  | 1330/1850 [1:28:37<33:42,  3.89s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.09814453125, 'learning_rate': 2.810810810810811e-05, 'epoch': 35.95}

 72%|███████▏  | 1330/1850 [1:28:37<33:42,  3.89s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 72%|███████▏  | 1331/1850 [1:28:41<34:02,  3.94s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0306396484375, 'learning_rate': 2.805405405405406e-05, 'epoch': 35.97}

 72%|███████▏  | 1331/1850 [1:28:41<34:02,  3.94s/it]
 72%|███████▏  | 1332/1850 [1:28:45<34:00,  3.94s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03125, 'learning_rate': 2.8000000000000003e-05, 'epoch': 36.0}

 72%|███████▏  | 1332/1850 [1:28:45<34:00,  3.94s/it]
 72%|███████▏  | 1333/1850 [1:28:48<32:58,  3.83s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0299072265625, 'learning_rate': 2.7945945945945944e-05, 'epoch': 36.03}

 72%|███████▏  | 1333/1850 [1:28:48<32:58,  3.83s/it]
 72%|███████▏  | 1334/1850 [1:28:53<34:17,  3.99s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.032470703125, 'learning_rate': 2.7891891891891892e-05, 'epoch': 36.05}

 72%|███████▏  | 1334/1850 [1:28:53<34:17,  3.99s/it]
 72%|███████▏  | 1335/1850 [1:28:57<34:06,  3.97s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.030029296875, 'learning_rate': 2.7837837837837836e-05, 'epoch': 36.08}

 72%|███████▏  | 1335/1850 [1:28:57<34:06,  3.97s/it]
 72%|███████▏  | 1336/1850 [1:29:00<33:40,  3.93s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0279541015625, 'learning_rate': 2.7783783783783784e-05, 'epoch': 36.11}

 72%|███████▏  | 1336/1850 [1:29:00<33:40,  3.93s/it]
 72%|███████▏  | 1337/1850 [1:29:04<33:08,  3.88s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0291748046875, 'learning_rate': 2.7729729729729732e-05, 'epoch': 36.14}

 72%|███████▏  | 1337/1850 [1:29:04<33:08,  3.88s/it]
 72%|███████▏  | 1338/1850 [1:29:08<33:47,  3.96s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0303955078125, 'learning_rate': 2.7675675675675676e-05, 'epoch': 36.16}

 72%|███████▏  | 1338/1850 [1:29:08<33:47,  3.96s/it]
 72%|███████▏  | 1339/1850 [1:29:12<34:01,  3.99s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0311279296875, 'learning_rate': 2.7621621621621624e-05, 'epoch': 36.19}

 72%|███████▏  | 1339/1850 [1:29:12<34:01,  3.99s/it]
 72%|███████▏  | 1340/1850 [1:29:17<34:54,  4.11s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03271484375, 'learning_rate': 2.7567567567567572e-05, 'epoch': 36.22}

 72%|███████▏  | 1340/1850 [1:29:17<34:54,  4.11s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 72%|███████▏  | 1341/1850 [1:29:21<34:57,  4.12s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.02978515625, 'learning_rate': 2.7513513513513517e-05, 'epoch': 36.24}

 72%|███████▏  | 1341/1850 [1:29:21<34:57,  4.12s/it]
 73%|███████▎  | 1342/1850 [1:29:25<34:37,  4.09s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0296630859375, 'learning_rate': 2.7459459459459464e-05, 'epoch': 36.27}

 73%|███████▎  | 1342/1850 [1:29:25<34:37,  4.09s/it]
 73%|███████▎  | 1343/1850 [1:29:29<33:57,  4.02s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03076171875, 'learning_rate': 2.7405405405405405e-05, 'epoch': 36.3}

 73%|███████▎  | 1343/1850 [1:29:29<33:57,  4.02s/it]
 73%|███████▎  | 1344/1850 [1:29:33<34:26,  4.08s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0303955078125, 'learning_rate': 2.735135135135135e-05, 'epoch': 36.32}

 73%|███████▎  | 1344/1850 [1:29:33<34:26,  4.08s/it]
 73%|███████▎  | 1345/1850 [1:29:38<37:13,  4.42s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.035400390625, 'learning_rate': 2.7297297297297298e-05, 'epoch': 36.35}

 73%|███████▎  | 1345/1850 [1:29:38<37:13,  4.42s/it]
 73%|███████▎  | 1346/1850 [1:29:42<36:34,  4.35s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0272216796875, 'learning_rate': 2.7243243243243242e-05, 'epoch': 36.38}

 73%|███████▎  | 1346/1850 [1:29:42<36:34,  4.35s/it]
 73%|███████▎  | 1347/1850 [1:29:47<35:50,  4.28s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03662109375, 'learning_rate': 2.718918918918919e-05, 'epoch': 36.41}

 73%|███████▎  | 1347/1850 [1:29:47<35:50,  4.28s/it]
 73%|███████▎  | 1348/1850 [1:29:51<36:15,  4.33s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.033935546875, 'learning_rate': 2.7135135135135138e-05, 'epoch': 36.43}

 73%|███████▎  | 1348/1850 [1:29:51<36:15,  4.33s/it]
 73%|███████▎  | 1349/1850 [1:29:55<35:52,  4.30s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031494140625, 'learning_rate': 2.7081081081081082e-05, 'epoch': 36.46}

 73%|███████▎  | 1349/1850 [1:29:55<35:52,  4.30s/it]
 73%|███████▎  | 1350/1850 [1:29:59<34:06,  4.09s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033447265625, 'learning_rate': 2.702702702702703e-05, 'epoch': 36.49}

 73%|███████▎  | 1350/1850 [1:29:59<34:06,  4.09s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 73%|███████▎  | 1351/1850 [1:30:03<34:27,  4.14s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0296630859375, 'learning_rate': 2.6972972972972978e-05, 'epoch': 36.51}

 73%|███████▎  | 1351/1850 [1:30:03<34:27,  4.14s/it]
 73%|███████▎  | 1352/1850 [1:30:07<34:31,  4.16s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029052734375, 'learning_rate': 2.6918918918918922e-05, 'epoch': 36.54}

 73%|███████▎  | 1352/1850 [1:30:07<34:31,  4.16s/it]
 73%|███████▎  | 1353/1850 [1:30:10<31:54,  3.85s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0284423828125, 'learning_rate': 2.6864864864864864e-05, 'epoch': 36.57}

 73%|███████▎  | 1353/1850 [1:30:10<31:54,  3.85s/it]
 73%|███████▎  | 1354/1850 [1:30:14<30:41,  3.71s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0213623046875, 'learning_rate': 2.681081081081081e-05, 'epoch': 36.59}

 73%|███████▎  | 1354/1850 [1:30:14<30:41,  3.71s/it]
 73%|███████▎  | 1355/1850 [1:30:18<30:38,  3.71s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02978515625, 'learning_rate': 2.6756756756756756e-05, 'epoch': 36.62}

 73%|███████▎  | 1355/1850 [1:30:18<30:38,  3.71s/it]
 73%|███████▎  | 1356/1850 [1:30:21<30:27,  3.70s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.130859375, 'learning_rate': 2.6702702702702704e-05, 'epoch': 36.65}

 73%|███████▎  | 1356/1850 [1:30:21<30:27,  3.70s/it]
 73%|███████▎  | 1357/1850 [1:30:26<32:53,  4.00s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 2.6648648648648648e-05, 'epoch': 36.68}

 73%|███████▎  | 1357/1850 [1:30:26<32:53,  4.00s/it]
 73%|███████▎  | 1358/1850 [1:30:30<33:26,  4.08s/it]
                                                     
{'loss': 0.0016, 'grad_norm': 0.154296875, 'learning_rate': 2.6594594594594596e-05, 'epoch': 36.7}

 73%|███████▎  | 1358/1850 [1:30:30<33:26,  4.08s/it]
 73%|███████▎  | 1359/1850 [1:30:34<31:51,  3.89s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0262451171875, 'learning_rate': 2.6540540540540544e-05, 'epoch': 36.73}

 73%|███████▎  | 1359/1850 [1:30:34<31:51,  3.89s/it]
 74%|███████▎  | 1360/1850 [1:30:37<31:42,  3.88s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03173828125, 'learning_rate': 2.6486486486486488e-05, 'epoch': 36.76}

 74%|███████▎  | 1360/1850 [1:30:37<31:42,  3.88s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 74%|███████▎  | 1361/1850 [1:30:41<31:51,  3.91s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033203125, 'learning_rate': 2.6432432432432436e-05, 'epoch': 36.78}

 74%|███████▎  | 1361/1850 [1:30:41<31:51,  3.91s/it]
 74%|███████▎  | 1362/1850 [1:30:45<30:33,  3.76s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0281982421875, 'learning_rate': 2.6378378378378384e-05, 'epoch': 36.81}

 74%|███████▎  | 1362/1850 [1:30:45<30:33,  3.76s/it]
 74%|███████▎  | 1363/1850 [1:30:49<30:47,  3.79s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0306396484375, 'learning_rate': 2.632432432432432e-05, 'epoch': 36.84}

 74%|███████▎  | 1363/1850 [1:30:49<30:47,  3.79s/it]
 74%|███████▎  | 1364/1850 [1:30:53<30:45,  3.80s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0286865234375, 'learning_rate': 2.627027027027027e-05, 'epoch': 36.86}

 74%|███████▎  | 1364/1850 [1:30:53<30:45,  3.80s/it]
 74%|███████▍  | 1365/1850 [1:30:57<31:10,  3.86s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0289306640625, 'learning_rate': 2.6216216216216217e-05, 'epoch': 36.89}

 74%|███████▍  | 1365/1850 [1:30:57<31:10,  3.86s/it]
 74%|███████▍  | 1366/1850 [1:31:01<32:32,  4.03s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03173828125, 'learning_rate': 2.616216216216216e-05, 'epoch': 36.92}

 74%|███████▍  | 1366/1850 [1:31:01<32:32,  4.03s/it]
 74%|███████▍  | 1367/1850 [1:31:05<32:18,  4.01s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.036376953125, 'learning_rate': 2.610810810810811e-05, 'epoch': 36.95}

 74%|███████▍  | 1367/1850 [1:31:05<32:18,  4.01s/it]
 74%|███████▍  | 1368/1850 [1:31:10<34:13,  4.26s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.035400390625, 'learning_rate': 2.6054054054054054e-05, 'epoch': 36.97}

 74%|███████▍  | 1368/1850 [1:31:10<34:13,  4.26s/it]
 74%|███████▍  | 1369/1850 [1:31:13<31:18,  3.91s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029541015625, 'learning_rate': 2.6000000000000002e-05, 'epoch': 37.0}

 74%|███████▍  | 1369/1850 [1:31:13<31:18,  3.91s/it]
 74%|███████▍  | 1370/1850 [1:31:16<30:03,  3.76s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0301513671875, 'learning_rate': 2.594594594594595e-05, 'epoch': 37.03}

 74%|███████▍  | 1370/1850 [1:31:16<30:03,  3.76s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 74%|███████▍  | 1371/1850 [1:31:20<29:56,  3.75s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0284423828125, 'learning_rate': 2.5891891891891894e-05, 'epoch': 37.05}

 74%|███████▍  | 1371/1850 [1:31:20<29:56,  3.75s/it]
 74%|███████▍  | 1372/1850 [1:31:24<29:51,  3.75s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0294189453125, 'learning_rate': 2.5837837837837842e-05, 'epoch': 37.08}

 74%|███████▍  | 1372/1850 [1:31:24<29:51,  3.75s/it]
 74%|███████▍  | 1373/1850 [1:31:28<30:06,  3.79s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.02392578125, 'learning_rate': 2.5783783783783783e-05, 'epoch': 37.11}

 74%|███████▍  | 1373/1850 [1:31:28<30:06,  3.79s/it]
 74%|███████▍  | 1374/1850 [1:31:32<30:21,  3.83s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0244140625, 'learning_rate': 2.5729729729729727e-05, 'epoch': 37.14}

 74%|███████▍  | 1374/1850 [1:31:32<30:21,  3.83s/it]
 74%|███████▍  | 1375/1850 [1:31:36<30:41,  3.88s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03125, 'learning_rate': 2.5675675675675675e-05, 'epoch': 37.16}

 74%|███████▍  | 1375/1850 [1:31:36<30:41,  3.88s/it]
 74%|███████▍  | 1376/1850 [1:31:39<30:45,  3.89s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029052734375, 'learning_rate': 2.5621621621621623e-05, 'epoch': 37.19}

 74%|███████▍  | 1376/1850 [1:31:39<30:45,  3.89s/it]
 74%|███████▍  | 1377/1850 [1:31:44<31:57,  4.05s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0303955078125, 'learning_rate': 2.5567567567567568e-05, 'epoch': 37.22}

 74%|███████▍  | 1377/1850 [1:31:44<31:57,  4.05s/it]
 74%|███████▍  | 1378/1850 [1:31:48<31:07,  3.96s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02978515625, 'learning_rate': 2.5513513513513515e-05, 'epoch': 37.24}

 74%|███████▍  | 1378/1850 [1:31:48<31:07,  3.96s/it]
 75%|███████▍  | 1379/1850 [1:31:52<31:27,  4.01s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03125, 'learning_rate': 2.5459459459459463e-05, 'epoch': 37.27}

 75%|███████▍  | 1379/1850 [1:31:52<31:27,  4.01s/it]
 75%|███████▍  | 1380/1850 [1:31:56<31:54,  4.07s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.11962890625, 'learning_rate': 2.5405405405405408e-05, 'epoch': 37.3}

 75%|███████▍  | 1380/1850 [1:31:56<31:54,  4.07s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 75%|███████▍  | 1381/1850 [1:32:00<31:29,  4.03s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03173828125, 'learning_rate': 2.5351351351351356e-05, 'epoch': 37.32}

 75%|███████▍  | 1381/1850 [1:32:00<31:29,  4.03s/it]
 75%|███████▍  | 1382/1850 [1:32:04<31:48,  4.08s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.032958984375, 'learning_rate': 2.52972972972973e-05, 'epoch': 37.35}

 75%|███████▍  | 1382/1850 [1:32:04<31:48,  4.08s/it]
 75%|███████▍  | 1383/1850 [1:32:08<30:37,  3.93s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0242919921875, 'learning_rate': 2.524324324324324e-05, 'epoch': 37.38}

 75%|███████▍  | 1383/1850 [1:32:08<30:37,  3.93s/it]
 75%|███████▍  | 1384/1850 [1:32:12<30:55,  3.98s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0281982421875, 'learning_rate': 2.518918918918919e-05, 'epoch': 37.41}

 75%|███████▍  | 1384/1850 [1:32:12<30:55,  3.98s/it]
 75%|███████▍  | 1385/1850 [1:32:16<30:36,  3.95s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0322265625, 'learning_rate': 2.5135135135135133e-05, 'epoch': 37.43}

 75%|███████▍  | 1385/1850 [1:32:16<30:36,  3.95s/it]
 75%|███████▍  | 1386/1850 [1:32:19<30:03,  3.89s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.025390625, 'learning_rate': 2.508108108108108e-05, 'epoch': 37.46}

 75%|███████▍  | 1386/1850 [1:32:19<30:03,  3.89s/it]
 75%|███████▍  | 1387/1850 [1:32:23<29:14,  3.79s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03125, 'learning_rate': 2.502702702702703e-05, 'epoch': 37.49}

 75%|███████▍  | 1387/1850 [1:32:23<29:14,  3.79s/it]
 75%|███████▌  | 1388/1850 [1:32:27<29:05,  3.78s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.029296875, 'learning_rate': 2.4972972972972973e-05, 'epoch': 37.51}

 75%|███████▌  | 1388/1850 [1:32:27<29:05,  3.78s/it]
 75%|███████▌  | 1389/1850 [1:32:31<29:40,  3.86s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0299072265625, 'learning_rate': 2.491891891891892e-05, 'epoch': 37.54}

 75%|███████▌  | 1389/1850 [1:32:31<29:40,  3.86s/it]
 75%|███████▌  | 1390/1850 [1:32:35<30:05,  3.92s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0311279296875, 'learning_rate': 2.486486486486487e-05, 'epoch': 37.57}

 75%|███████▌  | 1390/1850 [1:32:35<30:05,  3.92s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 75%|███████▌  | 1391/1850 [1:32:39<29:47,  3.90s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0283203125, 'learning_rate': 2.481081081081081e-05, 'epoch': 37.59}

 75%|███████▌  | 1391/1850 [1:32:39<29:47,  3.90s/it]
 75%|███████▌  | 1392/1850 [1:32:43<29:49,  3.91s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033447265625, 'learning_rate': 2.4756756756756758e-05, 'epoch': 37.62}

 75%|███████▌  | 1392/1850 [1:32:43<29:49,  3.91s/it]
 75%|███████▌  | 1393/1850 [1:32:46<28:59,  3.81s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.029296875, 'learning_rate': 2.4702702702702706e-05, 'epoch': 37.65}

 75%|███████▌  | 1393/1850 [1:32:46<28:59,  3.81s/it]
 75%|███████▌  | 1394/1850 [1:32:51<30:12,  3.97s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02783203125, 'learning_rate': 2.464864864864865e-05, 'epoch': 37.68}

 75%|███████▌  | 1394/1850 [1:32:51<30:12,  3.97s/it]
 75%|███████▌  | 1395/1850 [1:32:55<31:04,  4.10s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033203125, 'learning_rate': 2.4594594594594598e-05, 'epoch': 37.7}

 75%|███████▌  | 1395/1850 [1:32:55<31:04,  4.10s/it]
 75%|███████▌  | 1396/1850 [1:32:59<30:46,  4.07s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.034423828125, 'learning_rate': 2.454054054054054e-05, 'epoch': 37.73}

 75%|███████▌  | 1396/1850 [1:32:59<30:46,  4.07s/it]
 76%|███████▌  | 1397/1850 [1:33:03<31:25,  4.16s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0263671875, 'learning_rate': 2.4486486486486487e-05, 'epoch': 37.76}

 76%|███████▌  | 1397/1850 [1:33:03<31:25,  4.16s/it]
 76%|███████▌  | 1398/1850 [1:33:07<30:45,  4.08s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0311279296875, 'learning_rate': 2.4432432432432435e-05, 'epoch': 37.78}

 76%|███████▌  | 1398/1850 [1:33:07<30:45,  4.08s/it]
 76%|███████▌  | 1399/1850 [1:33:11<30:48,  4.10s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033447265625, 'learning_rate': 2.437837837837838e-05, 'epoch': 37.81}

 76%|███████▌  | 1399/1850 [1:33:11<30:48,  4.10s/it]
 76%|███████▌  | 1400/1850 [1:33:15<29:38,  3.95s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0281982421875, 'learning_rate': 2.4324324324324327e-05, 'epoch': 37.84}

 76%|███████▌  | 1400/1850 [1:33:15<29:38,  3.95s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 76%|███████▌  | 1401/1850 [1:33:19<30:42,  4.10s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03173828125, 'learning_rate': 2.427027027027027e-05, 'epoch': 37.86}

 76%|███████▌  | 1401/1850 [1:33:19<30:42,  4.10s/it]
 76%|███████▌  | 1402/1850 [1:33:23<29:47,  3.99s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0294189453125, 'learning_rate': 2.4216216216216216e-05, 'epoch': 37.89}

 76%|███████▌  | 1402/1850 [1:33:23<29:47,  3.99s/it]
 76%|███████▌  | 1403/1850 [1:33:27<29:26,  3.95s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.1083984375, 'learning_rate': 2.4162162162162164e-05, 'epoch': 37.92}

 76%|███████▌  | 1403/1850 [1:33:27<29:26,  3.95s/it]
 76%|███████▌  | 1404/1850 [1:33:31<29:56,  4.03s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0322265625, 'learning_rate': 2.4108108108108112e-05, 'epoch': 37.95}

 76%|███████▌  | 1404/1850 [1:33:31<29:56,  4.03s/it]
 76%|███████▌  | 1405/1850 [1:33:37<33:34,  4.53s/it]
                                                     
{'loss': 0.0016, 'grad_norm': 0.035888671875, 'learning_rate': 2.4054054054054056e-05, 'epoch': 37.97}

 76%|███████▌  | 1405/1850 [1:33:37<33:34,  4.53s/it]
 76%|███████▌  | 1406/1850 [1:33:41<31:33,  4.27s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 2.4e-05, 'epoch': 38.0}

 76%|███████▌  | 1406/1850 [1:33:41<31:33,  4.27s/it]
 76%|███████▌  | 1407/1850 [1:33:44<30:16,  4.10s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0274658203125, 'learning_rate': 2.394594594594595e-05, 'epoch': 38.03}

 76%|███████▌  | 1407/1850 [1:33:44<30:16,  4.10s/it]
 76%|███████▌  | 1408/1850 [1:33:48<29:38,  4.02s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.027587890625, 'learning_rate': 2.3891891891891893e-05, 'epoch': 38.05}

 76%|███████▌  | 1408/1850 [1:33:48<29:38,  4.02s/it]
 76%|███████▌  | 1409/1850 [1:33:52<28:46,  3.91s/it]
                                                     
{'loss': 0.0015, 'grad_norm': 0.1953125, 'learning_rate': 2.383783783783784e-05, 'epoch': 38.08}

 76%|███████▌  | 1409/1850 [1:33:52<28:46,  3.91s/it]
 76%|███████▌  | 1410/1850 [1:33:55<28:06,  3.83s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02880859375, 'learning_rate': 2.3783783783783785e-05, 'epoch': 38.11}

 76%|███████▌  | 1410/1850 [1:33:55<28:06,  3.83s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 76%|███████▋  | 1411/1850 [1:34:00<29:46,  4.07s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0272216796875, 'learning_rate': 2.372972972972973e-05, 'epoch': 38.14}

 76%|███████▋  | 1411/1850 [1:34:00<29:46,  4.07s/it]
 76%|███████▋  | 1412/1850 [1:34:04<30:20,  4.16s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.033935546875, 'learning_rate': 2.3675675675675677e-05, 'epoch': 38.16}

 76%|███████▋  | 1412/1850 [1:34:04<30:20,  4.16s/it]
 76%|███████▋  | 1413/1850 [1:34:10<32:21,  4.44s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.0361328125, 'learning_rate': 2.3621621621621622e-05, 'epoch': 38.19}

 76%|███████▋  | 1413/1850 [1:34:10<32:21,  4.44s/it]
 76%|███████▋  | 1414/1850 [1:34:14<31:23,  4.32s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0257568359375, 'learning_rate': 2.356756756756757e-05, 'epoch': 38.22}

 76%|███████▋  | 1414/1850 [1:34:14<31:23,  4.32s/it]
 76%|███████▋  | 1415/1850 [1:34:17<29:15,  4.03s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.034912109375, 'learning_rate': 2.3513513513513518e-05, 'epoch': 38.24}

 76%|███████▋  | 1415/1850 [1:34:17<29:15,  4.03s/it]
 77%|███████▋  | 1416/1850 [1:34:21<29:22,  4.06s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0306396484375, 'learning_rate': 2.345945945945946e-05, 'epoch': 38.27}

 77%|███████▋  | 1416/1850 [1:34:21<29:22,  4.06s/it]
 77%|███████▋  | 1417/1850 [1:34:25<28:36,  3.96s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0272216796875, 'learning_rate': 2.3405405405405406e-05, 'epoch': 38.3}

 77%|███████▋  | 1417/1850 [1:34:25<28:36,  3.96s/it]
 77%|███████▋  | 1418/1850 [1:34:29<28:17,  3.93s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0303955078125, 'learning_rate': 2.3351351351351354e-05, 'epoch': 38.32}

 77%|███████▋  | 1418/1850 [1:34:29<28:17,  3.93s/it]
 77%|███████▋  | 1419/1850 [1:34:33<28:56,  4.03s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 2.32972972972973e-05, 'epoch': 38.35}

 77%|███████▋  | 1419/1850 [1:34:33<28:56,  4.03s/it]
 77%|███████▋  | 1420/1850 [1:34:37<29:15,  4.08s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0277099609375, 'learning_rate': 2.3243243243243247e-05, 'epoch': 38.38}

 77%|███████▋  | 1420/1850 [1:34:37<29:15,  4.08s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 77%|███████▋  | 1421/1850 [1:34:42<31:12,  4.37s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0299072265625, 'learning_rate': 2.318918918918919e-05, 'epoch': 38.41}

 77%|███████▋  | 1421/1850 [1:34:42<31:12,  4.37s/it]
 77%|███████▋  | 1422/1850 [1:34:46<29:26,  4.13s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0245361328125, 'learning_rate': 2.3135135135135136e-05, 'epoch': 38.43}

 77%|███████▋  | 1422/1850 [1:34:46<29:26,  4.13s/it]
 77%|███████▋  | 1423/1850 [1:34:50<29:21,  4.12s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033203125, 'learning_rate': 2.3081081081081083e-05, 'epoch': 38.46}

 77%|███████▋  | 1423/1850 [1:34:50<29:21,  4.12s/it]
 77%|███████▋  | 1424/1850 [1:34:53<27:41,  3.90s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.028076171875, 'learning_rate': 2.3027027027027028e-05, 'epoch': 38.49}

 77%|███████▋  | 1424/1850 [1:34:53<27:41,  3.90s/it]
 77%|███████▋  | 1425/1850 [1:34:57<28:16,  3.99s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031005859375, 'learning_rate': 2.2972972972972976e-05, 'epoch': 38.51}

 77%|███████▋  | 1425/1850 [1:34:57<28:16,  3.99s/it]
 77%|███████▋  | 1426/1850 [1:35:02<29:01,  4.11s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0322265625, 'learning_rate': 2.291891891891892e-05, 'epoch': 38.54}

 77%|███████▋  | 1426/1850 [1:35:02<29:01,  4.11s/it]
 77%|███████▋  | 1427/1850 [1:35:06<28:35,  4.06s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.036865234375, 'learning_rate': 2.2864864864864865e-05, 'epoch': 38.57}

 77%|███████▋  | 1427/1850 [1:35:06<28:35,  4.06s/it]
 77%|███████▋  | 1428/1850 [1:35:09<27:50,  3.96s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03271484375, 'learning_rate': 2.2810810810810812e-05, 'epoch': 38.59}

 77%|███████▋  | 1428/1850 [1:35:09<27:50,  3.96s/it]
 77%|███████▋  | 1429/1850 [1:35:14<28:01,  4.00s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0311279296875, 'learning_rate': 2.275675675675676e-05, 'epoch': 38.62}

 77%|███████▋  | 1429/1850 [1:35:14<28:01,  4.00s/it]
 77%|███████▋  | 1430/1850 [1:35:17<27:03,  3.87s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.025390625, 'learning_rate': 2.2702702702702705e-05, 'epoch': 38.65}

 77%|███████▋  | 1430/1850 [1:35:17<27:03,  3.87s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 77%|███████▋  | 1431/1850 [1:35:22<29:43,  4.26s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0272216796875, 'learning_rate': 2.264864864864865e-05, 'epoch': 38.68}

 77%|███████▋  | 1431/1850 [1:35:22<29:43,  4.26s/it]
 77%|███████▋  | 1432/1850 [1:35:26<28:38,  4.11s/it]
                                                     
{'loss': 0.0018, 'grad_norm': 0.158203125, 'learning_rate': 2.2594594594594597e-05, 'epoch': 38.7}

 77%|███████▋  | 1432/1850 [1:35:26<28:38,  4.11s/it]
 77%|███████▋  | 1433/1850 [1:35:30<29:03,  4.18s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029296875, 'learning_rate': 2.254054054054054e-05, 'epoch': 38.73}

 77%|███████▋  | 1433/1850 [1:35:30<29:03,  4.18s/it]
 78%|███████▊  | 1434/1850 [1:35:34<27:15,  3.93s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031982421875, 'learning_rate': 2.248648648648649e-05, 'epoch': 38.76}

 78%|███████▊  | 1434/1850 [1:35:34<27:15,  3.93s/it]
 78%|███████▊  | 1435/1850 [1:35:37<26:42,  3.86s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0306396484375, 'learning_rate': 2.2432432432432434e-05, 'epoch': 38.78}

 78%|███████▊  | 1435/1850 [1:35:37<26:42,  3.86s/it]
 78%|███████▊  | 1436/1850 [1:35:42<27:45,  4.02s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0269775390625, 'learning_rate': 2.2378378378378378e-05, 'epoch': 38.81}

 78%|███████▊  | 1436/1850 [1:35:42<27:45,  4.02s/it]
 78%|███████▊  | 1437/1850 [1:35:46<27:14,  3.96s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.031494140625, 'learning_rate': 2.2324324324324326e-05, 'epoch': 38.84}

 78%|███████▊  | 1437/1850 [1:35:46<27:14,  3.96s/it]
 78%|███████▊  | 1438/1850 [1:35:50<28:09,  4.10s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03271484375, 'learning_rate': 2.227027027027027e-05, 'epoch': 38.86}

 78%|███████▊  | 1438/1850 [1:35:50<28:09,  4.10s/it]
 78%|███████▊  | 1439/1850 [1:35:54<28:07,  4.11s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031982421875, 'learning_rate': 2.2216216216216218e-05, 'epoch': 38.89}

 78%|███████▊  | 1439/1850 [1:35:54<28:07,  4.11s/it]
 78%|███████▊  | 1440/1850 [1:35:58<26:58,  3.95s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.028564453125, 'learning_rate': 2.2162162162162166e-05, 'epoch': 38.92}

 78%|███████▊  | 1440/1850 [1:35:58<26:58,  3.95s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 78%|███████▊  | 1441/1850 [1:36:02<28:32,  4.19s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029541015625, 'learning_rate': 2.2108108108108107e-05, 'epoch': 38.95}

 78%|███████▊  | 1441/1850 [1:36:02<28:32,  4.19s/it]
 78%|███████▊  | 1442/1850 [1:36:06<27:43,  4.08s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02685546875, 'learning_rate': 2.2054054054054055e-05, 'epoch': 38.97}

 78%|███████▊  | 1442/1850 [1:36:06<27:43,  4.08s/it]
 78%|███████▊  | 1443/1850 [1:36:10<27:01,  3.98s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031982421875, 'learning_rate': 2.2000000000000003e-05, 'epoch': 39.0}

 78%|███████▊  | 1443/1850 [1:36:10<27:01,  3.98s/it]
 78%|███████▊  | 1444/1850 [1:36:14<27:17,  4.03s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029052734375, 'learning_rate': 2.1945945945945947e-05, 'epoch': 39.03}

 78%|███████▊  | 1444/1850 [1:36:14<27:17,  4.03s/it]
 78%|███████▊  | 1445/1850 [1:36:18<27:02,  4.01s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0296630859375, 'learning_rate': 2.1891891891891895e-05, 'epoch': 39.05}

 78%|███████▊  | 1445/1850 [1:36:18<27:02,  4.01s/it]
 78%|███████▊  | 1446/1850 [1:36:22<26:31,  3.94s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0311279296875, 'learning_rate': 2.183783783783784e-05, 'epoch': 39.08}

 78%|███████▊  | 1446/1850 [1:36:22<26:31,  3.94s/it]
 78%|███████▊  | 1447/1850 [1:36:27<28:13,  4.20s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0303955078125, 'learning_rate': 2.1783783783783784e-05, 'epoch': 39.11}

 78%|███████▊  | 1447/1850 [1:36:27<28:13,  4.20s/it]
 78%|███████▊  | 1448/1850 [1:36:31<27:41,  4.13s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0296630859375, 'learning_rate': 2.1729729729729732e-05, 'epoch': 39.14}

 78%|███████▊  | 1448/1850 [1:36:31<27:41,  4.13s/it]
 78%|███████▊  | 1449/1850 [1:36:35<27:56,  4.18s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03466796875, 'learning_rate': 2.1675675675675676e-05, 'epoch': 39.16}

 78%|███████▊  | 1449/1850 [1:36:35<27:56,  4.18s/it]
 78%|███████▊  | 1450/1850 [1:36:39<28:26,  4.27s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02880859375, 'learning_rate': 2.1621621621621624e-05, 'epoch': 39.19}

 78%|███████▊  | 1450/1850 [1:36:39<28:26,  4.27s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 78%|███████▊  | 1451/1850 [1:36:44<28:58,  4.36s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.032958984375, 'learning_rate': 2.156756756756757e-05, 'epoch': 39.22}

 78%|███████▊  | 1451/1850 [1:36:44<28:58,  4.36s/it]
 78%|███████▊  | 1452/1850 [1:36:48<27:10,  4.10s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0289306640625, 'learning_rate': 2.1513513513513513e-05, 'epoch': 39.24}

 78%|███████▊  | 1452/1850 [1:36:48<27:10,  4.10s/it]
 79%|███████▊  | 1453/1850 [1:36:52<27:39,  4.18s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.031494140625, 'learning_rate': 2.145945945945946e-05, 'epoch': 39.27}

 79%|███████▊  | 1453/1850 [1:36:52<27:39,  4.18s/it]
 79%|███████▊  | 1454/1850 [1:36:56<26:39,  4.04s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0296630859375, 'learning_rate': 2.140540540540541e-05, 'epoch': 39.3}

 79%|███████▊  | 1454/1850 [1:36:56<26:39,  4.04s/it]
 79%|███████▊  | 1455/1850 [1:37:00<26:20,  4.00s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0286865234375, 'learning_rate': 2.1351351351351353e-05, 'epoch': 39.32}

 79%|███████▊  | 1455/1850 [1:37:00<26:20,  4.00s/it]
 79%|███████▊  | 1456/1850 [1:37:04<26:19,  4.01s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029052734375, 'learning_rate': 2.1297297297297298e-05, 'epoch': 39.35}

 79%|███████▊  | 1456/1850 [1:37:04<26:19,  4.01s/it]
 79%|███████▉  | 1457/1850 [1:37:08<27:42,  4.23s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0306396484375, 'learning_rate': 2.1243243243243245e-05, 'epoch': 39.38}

 79%|███████▉  | 1457/1850 [1:37:08<27:42,  4.23s/it]
 79%|███████▉  | 1458/1850 [1:37:12<26:34,  4.07s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.029296875, 'learning_rate': 2.118918918918919e-05, 'epoch': 39.41}

 79%|███████▉  | 1458/1850 [1:37:12<26:34,  4.07s/it]
 79%|███████▉  | 1459/1850 [1:37:16<25:41,  3.94s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0306396484375, 'learning_rate': 2.1135135135135138e-05, 'epoch': 39.43}

 79%|███████▉  | 1459/1850 [1:37:16<25:41,  3.94s/it]
 79%|███████▉  | 1460/1850 [1:37:20<25:37,  3.94s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031005859375, 'learning_rate': 2.1081081081081082e-05, 'epoch': 39.46}

 79%|███████▉  | 1460/1850 [1:37:20<25:37,  3.94s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 79%|███████▉  | 1461/1850 [1:37:23<25:11,  3.89s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0281982421875, 'learning_rate': 2.1027027027027027e-05, 'epoch': 39.49}

 79%|███████▉  | 1461/1850 [1:37:23<25:11,  3.89s/it]
 79%|███████▉  | 1462/1850 [1:37:27<25:31,  3.95s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0299072265625, 'learning_rate': 2.0972972972972974e-05, 'epoch': 39.51}

 79%|███████▉  | 1462/1850 [1:37:27<25:31,  3.95s/it]
 79%|███████▉  | 1463/1850 [1:37:31<25:36,  3.97s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.030517578125, 'learning_rate': 2.091891891891892e-05, 'epoch': 39.54}

 79%|███████▉  | 1463/1850 [1:37:31<25:36,  3.97s/it]
 79%|███████▉  | 1464/1850 [1:37:35<24:52,  3.87s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0301513671875, 'learning_rate': 2.0864864864864867e-05, 'epoch': 39.57}

 79%|███████▉  | 1464/1850 [1:37:35<24:52,  3.87s/it]
 79%|███████▉  | 1465/1850 [1:37:39<25:10,  3.92s/it]
                                                     
{'loss': 0.0015, 'grad_norm': 0.1591796875, 'learning_rate': 2.0810810810810815e-05, 'epoch': 39.59}

 79%|███████▉  | 1465/1850 [1:37:39<25:10,  3.92s/it]
 79%|███████▉  | 1466/1850 [1:37:44<26:07,  4.08s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.02587890625, 'learning_rate': 2.0756756756756756e-05, 'epoch': 39.62}

 79%|███████▉  | 1466/1850 [1:37:44<26:07,  4.08s/it]
 79%|███████▉  | 1467/1850 [1:37:48<26:29,  4.15s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0296630859375, 'learning_rate': 2.0702702702702703e-05, 'epoch': 39.65}

 79%|███████▉  | 1467/1850 [1:37:48<26:29,  4.15s/it]
 79%|███████▉  | 1468/1850 [1:37:53<27:17,  4.29s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.03271484375, 'learning_rate': 2.064864864864865e-05, 'epoch': 39.68}

 79%|███████▉  | 1468/1850 [1:37:53<27:17,  4.29s/it]
 79%|███████▉  | 1469/1850 [1:37:56<26:37,  4.19s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0301513671875, 'learning_rate': 2.0594594594594596e-05, 'epoch': 39.7}

 79%|███████▉  | 1469/1850 [1:37:56<26:37,  4.19s/it]
 79%|███████▉  | 1470/1850 [1:38:00<26:08,  4.13s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0296630859375, 'learning_rate': 2.0540540540540544e-05, 'epoch': 39.73}

 79%|███████▉  | 1470/1850 [1:38:00<26:08,  4.13s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 80%|███████▉  | 1471/1850 [1:38:05<27:16,  4.32s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.029541015625, 'learning_rate': 2.0486486486486488e-05, 'epoch': 39.76}

 80%|███████▉  | 1471/1850 [1:38:05<27:16,  4.32s/it]
 80%|███████▉  | 1472/1850 [1:38:09<27:05,  4.30s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0283203125, 'learning_rate': 2.0432432432432432e-05, 'epoch': 39.78}

 80%|███████▉  | 1472/1850 [1:38:09<27:05,  4.30s/it]
 80%|███████▉  | 1473/1850 [1:38:13<26:25,  4.20s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.026123046875, 'learning_rate': 2.037837837837838e-05, 'epoch': 39.81}

 80%|███████▉  | 1473/1850 [1:38:13<26:25,  4.20s/it]
 80%|███████▉  | 1474/1850 [1:38:17<24:42,  3.94s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.027587890625, 'learning_rate': 2.0324324324324325e-05, 'epoch': 39.84}

 80%|███████▉  | 1474/1850 [1:38:17<24:42,  3.94s/it]
 80%|███████▉  | 1475/1850 [1:38:21<25:05,  4.01s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0291748046875, 'learning_rate': 2.0270270270270273e-05, 'epoch': 39.86}

 80%|███████▉  | 1475/1850 [1:38:21<25:05,  4.01s/it]
 80%|███████▉  | 1476/1850 [1:38:25<24:24,  3.92s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0303955078125, 'learning_rate': 2.0216216216216217e-05, 'epoch': 39.89}

 80%|███████▉  | 1476/1850 [1:38:25<24:24,  3.92s/it]
 80%|███████▉  | 1477/1850 [1:38:29<24:58,  4.02s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 2.016216216216216e-05, 'epoch': 39.92}

 80%|███████▉  | 1477/1850 [1:38:29<24:58,  4.02s/it]
 80%|███████▉  | 1478/1850 [1:38:33<25:24,  4.10s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.1015625, 'learning_rate': 2.010810810810811e-05, 'epoch': 39.95}

 80%|███████▉  | 1478/1850 [1:38:33<25:24,  4.10s/it]
 80%|███████▉  | 1479/1850 [1:38:36<23:28,  3.80s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.028564453125, 'learning_rate': 2.0054054054054057e-05, 'epoch': 39.97}

 80%|███████▉  | 1479/1850 [1:38:36<23:28,  3.80s/it]
 80%|████████  | 1480/1850 [1:38:40<23:55,  3.88s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0308837890625, 'learning_rate': 2e-05, 'epoch': 40.0}

 80%|████████  | 1480/1850 [1:38:40<23:55,  3.88s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 80%|████████  | 1481/1850 [1:38:45<25:04,  4.08s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02978515625, 'learning_rate': 1.9945945945945946e-05, 'epoch': 40.03}

 80%|████████  | 1481/1850 [1:38:45<25:04,  4.08s/it]
 80%|████████  | 1482/1850 [1:38:50<26:25,  4.31s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.037841796875, 'learning_rate': 1.9891891891891894e-05, 'epoch': 40.05}

 80%|████████  | 1482/1850 [1:38:50<26:25,  4.31s/it]
 80%|████████  | 1483/1850 [1:38:54<25:55,  4.24s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.028076171875, 'learning_rate': 1.983783783783784e-05, 'epoch': 40.08}

 80%|████████  | 1483/1850 [1:38:54<25:55,  4.24s/it]
 80%|████████  | 1484/1850 [1:38:58<25:03,  4.11s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02734375, 'learning_rate': 1.9783783783783786e-05, 'epoch': 40.11}

 80%|████████  | 1484/1850 [1:38:58<25:03,  4.11s/it]
 80%|████████  | 1485/1850 [1:39:02<25:26,  4.18s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.028076171875, 'learning_rate': 1.972972972972973e-05, 'epoch': 40.14}

 80%|████████  | 1485/1850 [1:39:02<25:26,  4.18s/it]
 80%|████████  | 1486/1850 [1:39:06<24:23,  4.02s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.1435546875, 'learning_rate': 1.9675675675675675e-05, 'epoch': 40.16}

 80%|████████  | 1486/1850 [1:39:06<24:23,  4.02s/it]
 80%|████████  | 1487/1850 [1:39:10<25:42,  4.25s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.031494140625, 'learning_rate': 1.9621621621621623e-05, 'epoch': 40.19}

 80%|████████  | 1487/1850 [1:39:10<25:42,  4.25s/it]
 80%|████████  | 1488/1850 [1:39:15<25:27,  4.22s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0303955078125, 'learning_rate': 1.9567567567567567e-05, 'epoch': 40.22}

 80%|████████  | 1488/1850 [1:39:15<25:27,  4.22s/it]
 80%|████████  | 1489/1850 [1:39:18<24:34,  4.09s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.028564453125, 'learning_rate': 1.9513513513513515e-05, 'epoch': 40.24}

 80%|████████  | 1489/1850 [1:39:18<24:34,  4.09s/it]
 81%|████████  | 1490/1850 [1:39:22<24:32,  4.09s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.125, 'learning_rate': 1.9459459459459463e-05, 'epoch': 40.27}

 81%|████████  | 1490/1850 [1:39:22<24:32,  4.09s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 81%|████████  | 1491/1850 [1:39:27<24:32,  4.10s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029052734375, 'learning_rate': 1.9405405405405404e-05, 'epoch': 40.3}

 81%|████████  | 1491/1850 [1:39:27<24:32,  4.10s/it]
 81%|████████  | 1492/1850 [1:39:30<23:37,  3.96s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.033447265625, 'learning_rate': 1.9351351351351352e-05, 'epoch': 40.32}

 81%|████████  | 1492/1850 [1:39:30<23:37,  3.96s/it]
 81%|████████  | 1493/1850 [1:39:33<22:17,  3.75s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0225830078125, 'learning_rate': 1.92972972972973e-05, 'epoch': 40.35}

 81%|████████  | 1493/1850 [1:39:33<22:17,  3.75s/it]
 81%|████████  | 1494/1850 [1:39:37<21:58,  3.70s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.030029296875, 'learning_rate': 1.9243243243243244e-05, 'epoch': 40.38}

 81%|████████  | 1494/1850 [1:39:37<21:58,  3.70s/it]
 81%|████████  | 1495/1850 [1:39:41<22:02,  3.72s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029296875, 'learning_rate': 1.9189189189189192e-05, 'epoch': 40.41}

 81%|████████  | 1495/1850 [1:39:41<22:02,  3.72s/it]
 81%|████████  | 1496/1850 [1:39:45<22:19,  3.78s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.028564453125, 'learning_rate': 1.9135135135135137e-05, 'epoch': 40.43}

 81%|████████  | 1496/1850 [1:39:45<22:19,  3.78s/it]
 81%|████████  | 1497/1850 [1:39:49<23:38,  4.02s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03125, 'learning_rate': 1.908108108108108e-05, 'epoch': 40.46}

 81%|████████  | 1497/1850 [1:39:49<23:38,  4.02s/it]
 81%|████████  | 1498/1850 [1:39:54<24:19,  4.15s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0281982421875, 'learning_rate': 1.902702702702703e-05, 'epoch': 40.49}

 81%|████████  | 1498/1850 [1:39:54<24:19,  4.15s/it]
 81%|████████  | 1499/1850 [1:39:57<23:20,  3.99s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.03125, 'learning_rate': 1.8972972972972973e-05, 'epoch': 40.51}

 81%|████████  | 1499/1850 [1:39:57<23:20,  3.99s/it]
 81%|████████  | 1500/1850 [1:40:02<24:31,  4.20s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.0341796875, 'learning_rate': 1.891891891891892e-05, 'epoch': 40.54}

 81%|████████  | 1500/1850 [1:40:02<24:31,  4.20s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 81%|████████  | 1501/1850 [1:40:06<23:59,  4.13s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0341796875, 'learning_rate': 1.8864864864864866e-05, 'epoch': 40.57}

 81%|████████  | 1501/1850 [1:40:06<23:59,  4.13s/it]
 81%|████████  | 1502/1850 [1:40:10<22:50,  3.94s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.028076171875, 'learning_rate': 1.881081081081081e-05, 'epoch': 40.59}

 81%|████████  | 1502/1850 [1:40:10<22:50,  3.94s/it]
 81%|████████  | 1503/1850 [1:40:13<22:42,  3.93s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029052734375, 'learning_rate': 1.8756756756756758e-05, 'epoch': 40.62}

 81%|████████  | 1503/1850 [1:40:13<22:42,  3.93s/it]
 81%|████████▏ | 1504/1850 [1:40:17<22:43,  3.94s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.027587890625, 'learning_rate': 1.8702702702702706e-05, 'epoch': 40.65}

 81%|████████▏ | 1504/1850 [1:40:17<22:43,  3.94s/it]
 81%|████████▏ | 1505/1850 [1:40:21<22:10,  3.86s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0272216796875, 'learning_rate': 1.864864864864865e-05, 'epoch': 40.68}

 81%|████████▏ | 1505/1850 [1:40:21<22:10,  3.86s/it]
 81%|████████▏ | 1506/1850 [1:40:26<24:09,  4.21s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.031982421875, 'learning_rate': 1.8594594594594595e-05, 'epoch': 40.7}

 81%|████████▏ | 1506/1850 [1:40:26<24:09,  4.21s/it]
 81%|████████▏ | 1507/1850 [1:40:30<23:42,  4.15s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029541015625, 'learning_rate': 1.8540540540540542e-05, 'epoch': 40.73}

 81%|████████▏ | 1507/1850 [1:40:30<23:42,  4.15s/it]
 82%|████████▏ | 1508/1850 [1:40:34<22:42,  3.98s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.026123046875, 'learning_rate': 1.8486486486486487e-05, 'epoch': 40.76}

 82%|████████▏ | 1508/1850 [1:40:34<22:42,  3.98s/it]
 82%|████████▏ | 1509/1850 [1:40:38<23:02,  4.05s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0286865234375, 'learning_rate': 1.8432432432432435e-05, 'epoch': 40.78}

 82%|████████▏ | 1509/1850 [1:40:38<23:02,  4.05s/it]
 82%|████████▏ | 1510/1850 [1:40:42<23:06,  4.08s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0291748046875, 'learning_rate': 1.837837837837838e-05, 'epoch': 40.81}

 82%|████████▏ | 1510/1850 [1:40:42<23:06,  4.08s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 82%|████████▏ | 1511/1850 [1:40:46<22:46,  4.03s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03076171875, 'learning_rate': 1.8324324324324324e-05, 'epoch': 40.84}

 82%|████████▏ | 1511/1850 [1:40:46<22:46,  4.03s/it]
 82%|████████▏ | 1512/1850 [1:40:49<21:15,  3.77s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.03173828125, 'learning_rate': 1.827027027027027e-05, 'epoch': 40.86}

 82%|████████▏ | 1512/1850 [1:40:49<21:15,  3.77s/it]
 82%|████████▏ | 1513/1850 [1:40:53<21:17,  3.79s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0252685546875, 'learning_rate': 1.8216216216216216e-05, 'epoch': 40.89}

 82%|████████▏ | 1513/1850 [1:40:53<21:17,  3.79s/it]
 82%|████████▏ | 1514/1850 [1:40:57<21:58,  3.92s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03173828125, 'learning_rate': 1.8162162162162164e-05, 'epoch': 40.92}

 82%|████████▏ | 1514/1850 [1:40:57<21:58,  3.92s/it]
 82%|████████▏ | 1515/1850 [1:41:01<21:02,  3.77s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0301513671875, 'learning_rate': 1.810810810810811e-05, 'epoch': 40.95}

 82%|████████▏ | 1515/1850 [1:41:01<21:02,  3.77s/it]
 82%|████████▏ | 1516/1850 [1:41:04<20:59,  3.77s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0279541015625, 'learning_rate': 1.8054054054054053e-05, 'epoch': 40.97}

 82%|████████▏ | 1516/1850 [1:41:04<20:59,  3.77s/it]
 82%|████████▏ | 1517/1850 [1:41:07<19:43,  3.55s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03173828125, 'learning_rate': 1.8e-05, 'epoch': 41.0}

 82%|████████▏ | 1517/1850 [1:41:07<19:43,  3.55s/it]
 82%|████████▏ | 1518/1850 [1:41:13<22:19,  4.03s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0294189453125, 'learning_rate': 1.7945945945945948e-05, 'epoch': 41.03}

 82%|████████▏ | 1518/1850 [1:41:13<22:19,  4.03s/it]
 82%|████████▏ | 1519/1850 [1:41:16<21:47,  3.95s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0301513671875, 'learning_rate': 1.7891891891891893e-05, 'epoch': 41.05}

 82%|████████▏ | 1519/1850 [1:41:16<21:47,  3.95s/it]
 82%|████████▏ | 1520/1850 [1:41:20<21:54,  3.98s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.029052734375, 'learning_rate': 1.783783783783784e-05, 'epoch': 41.08}

 82%|████████▏ | 1520/1850 [1:41:20<21:54,  3.98s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 82%|████████▏ | 1521/1850 [1:41:24<21:57,  4.00s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.031982421875, 'learning_rate': 1.7783783783783785e-05, 'epoch': 41.11}

 82%|████████▏ | 1521/1850 [1:41:24<21:57,  4.00s/it]
 82%|████████▏ | 1522/1850 [1:41:28<21:17,  3.90s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0250244140625, 'learning_rate': 1.772972972972973e-05, 'epoch': 41.14}

 82%|████████▏ | 1522/1850 [1:41:28<21:17,  3.90s/it]
 82%|████████▏ | 1523/1850 [1:41:32<20:41,  3.80s/it]
                                                     
{'loss': 0.0015, 'grad_norm': 0.1591796875, 'learning_rate': 1.7675675675675677e-05, 'epoch': 41.16}

 82%|████████▏ | 1523/1850 [1:41:32<20:41,  3.80s/it]
 82%|████████▏ | 1524/1850 [1:41:36<22:05,  4.06s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031494140625, 'learning_rate': 1.7621621621621622e-05, 'epoch': 41.19}

 82%|████████▏ | 1524/1850 [1:41:36<22:05,  4.06s/it]
 82%|████████▏ | 1525/1850 [1:41:40<21:44,  4.01s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 1.756756756756757e-05, 'epoch': 41.22}

 82%|████████▏ | 1525/1850 [1:41:40<21:44,  4.01s/it]
 82%|████████▏ | 1526/1850 [1:41:45<22:02,  4.08s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0294189453125, 'learning_rate': 1.7513513513513514e-05, 'epoch': 41.24}

 82%|████████▏ | 1526/1850 [1:41:45<22:02,  4.08s/it]
 83%|████████▎ | 1527/1850 [1:41:49<22:37,  4.20s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.11767578125, 'learning_rate': 1.745945945945946e-05, 'epoch': 41.27}

 83%|████████▎ | 1527/1850 [1:41:49<22:37,  4.20s/it]
 83%|████████▎ | 1528/1850 [1:41:53<22:21,  4.17s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.034423828125, 'learning_rate': 1.7405405405405406e-05, 'epoch': 41.3}

 83%|████████▎ | 1528/1850 [1:41:53<22:21,  4.17s/it]
 83%|████████▎ | 1529/1850 [1:41:57<21:46,  4.07s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0286865234375, 'learning_rate': 1.7351351351351354e-05, 'epoch': 41.32}

 83%|████████▎ | 1529/1850 [1:41:57<21:46,  4.07s/it]
 83%|████████▎ | 1530/1850 [1:42:02<22:57,  4.30s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.031494140625, 'learning_rate': 1.72972972972973e-05, 'epoch': 41.35}

 83%|████████▎ | 1530/1850 [1:42:02<22:57,  4.30s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 83%|████████▎ | 1531/1850 [1:42:07<23:39,  4.45s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029541015625, 'learning_rate': 1.7243243243243243e-05, 'epoch': 41.38}

 83%|████████▎ | 1531/1850 [1:42:07<23:39,  4.45s/it]
 83%|████████▎ | 1532/1850 [1:42:11<23:30,  4.44s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.0322265625, 'learning_rate': 1.718918918918919e-05, 'epoch': 41.41}

 83%|████████▎ | 1532/1850 [1:42:11<23:30,  4.44s/it]
 83%|████████▎ | 1533/1850 [1:42:15<22:09,  4.20s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.029052734375, 'learning_rate': 1.7135135135135135e-05, 'epoch': 41.43}

 83%|████████▎ | 1533/1850 [1:42:15<22:09,  4.20s/it]
 83%|████████▎ | 1534/1850 [1:42:19<21:58,  4.17s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0277099609375, 'learning_rate': 1.7081081081081083e-05, 'epoch': 41.46}

 83%|████████▎ | 1534/1850 [1:42:19<21:58,  4.17s/it]
 83%|████████▎ | 1535/1850 [1:42:23<21:55,  4.18s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0308837890625, 'learning_rate': 1.7027027027027028e-05, 'epoch': 41.49}

 83%|████████▎ | 1535/1850 [1:42:23<21:55,  4.18s/it]
 83%|████████▎ | 1536/1850 [1:42:26<20:47,  3.97s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0260009765625, 'learning_rate': 1.6972972972972972e-05, 'epoch': 41.51}

 83%|████████▎ | 1536/1850 [1:42:26<20:47,  3.97s/it]
 83%|████████▎ | 1537/1850 [1:42:30<20:34,  3.95s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029541015625, 'learning_rate': 1.691891891891892e-05, 'epoch': 41.54}

 83%|████████▎ | 1537/1850 [1:42:30<20:34,  3.95s/it]
 83%|████████▎ | 1538/1850 [1:42:34<20:54,  4.02s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033203125, 'learning_rate': 1.6864864864864864e-05, 'epoch': 41.57}

 83%|████████▎ | 1538/1850 [1:42:34<20:54,  4.02s/it]
 83%|████████▎ | 1539/1850 [1:42:38<19:40,  3.80s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.02978515625, 'learning_rate': 1.6810810810810812e-05, 'epoch': 41.59}

 83%|████████▎ | 1539/1850 [1:42:38<19:40,  3.80s/it]
 83%|████████▎ | 1540/1850 [1:42:41<18:26,  3.57s/it]
                                                     
{'loss': 0.0005, 'grad_norm': 0.02685546875, 'learning_rate': 1.675675675675676e-05, 'epoch': 41.62}

 83%|████████▎ | 1540/1850 [1:42:41<18:26,  3.57s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 83%|████████▎ | 1541/1850 [1:42:45<19:13,  3.73s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0238037109375, 'learning_rate': 1.67027027027027e-05, 'epoch': 41.65}

 83%|████████▎ | 1541/1850 [1:42:45<19:13,  3.73s/it]
 83%|████████▎ | 1542/1850 [1:42:49<19:05,  3.72s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0247802734375, 'learning_rate': 1.664864864864865e-05, 'epoch': 41.68}

 83%|████████▎ | 1542/1850 [1:42:49<19:05,  3.72s/it]
 83%|████████▎ | 1543/1850 [1:42:53<19:45,  3.86s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02978515625, 'learning_rate': 1.6594594594594597e-05, 'epoch': 41.7}

 83%|████████▎ | 1543/1850 [1:42:53<19:45,  3.86s/it]
 83%|████████▎ | 1544/1850 [1:42:57<19:37,  3.85s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02783203125, 'learning_rate': 1.654054054054054e-05, 'epoch': 41.73}

 83%|████████▎ | 1544/1850 [1:42:57<19:37,  3.85s/it]
 84%|████████▎ | 1545/1850 [1:43:00<19:21,  3.81s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.031494140625, 'learning_rate': 1.648648648648649e-05, 'epoch': 41.76}

 84%|████████▎ | 1545/1850 [1:43:00<19:21,  3.81s/it]
 84%|████████▎ | 1546/1850 [1:43:04<19:29,  3.85s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0284423828125, 'learning_rate': 1.6432432432432434e-05, 'epoch': 41.78}

 84%|████████▎ | 1546/1850 [1:43:04<19:29,  3.85s/it]
 84%|████████▎ | 1547/1850 [1:43:08<19:25,  3.85s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0257568359375, 'learning_rate': 1.6378378378378378e-05, 'epoch': 41.81}

 84%|████████▎ | 1547/1850 [1:43:08<19:25,  3.85s/it]
 84%|████████▎ | 1548/1850 [1:43:12<19:33,  3.89s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.032470703125, 'learning_rate': 1.6324324324324326e-05, 'epoch': 41.84}

 84%|████████▎ | 1548/1850 [1:43:12<19:33,  3.89s/it]
 84%|████████▎ | 1549/1850 [1:43:16<19:31,  3.89s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 1.627027027027027e-05, 'epoch': 41.86}

 84%|████████▎ | 1549/1850 [1:43:16<19:31,  3.89s/it]
 84%|████████▍ | 1550/1850 [1:43:20<19:51,  3.97s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03076171875, 'learning_rate': 1.6216216216216218e-05, 'epoch': 41.89}

 84%|████████▍ | 1550/1850 [1:43:20<19:51,  3.97s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 84%|████████▍ | 1551/1850 [1:43:24<19:44,  3.96s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0294189453125, 'learning_rate': 1.6162162162162163e-05, 'epoch': 41.92}

 84%|████████▍ | 1551/1850 [1:43:24<19:44,  3.96s/it]
 84%|████████▍ | 1552/1850 [1:43:28<19:32,  3.94s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02880859375, 'learning_rate': 1.6108108108108107e-05, 'epoch': 41.95}

 84%|████████▍ | 1552/1850 [1:43:28<19:32,  3.94s/it]
 84%|████████▍ | 1553/1850 [1:43:32<19:03,  3.85s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0255126953125, 'learning_rate': 1.6054054054054055e-05, 'epoch': 41.97}

 84%|████████▍ | 1553/1850 [1:43:32<19:03,  3.85s/it]
 84%|████████▍ | 1554/1850 [1:43:36<19:13,  3.90s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0294189453125, 'learning_rate': 1.6000000000000003e-05, 'epoch': 42.0}

 84%|████████▍ | 1554/1850 [1:43:36<19:13,  3.90s/it]
 84%|████████▍ | 1555/1850 [1:43:39<18:21,  3.73s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.1494140625, 'learning_rate': 1.5945945945945947e-05, 'epoch': 42.03}

 84%|████████▍ | 1555/1850 [1:43:39<18:21,  3.73s/it]
 84%|████████▍ | 1556/1850 [1:43:43<18:36,  3.80s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0296630859375, 'learning_rate': 1.589189189189189e-05, 'epoch': 42.05}

 84%|████████▍ | 1556/1850 [1:43:43<18:36,  3.80s/it]
 84%|████████▍ | 1557/1850 [1:43:47<18:19,  3.75s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.028564453125, 'learning_rate': 1.583783783783784e-05, 'epoch': 42.08}

 84%|████████▍ | 1557/1850 [1:43:47<18:19,  3.75s/it]
 84%|████████▍ | 1558/1850 [1:43:51<18:47,  3.86s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0303955078125, 'learning_rate': 1.5783783783783784e-05, 'epoch': 42.11}

 84%|████████▍ | 1558/1850 [1:43:51<18:47,  3.86s/it]
 84%|████████▍ | 1559/1850 [1:43:54<18:32,  3.82s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03369140625, 'learning_rate': 1.572972972972973e-05, 'epoch': 42.14}

 84%|████████▍ | 1559/1850 [1:43:54<18:32,  3.82s/it]
 84%|████████▍ | 1560/1850 [1:43:58<18:04,  3.74s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0308837890625, 'learning_rate': 1.5675675675675676e-05, 'epoch': 42.16}

 84%|████████▍ | 1560/1850 [1:43:58<18:04,  3.74s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 84%|████████▍ | 1561/1850 [1:44:03<19:22,  4.02s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0277099609375, 'learning_rate': 1.562162162162162e-05, 'epoch': 42.19}

 84%|████████▍ | 1561/1850 [1:44:03<19:22,  4.02s/it]
 84%|████████▍ | 1562/1850 [1:44:06<18:31,  3.86s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0303955078125, 'learning_rate': 1.556756756756757e-05, 'epoch': 42.22}

 84%|████████▍ | 1562/1850 [1:44:06<18:31,  3.86s/it]
 84%|████████▍ | 1563/1850 [1:44:10<18:11,  3.80s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0286865234375, 'learning_rate': 1.5513513513513513e-05, 'epoch': 42.24}

 84%|████████▍ | 1563/1850 [1:44:10<18:11,  3.80s/it]
 85%|████████▍ | 1564/1850 [1:44:14<19:15,  4.04s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0294189453125, 'learning_rate': 1.545945945945946e-05, 'epoch': 42.27}

 85%|████████▍ | 1564/1850 [1:44:14<19:15,  4.04s/it]
 85%|████████▍ | 1565/1850 [1:44:18<18:31,  3.90s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0234375, 'learning_rate': 1.540540540540541e-05, 'epoch': 42.3}

 85%|████████▍ | 1565/1850 [1:44:18<18:31,  3.90s/it]
 85%|████████▍ | 1566/1850 [1:44:21<17:41,  3.74s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 1.535135135135135e-05, 'epoch': 42.32}

 85%|████████▍ | 1566/1850 [1:44:21<17:41,  3.74s/it]
 85%|████████▍ | 1567/1850 [1:44:25<17:09,  3.64s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0263671875, 'learning_rate': 1.5297297297297297e-05, 'epoch': 42.35}

 85%|████████▍ | 1567/1850 [1:44:25<17:09,  3.64s/it]
 85%|████████▍ | 1568/1850 [1:44:29<18:19,  3.90s/it]
                                                     
{'loss': 0.0015, 'grad_norm': 0.11767578125, 'learning_rate': 1.5243243243243244e-05, 'epoch': 42.38}

 85%|████████▍ | 1568/1850 [1:44:29<18:19,  3.90s/it]
 85%|████████▍ | 1569/1850 [1:44:33<18:45,  4.00s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029052734375, 'learning_rate': 1.518918918918919e-05, 'epoch': 42.41}

 85%|████████▍ | 1569/1850 [1:44:33<18:45,  4.00s/it]
 85%|████████▍ | 1570/1850 [1:44:38<18:57,  4.06s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.032958984375, 'learning_rate': 1.5135135135135136e-05, 'epoch': 42.43}

 85%|████████▍ | 1570/1850 [1:44:38<18:57,  4.06s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 85%|████████▍ | 1571/1850 [1:44:43<20:12,  4.35s/it]
                                                     
{'loss': 0.0015, 'grad_norm': 0.0361328125, 'learning_rate': 1.508108108108108e-05, 'epoch': 42.46}

 85%|████████▍ | 1571/1850 [1:44:43<20:12,  4.35s/it]
 85%|████████▍ | 1572/1850 [1:44:46<18:46,  4.05s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0279541015625, 'learning_rate': 1.5027027027027026e-05, 'epoch': 42.49}

 85%|████████▍ | 1572/1850 [1:44:46<18:46,  4.05s/it]
 85%|████████▌ | 1573/1850 [1:44:49<17:22,  3.76s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0281982421875, 'learning_rate': 1.4972972972972973e-05, 'epoch': 42.51}

 85%|████████▌ | 1573/1850 [1:44:49<17:22,  3.76s/it]
 85%|████████▌ | 1574/1850 [1:44:53<17:46,  3.86s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0303955078125, 'learning_rate': 1.491891891891892e-05, 'epoch': 42.54}

 85%|████████▌ | 1574/1850 [1:44:53<17:46,  3.86s/it]
 85%|████████▌ | 1575/1850 [1:44:57<17:06,  3.73s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.030029296875, 'learning_rate': 1.4864864864864867e-05, 'epoch': 42.57}

 85%|████████▌ | 1575/1850 [1:44:57<17:06,  3.73s/it]
 85%|████████▌ | 1576/1850 [1:45:01<17:18,  3.79s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02734375, 'learning_rate': 1.4810810810810813e-05, 'epoch': 42.59}

 85%|████████▌ | 1576/1850 [1:45:01<17:18,  3.79s/it]
 85%|████████▌ | 1577/1850 [1:45:05<18:07,  3.98s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.043701171875, 'learning_rate': 1.4756756756756757e-05, 'epoch': 42.62}

 85%|████████▌ | 1577/1850 [1:45:05<18:07,  3.98s/it]
 85%|████████▌ | 1578/1850 [1:45:09<18:19,  4.04s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0281982421875, 'learning_rate': 1.4702702702702703e-05, 'epoch': 42.65}

 85%|████████▌ | 1578/1850 [1:45:09<18:19,  4.04s/it]
 85%|████████▌ | 1579/1850 [1:45:13<18:21,  4.06s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03466796875, 'learning_rate': 1.464864864864865e-05, 'epoch': 42.68}

 85%|████████▌ | 1579/1850 [1:45:13<18:21,  4.06s/it]
 85%|████████▌ | 1580/1850 [1:45:18<19:18,  4.29s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 1.4594594594594596e-05, 'epoch': 42.7}

 85%|████████▌ | 1580/1850 [1:45:18<19:18,  4.29s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 85%|████████▌ | 1581/1850 [1:45:23<20:37,  4.60s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.035400390625, 'learning_rate': 1.4540540540540543e-05, 'epoch': 42.73}

 85%|████████▌ | 1581/1850 [1:45:23<20:37,  4.60s/it]
 86%|████████▌ | 1582/1850 [1:45:27<19:24,  4.35s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03173828125, 'learning_rate': 1.4486486486486486e-05, 'epoch': 42.76}

 86%|████████▌ | 1582/1850 [1:45:27<19:24,  4.35s/it]
 86%|████████▌ | 1583/1850 [1:45:31<19:11,  4.31s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.027587890625, 'learning_rate': 1.4432432432432432e-05, 'epoch': 42.78}

 86%|████████▌ | 1583/1850 [1:45:31<19:11,  4.31s/it]
 86%|████████▌ | 1584/1850 [1:45:36<18:53,  4.26s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029052734375, 'learning_rate': 1.4378378378378378e-05, 'epoch': 42.81}

 86%|████████▌ | 1584/1850 [1:45:36<18:53,  4.26s/it]
 86%|████████▌ | 1585/1850 [1:45:39<18:00,  4.08s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.030029296875, 'learning_rate': 1.4324324324324326e-05, 'epoch': 42.84}

 86%|████████▌ | 1585/1850 [1:45:39<18:00,  4.08s/it]
 86%|████████▌ | 1586/1850 [1:45:43<17:48,  4.05s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 1.4270270270270272e-05, 'epoch': 42.86}

 86%|████████▌ | 1586/1850 [1:45:43<17:48,  4.05s/it]
 86%|████████▌ | 1587/1850 [1:45:48<18:07,  4.13s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031494140625, 'learning_rate': 1.4216216216216215e-05, 'epoch': 42.89}

 86%|████████▌ | 1587/1850 [1:45:48<18:07,  4.13s/it]
 86%|████████▌ | 1588/1850 [1:45:52<17:57,  4.11s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02783203125, 'learning_rate': 1.4162162162162163e-05, 'epoch': 42.92}

 86%|████████▌ | 1588/1850 [1:45:52<17:57,  4.11s/it]
 86%|████████▌ | 1589/1850 [1:45:55<17:17,  3.97s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0264892578125, 'learning_rate': 1.410810810810811e-05, 'epoch': 42.95}

 86%|████████▌ | 1589/1850 [1:45:55<17:17,  3.97s/it]
 86%|████████▌ | 1590/1850 [1:46:00<17:40,  4.08s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0262451171875, 'learning_rate': 1.4054054054054055e-05, 'epoch': 42.97}

 86%|████████▌ | 1590/1850 [1:46:00<17:40,  4.08s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 86%|████████▌ | 1591/1850 [1:46:04<17:47,  4.12s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03125, 'learning_rate': 1.4000000000000001e-05, 'epoch': 43.0}

 86%|████████▌ | 1591/1850 [1:46:04<17:47,  4.12s/it]
 86%|████████▌ | 1592/1850 [1:46:08<17:59,  4.19s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0296630859375, 'learning_rate': 1.3945945945945946e-05, 'epoch': 43.03}

 86%|████████▌ | 1592/1850 [1:46:08<17:59,  4.19s/it]
 86%|████████▌ | 1593/1850 [1:46:12<17:49,  4.16s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.09423828125, 'learning_rate': 1.3891891891891892e-05, 'epoch': 43.05}

 86%|████████▌ | 1593/1850 [1:46:12<17:49,  4.16s/it]
 86%|████████▌ | 1594/1850 [1:46:16<17:23,  4.07s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.030517578125, 'learning_rate': 1.3837837837837838e-05, 'epoch': 43.08}

 86%|████████▌ | 1594/1850 [1:46:16<17:23,  4.07s/it]
 86%|████████▌ | 1595/1850 [1:46:20<17:14,  4.06s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0283203125, 'learning_rate': 1.3783783783783786e-05, 'epoch': 43.11}

 86%|████████▌ | 1595/1850 [1:46:20<17:14,  4.06s/it]
 86%|████████▋ | 1596/1850 [1:46:24<17:10,  4.06s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02734375, 'learning_rate': 1.3729729729729732e-05, 'epoch': 43.14}

 86%|████████▋ | 1596/1850 [1:46:24<17:10,  4.06s/it]
 86%|████████▋ | 1597/1850 [1:46:28<17:20,  4.11s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02880859375, 'learning_rate': 1.3675675675675675e-05, 'epoch': 43.16}

 86%|████████▋ | 1597/1850 [1:46:28<17:20,  4.11s/it]
 86%|████████▋ | 1598/1850 [1:46:33<17:21,  4.13s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0301513671875, 'learning_rate': 1.3621621621621621e-05, 'epoch': 43.19}

 86%|████████▋ | 1598/1850 [1:46:33<17:21,  4.13s/it]
 86%|████████▋ | 1599/1850 [1:46:37<17:01,  4.07s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029296875, 'learning_rate': 1.3567567567567569e-05, 'epoch': 43.22}

 86%|████████▋ | 1599/1850 [1:46:37<17:01,  4.07s/it]
 86%|████████▋ | 1600/1850 [1:46:41<17:58,  4.31s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031494140625, 'learning_rate': 1.3513513513513515e-05, 'epoch': 43.24}

 86%|████████▋ | 1600/1850 [1:46:41<17:58,  4.31s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 87%|████████▋ | 1601/1850 [1:46:46<17:45,  4.28s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.029541015625, 'learning_rate': 1.3459459459459461e-05, 'epoch': 43.27}

 87%|████████▋ | 1601/1850 [1:46:46<17:45,  4.28s/it]
 87%|████████▋ | 1602/1850 [1:46:49<17:04,  4.13s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0303955078125, 'learning_rate': 1.3405405405405406e-05, 'epoch': 43.3}

 87%|████████▋ | 1602/1850 [1:46:49<17:04,  4.13s/it]
 87%|████████▋ | 1603/1850 [1:46:53<16:33,  4.02s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.02685546875, 'learning_rate': 1.3351351351351352e-05, 'epoch': 43.32}

 87%|████████▋ | 1603/1850 [1:46:53<16:33,  4.02s/it]
 87%|████████▋ | 1604/1850 [1:46:57<15:41,  3.83s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0296630859375, 'learning_rate': 1.3297297297297298e-05, 'epoch': 43.35}

 87%|████████▋ | 1604/1850 [1:46:57<15:41,  3.83s/it]
 87%|████████▋ | 1605/1850 [1:47:00<15:37,  3.83s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0289306640625, 'learning_rate': 1.3243243243243244e-05, 'epoch': 43.38}

 87%|████████▋ | 1605/1850 [1:47:00<15:37,  3.83s/it]
 87%|████████▋ | 1606/1850 [1:47:05<15:58,  3.93s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02880859375, 'learning_rate': 1.3189189189189192e-05, 'epoch': 43.41}

 87%|████████▋ | 1606/1850 [1:47:05<15:58,  3.93s/it]
 87%|████████▋ | 1607/1850 [1:47:09<17:04,  4.22s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.03759765625, 'learning_rate': 1.3135135135135135e-05, 'epoch': 43.43}

 87%|████████▋ | 1607/1850 [1:47:09<17:04,  4.22s/it]
 87%|████████▋ | 1608/1850 [1:47:13<16:31,  4.10s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.031982421875, 'learning_rate': 1.308108108108108e-05, 'epoch': 43.46}

 87%|████████▋ | 1608/1850 [1:47:13<16:31,  4.10s/it]
 87%|████████▋ | 1609/1850 [1:47:17<16:21,  4.07s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0263671875, 'learning_rate': 1.3027027027027027e-05, 'epoch': 43.49}

 87%|████████▋ | 1609/1850 [1:47:17<16:21,  4.07s/it]
 87%|████████▋ | 1610/1850 [1:47:21<15:46,  3.94s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029296875, 'learning_rate': 1.2972972972972975e-05, 'epoch': 43.51}

 87%|████████▋ | 1610/1850 [1:47:21<15:46,  3.94s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 87%|████████▋ | 1611/1850 [1:47:25<16:05,  4.04s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0283203125, 'learning_rate': 1.2918918918918921e-05, 'epoch': 43.54}

 87%|████████▋ | 1611/1850 [1:47:25<16:05,  4.04s/it]
 87%|████████▋ | 1612/1850 [1:47:29<15:34,  3.93s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0322265625, 'learning_rate': 1.2864864864864864e-05, 'epoch': 43.57}

 87%|████████▋ | 1612/1850 [1:47:29<15:34,  3.93s/it]
 87%|████████▋ | 1613/1850 [1:47:33<15:52,  4.02s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0289306640625, 'learning_rate': 1.2810810810810812e-05, 'epoch': 43.59}

 87%|████████▋ | 1613/1850 [1:47:33<15:52,  4.02s/it]
 87%|████████▋ | 1614/1850 [1:47:38<16:47,  4.27s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0291748046875, 'learning_rate': 1.2756756756756758e-05, 'epoch': 43.62}

 87%|████████▋ | 1614/1850 [1:47:38<16:47,  4.27s/it]
 87%|████████▋ | 1615/1850 [1:47:42<16:48,  4.29s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0279541015625, 'learning_rate': 1.2702702702702704e-05, 'epoch': 43.65}

 87%|████████▋ | 1615/1850 [1:47:42<16:48,  4.29s/it]
 87%|████████▋ | 1616/1850 [1:47:47<16:44,  4.29s/it]
                                                     
{'loss': 0.0018, 'grad_norm': 0.1728515625, 'learning_rate': 1.264864864864865e-05, 'epoch': 43.68}

 87%|████████▋ | 1616/1850 [1:47:47<16:44,  4.29s/it]
 87%|████████▋ | 1617/1850 [1:47:50<15:49,  4.08s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0252685546875, 'learning_rate': 1.2594594594594594e-05, 'epoch': 43.7}

 87%|████████▋ | 1617/1850 [1:47:50<15:49,  4.08s/it]
 87%|████████▋ | 1618/1850 [1:47:55<16:32,  4.28s/it]
                                                     
{'loss': 0.0014, 'grad_norm': 0.03466796875, 'learning_rate': 1.254054054054054e-05, 'epoch': 43.73}

 87%|████████▋ | 1618/1850 [1:47:55<16:32,  4.28s/it]
 88%|████████▊ | 1619/1850 [1:47:59<16:03,  4.17s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029052734375, 'learning_rate': 1.2486486486486487e-05, 'epoch': 43.76}

 88%|████████▊ | 1619/1850 [1:47:59<16:03,  4.17s/it]
 88%|████████▊ | 1620/1850 [1:48:03<15:30,  4.05s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.02685546875, 'learning_rate': 1.2432432432432435e-05, 'epoch': 43.78}

 88%|████████▊ | 1620/1850 [1:48:03<15:30,  4.05s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 88%|████████▊ | 1621/1850 [1:48:07<16:08,  4.23s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0308837890625, 'learning_rate': 1.2378378378378379e-05, 'epoch': 43.81}

 88%|████████▊ | 1621/1850 [1:48:07<16:08,  4.23s/it]
 88%|████████▊ | 1622/1850 [1:48:10<14:36,  3.85s/it]
                                                     
{'loss': 0.0005, 'grad_norm': 0.0277099609375, 'learning_rate': 1.2324324324324325e-05, 'epoch': 43.84}

 88%|████████▊ | 1622/1850 [1:48:10<14:36,  3.85s/it]
 88%|████████▊ | 1623/1850 [1:48:13<13:50,  3.66s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029296875, 'learning_rate': 1.227027027027027e-05, 'epoch': 43.86}

 88%|████████▊ | 1623/1850 [1:48:13<13:50,  3.66s/it]
 88%|████████▊ | 1624/1850 [1:48:17<13:57,  3.70s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0301513671875, 'learning_rate': 1.2216216216216217e-05, 'epoch': 43.89}

 88%|████████▊ | 1624/1850 [1:48:17<13:57,  3.70s/it]
 88%|████████▊ | 1625/1850 [1:48:21<14:26,  3.85s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03662109375, 'learning_rate': 1.2162162162162164e-05, 'epoch': 43.92}

 88%|████████▊ | 1625/1850 [1:48:21<14:26,  3.85s/it]
 88%|████████▊ | 1626/1850 [1:48:26<15:02,  4.03s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03271484375, 'learning_rate': 1.2108108108108108e-05, 'epoch': 43.95}

 88%|████████▊ | 1626/1850 [1:48:26<15:02,  4.03s/it]
 88%|████████▊ | 1627/1850 [1:48:30<14:58,  4.03s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.029052734375, 'learning_rate': 1.2054054054054056e-05, 'epoch': 43.97}

 88%|████████▊ | 1627/1850 [1:48:30<14:58,  4.03s/it]
 88%|████████▊ | 1628/1850 [1:48:34<14:54,  4.03s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0301513671875, 'learning_rate': 1.2e-05, 'epoch': 44.0}

 88%|████████▊ | 1628/1850 [1:48:34<14:54,  4.03s/it]
 88%|████████▊ | 1629/1850 [1:48:38<14:34,  3.95s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02978515625, 'learning_rate': 1.1945945945945946e-05, 'epoch': 44.03}

 88%|████████▊ | 1629/1850 [1:48:38<14:34,  3.95s/it]
 88%|████████▊ | 1630/1850 [1:48:42<14:21,  3.92s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.034423828125, 'learning_rate': 1.1891891891891893e-05, 'epoch': 44.05}

 88%|████████▊ | 1630/1850 [1:48:42<14:21,  3.92s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 88%|████████▊ | 1631/1850 [1:48:46<14:48,  4.06s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0269775390625, 'learning_rate': 1.1837837837837839e-05, 'epoch': 44.08}

 88%|████████▊ | 1631/1850 [1:48:46<14:48,  4.06s/it]
 88%|████████▊ | 1632/1850 [1:48:50<14:52,  4.09s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0269775390625, 'learning_rate': 1.1783783783783785e-05, 'epoch': 44.11}

 88%|████████▊ | 1632/1850 [1:48:50<14:52,  4.09s/it]
 88%|████████▊ | 1633/1850 [1:48:54<14:05,  3.90s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.027099609375, 'learning_rate': 1.172972972972973e-05, 'epoch': 44.14}

 88%|████████▊ | 1633/1850 [1:48:54<14:05,  3.90s/it]
 88%|████████▊ | 1634/1850 [1:48:57<14:00,  3.89s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02490234375, 'learning_rate': 1.1675675675675677e-05, 'epoch': 44.16}

 88%|████████▊ | 1634/1850 [1:48:57<14:00,  3.89s/it]
 88%|████████▊ | 1635/1850 [1:49:02<14:20,  4.00s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 1.1621621621621623e-05, 'epoch': 44.19}

 88%|████████▊ | 1635/1850 [1:49:02<14:20,  4.00s/it]
 88%|████████▊ | 1636/1850 [1:49:05<14:03,  3.94s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02978515625, 'learning_rate': 1.1567567567567568e-05, 'epoch': 44.22}

 88%|████████▊ | 1636/1850 [1:49:05<14:03,  3.94s/it]
 88%|████████▊ | 1637/1850 [1:49:10<14:25,  4.06s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0322265625, 'learning_rate': 1.1513513513513514e-05, 'epoch': 44.24}

 88%|████████▊ | 1637/1850 [1:49:10<14:25,  4.06s/it]
 89%|████████▊ | 1638/1850 [1:49:14<14:29,  4.10s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 1.145945945945946e-05, 'epoch': 44.27}

 89%|████████▊ | 1638/1850 [1:49:14<14:29,  4.10s/it]
 89%|████████▊ | 1639/1850 [1:49:18<14:02,  3.99s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.029541015625, 'learning_rate': 1.1405405405405406e-05, 'epoch': 44.3}

 89%|████████▊ | 1639/1850 [1:49:18<14:02,  3.99s/it]
 89%|████████▊ | 1640/1850 [1:49:21<13:28,  3.85s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0277099609375, 'learning_rate': 1.1351351351351352e-05, 'epoch': 44.32}

 89%|████████▊ | 1640/1850 [1:49:21<13:28,  3.85s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 89%|████████▊ | 1641/1850 [1:49:25<13:29,  3.87s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0322265625, 'learning_rate': 1.1297297297297298e-05, 'epoch': 44.35}

 89%|████████▊ | 1641/1850 [1:49:25<13:29,  3.87s/it]
 89%|████████▉ | 1642/1850 [1:49:29<13:49,  3.99s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.07763671875, 'learning_rate': 1.1243243243243245e-05, 'epoch': 44.38}

 89%|████████▉ | 1642/1850 [1:49:29<13:49,  3.99s/it]
 89%|████████▉ | 1643/1850 [1:49:34<13:58,  4.05s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0294189453125, 'learning_rate': 1.1189189189189189e-05, 'epoch': 44.41}

 89%|████████▉ | 1643/1850 [1:49:34<13:58,  4.05s/it]
 89%|████████▉ | 1644/1850 [1:49:38<14:05,  4.11s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0296630859375, 'learning_rate': 1.1135135135135135e-05, 'epoch': 44.43}

 89%|████████▉ | 1644/1850 [1:49:38<14:05,  4.11s/it]
 89%|████████▉ | 1645/1850 [1:49:42<13:51,  4.06s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0296630859375, 'learning_rate': 1.1081081081081083e-05, 'epoch': 44.46}

 89%|████████▉ | 1645/1850 [1:49:42<13:51,  4.06s/it]
 89%|████████▉ | 1646/1850 [1:49:46<14:24,  4.24s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.031982421875, 'learning_rate': 1.1027027027027027e-05, 'epoch': 44.49}

 89%|████████▉ | 1646/1850 [1:49:46<14:24,  4.24s/it]
 89%|████████▉ | 1647/1850 [1:49:50<13:56,  4.12s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02880859375, 'learning_rate': 1.0972972972972974e-05, 'epoch': 44.51}

 89%|████████▉ | 1647/1850 [1:49:50<13:56,  4.12s/it]
 89%|████████▉ | 1648/1850 [1:49:55<14:28,  4.30s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031982421875, 'learning_rate': 1.091891891891892e-05, 'epoch': 44.54}

 89%|████████▉ | 1648/1850 [1:49:55<14:28,  4.30s/it]
 89%|████████▉ | 1649/1850 [1:49:59<14:02,  4.19s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02880859375, 'learning_rate': 1.0864864864864866e-05, 'epoch': 44.57}

 89%|████████▉ | 1649/1850 [1:49:59<14:02,  4.19s/it]
 89%|████████▉ | 1650/1850 [1:50:03<13:48,  4.14s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0250244140625, 'learning_rate': 1.0810810810810812e-05, 'epoch': 44.59}

 89%|████████▉ | 1650/1850 [1:50:03<13:48,  4.14s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 89%|████████▉ | 1651/1850 [1:50:06<12:59,  3.92s/it]
                                                     
{'loss': 0.0005, 'grad_norm': 0.0302734375, 'learning_rate': 1.0756756756756757e-05, 'epoch': 44.62}

 89%|████████▉ | 1651/1850 [1:50:06<12:59,  3.92s/it]
 89%|████████▉ | 1652/1850 [1:50:10<12:34,  3.81s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02978515625, 'learning_rate': 1.0702702702702704e-05, 'epoch': 44.65}

 89%|████████▉ | 1652/1850 [1:50:10<12:34,  3.81s/it]
 89%|████████▉ | 1653/1850 [1:50:14<12:55,  3.94s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.030029296875, 'learning_rate': 1.0648648648648649e-05, 'epoch': 44.68}

 89%|████████▉ | 1653/1850 [1:50:14<12:55,  3.94s/it]
 89%|████████▉ | 1654/1850 [1:50:18<12:36,  3.86s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02978515625, 'learning_rate': 1.0594594594594595e-05, 'epoch': 44.7}

 89%|████████▉ | 1654/1850 [1:50:18<12:36,  3.86s/it]
 89%|████████▉ | 1655/1850 [1:50:21<11:59,  3.69s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.1552734375, 'learning_rate': 1.0540540540540541e-05, 'epoch': 44.73}

 89%|████████▉ | 1655/1850 [1:50:21<11:59,  3.69s/it]
 90%|████████▉ | 1656/1850 [1:50:25<12:24,  3.84s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.030517578125, 'learning_rate': 1.0486486486486487e-05, 'epoch': 44.76}

 90%|████████▉ | 1656/1850 [1:50:25<12:24,  3.84s/it]
 90%|████████▉ | 1657/1850 [1:50:30<13:11,  4.10s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0269775390625, 'learning_rate': 1.0432432432432433e-05, 'epoch': 44.78}

 90%|████████▉ | 1657/1850 [1:50:30<13:11,  4.10s/it]
 90%|████████▉ | 1658/1850 [1:50:34<12:44,  3.98s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0242919921875, 'learning_rate': 1.0378378378378378e-05, 'epoch': 44.81}

 90%|████████▉ | 1658/1850 [1:50:34<12:44,  3.98s/it]
 90%|████████▉ | 1659/1850 [1:50:38<12:51,  4.04s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0291748046875, 'learning_rate': 1.0324324324324326e-05, 'epoch': 44.84}

 90%|████████▉ | 1659/1850 [1:50:38<12:51,  4.04s/it]
 90%|████████▉ | 1660/1850 [1:50:41<12:13,  3.86s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0286865234375, 'learning_rate': 1.0270270270270272e-05, 'epoch': 44.86}

 90%|████████▉ | 1660/1850 [1:50:41<12:13,  3.86s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 90%|████████▉ | 1661/1850 [1:50:45<12:09,  3.86s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.02783203125, 'learning_rate': 1.0216216216216216e-05, 'epoch': 44.89}

 90%|████████▉ | 1661/1850 [1:50:45<12:09,  3.86s/it]
 90%|████████▉ | 1662/1850 [1:50:49<11:50,  3.78s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02978515625, 'learning_rate': 1.0162162162162162e-05, 'epoch': 44.92}

 90%|████████▉ | 1662/1850 [1:50:49<11:50,  3.78s/it]
 90%|████████▉ | 1663/1850 [1:50:53<12:09,  3.90s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0286865234375, 'learning_rate': 1.0108108108108109e-05, 'epoch': 44.95}

 90%|████████▉ | 1663/1850 [1:50:53<12:09,  3.90s/it]
 90%|████████▉ | 1664/1850 [1:50:57<11:48,  3.81s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.028564453125, 'learning_rate': 1.0054054054054055e-05, 'epoch': 44.97}

 90%|████████▉ | 1664/1850 [1:50:57<11:48,  3.81s/it]
 90%|█████████ | 1665/1850 [1:51:01<12:12,  3.96s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.03271484375, 'learning_rate': 1e-05, 'epoch': 45.0}

 90%|█████████ | 1665/1850 [1:51:01<12:12,  3.96s/it]
 90%|█████████ | 1666/1850 [1:51:04<11:19,  3.69s/it]
                                                     
{'loss': 0.0005, 'grad_norm': 0.02685546875, 'learning_rate': 9.945945945945947e-06, 'epoch': 45.03}

 90%|█████████ | 1666/1850 [1:51:04<11:19,  3.69s/it]
 90%|█████████ | 1667/1850 [1:51:08<11:12,  3.68s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0284423828125, 'learning_rate': 9.891891891891893e-06, 'epoch': 45.05}

 90%|█████████ | 1667/1850 [1:51:08<11:12,  3.68s/it]
 90%|█████████ | 1668/1850 [1:51:12<12:04,  3.98s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.029541015625, 'learning_rate': 9.837837837837838e-06, 'epoch': 45.08}

 90%|█████████ | 1668/1850 [1:51:12<12:04,  3.98s/it]
 90%|█████████ | 1669/1850 [1:51:16<11:57,  3.96s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.023193359375, 'learning_rate': 9.783783783783784e-06, 'epoch': 45.11}

 90%|█████████ | 1669/1850 [1:51:16<11:57,  3.96s/it]
 90%|█████████ | 1670/1850 [1:51:20<12:07,  4.04s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0341796875, 'learning_rate': 9.729729729729732e-06, 'epoch': 45.14}

 90%|█████████ | 1670/1850 [1:51:20<12:07,  4.04s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 90%|█████████ | 1671/1850 [1:51:24<11:41,  3.92s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0255126953125, 'learning_rate': 9.675675675675676e-06, 'epoch': 45.16}

 90%|█████████ | 1671/1850 [1:51:24<11:41,  3.92s/it]
 90%|█████████ | 1672/1850 [1:51:27<11:05,  3.74s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0286865234375, 'learning_rate': 9.621621621621622e-06, 'epoch': 45.19}

 90%|█████████ | 1672/1850 [1:51:27<11:05,  3.74s/it]
 90%|█████████ | 1673/1850 [1:51:31<11:12,  3.80s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0303955078125, 'learning_rate': 9.567567567567568e-06, 'epoch': 45.22}

 90%|█████████ | 1673/1850 [1:51:31<11:12,  3.80s/it]
 90%|█████████ | 1674/1850 [1:51:36<11:50,  4.04s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.0299072265625, 'learning_rate': 9.513513513513514e-06, 'epoch': 45.24}

 90%|█████████ | 1674/1850 [1:51:36<11:50,  4.04s/it]
 91%|█████████ | 1675/1850 [1:51:40<11:56,  4.09s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0306396484375, 'learning_rate': 9.45945945945946e-06, 'epoch': 45.27}

 91%|█████████ | 1675/1850 [1:51:40<11:56,  4.09s/it]
 91%|█████████ | 1676/1850 [1:51:44<11:50,  4.08s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0257568359375, 'learning_rate': 9.405405405405405e-06, 'epoch': 45.3}

 91%|█████████ | 1676/1850 [1:51:44<11:50,  4.08s/it]
 91%|█████████ | 1677/1850 [1:51:48<11:44,  4.07s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0302734375, 'learning_rate': 9.351351351351353e-06, 'epoch': 45.32}

 91%|█████████ | 1677/1850 [1:51:48<11:44,  4.07s/it]
 91%|█████████ | 1678/1850 [1:51:52<11:11,  3.90s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.1220703125, 'learning_rate': 9.297297297297297e-06, 'epoch': 45.35}

 91%|█████████ | 1678/1850 [1:51:52<11:11,  3.90s/it]
 91%|█████████ | 1679/1850 [1:51:55<10:53,  3.82s/it]
                                                     
{'loss': 0.0015, 'grad_norm': 0.125, 'learning_rate': 9.243243243243243e-06, 'epoch': 45.38}

 91%|█████████ | 1679/1850 [1:51:55<10:53,  3.82s/it]
 91%|█████████ | 1680/1850 [1:51:59<10:56,  3.86s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0296630859375, 'learning_rate': 9.18918918918919e-06, 'epoch': 45.41}

 91%|█████████ | 1680/1850 [1:51:59<10:56,  3.86s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 91%|█████████ | 1681/1850 [1:52:03<10:48,  3.84s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.024658203125, 'learning_rate': 9.135135135135136e-06, 'epoch': 45.43}

 91%|█████████ | 1681/1850 [1:52:03<10:48,  3.84s/it]
 91%|█████████ | 1682/1850 [1:52:07<10:36,  3.79s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.031982421875, 'learning_rate': 9.081081081081082e-06, 'epoch': 45.46}

 91%|█████████ | 1682/1850 [1:52:07<10:36,  3.79s/it]
 91%|█████████ | 1683/1850 [1:52:11<11:05,  3.99s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0306396484375, 'learning_rate': 9.027027027027026e-06, 'epoch': 45.49}

 91%|█████████ | 1683/1850 [1:52:11<11:05,  3.99s/it]
 91%|█████████ | 1684/1850 [1:52:15<11:11,  4.05s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03271484375, 'learning_rate': 8.972972972972974e-06, 'epoch': 45.51}

 91%|█████████ | 1684/1850 [1:52:15<11:11,  4.05s/it]
 91%|█████████ | 1685/1850 [1:52:19<11:02,  4.02s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0294189453125, 'learning_rate': 8.91891891891892e-06, 'epoch': 45.54}

 91%|█████████ | 1685/1850 [1:52:19<11:02,  4.02s/it]
 91%|█████████ | 1686/1850 [1:52:24<11:33,  4.23s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.0341796875, 'learning_rate': 8.864864864864865e-06, 'epoch': 45.57}

 91%|█████████ | 1686/1850 [1:52:24<11:33,  4.23s/it]
 91%|█████████ | 1687/1850 [1:52:29<11:37,  4.28s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0322265625, 'learning_rate': 8.810810810810811e-06, 'epoch': 45.59}

 91%|█████████ | 1687/1850 [1:52:29<11:37,  4.28s/it]
 91%|█████████ | 1688/1850 [1:52:33<11:26,  4.24s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.026611328125, 'learning_rate': 8.756756756756757e-06, 'epoch': 45.62}

 91%|█████████ | 1688/1850 [1:52:33<11:26,  4.24s/it]
 91%|█████████▏| 1689/1850 [1:52:36<10:52,  4.05s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0296630859375, 'learning_rate': 8.702702702702703e-06, 'epoch': 45.65}

 91%|█████████▏| 1689/1850 [1:52:36<10:52,  4.05s/it]
 91%|█████████▏| 1690/1850 [1:52:40<10:30,  3.94s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.031494140625, 'learning_rate': 8.64864864864865e-06, 'epoch': 45.68}

 91%|█████████▏| 1690/1850 [1:52:40<10:30,  3.94s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 91%|█████████▏| 1691/1850 [1:52:44<10:52,  4.10s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.030517578125, 'learning_rate': 8.594594594594595e-06, 'epoch': 45.7}

 91%|█████████▏| 1691/1850 [1:52:44<10:52,  4.10s/it]
 91%|█████████▏| 1692/1850 [1:52:49<10:46,  4.09s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.032470703125, 'learning_rate': 8.540540540540542e-06, 'epoch': 45.73}

 91%|█████████▏| 1692/1850 [1:52:49<10:46,  4.09s/it]
 92%|█████████▏| 1693/1850 [1:52:53<10:44,  4.11s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0242919921875, 'learning_rate': 8.486486486486486e-06, 'epoch': 45.76}

 92%|█████████▏| 1693/1850 [1:52:53<10:44,  4.11s/it]
 92%|█████████▏| 1694/1850 [1:52:57<10:58,  4.22s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.032958984375, 'learning_rate': 8.432432432432432e-06, 'epoch': 45.78}

 92%|█████████▏| 1694/1850 [1:52:57<10:58,  4.22s/it]
 92%|█████████▏| 1695/1850 [1:53:01<10:33,  4.09s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.02587890625, 'learning_rate': 8.37837837837838e-06, 'epoch': 45.81}

 92%|█████████▏| 1695/1850 [1:53:01<10:33,  4.09s/it]
 92%|█████████▏| 1696/1850 [1:53:05<10:21,  4.04s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0279541015625, 'learning_rate': 8.324324324324324e-06, 'epoch': 45.84}

 92%|█████████▏| 1696/1850 [1:53:05<10:21,  4.04s/it]
 92%|█████████▏| 1697/1850 [1:53:09<10:39,  4.18s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03125, 'learning_rate': 8.27027027027027e-06, 'epoch': 45.86}

 92%|█████████▏| 1697/1850 [1:53:09<10:39,  4.18s/it]
 92%|█████████▏| 1698/1850 [1:53:13<10:31,  4.16s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03369140625, 'learning_rate': 8.216216216216217e-06, 'epoch': 45.89}

 92%|█████████▏| 1698/1850 [1:53:13<10:31,  4.16s/it]
 92%|█████████▏| 1699/1850 [1:53:18<10:35,  4.21s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03173828125, 'learning_rate': 8.162162162162163e-06, 'epoch': 45.92}

 92%|█████████▏| 1699/1850 [1:53:18<10:35,  4.21s/it]
 92%|█████████▏| 1700/1850 [1:53:22<10:16,  4.11s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031005859375, 'learning_rate': 8.108108108108109e-06, 'epoch': 45.95}

 92%|█████████▏| 1700/1850 [1:53:22<10:16,  4.11s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 92%|█████████▏| 1701/1850 [1:53:26<10:20,  4.16s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.022705078125, 'learning_rate': 8.054054054054053e-06, 'epoch': 45.97}

 92%|█████████▏| 1701/1850 [1:53:26<10:20,  4.16s/it]
 92%|█████████▏| 1702/1850 [1:53:29<09:45,  3.95s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03076171875, 'learning_rate': 8.000000000000001e-06, 'epoch': 46.0}

 92%|█████████▏| 1702/1850 [1:53:29<09:45,  3.95s/it]
 92%|█████████▏| 1703/1850 [1:53:34<10:01,  4.09s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033203125, 'learning_rate': 7.945945945945946e-06, 'epoch': 46.03}

 92%|█████████▏| 1703/1850 [1:53:34<10:01,  4.09s/it]
 92%|█████████▏| 1704/1850 [1:53:38<09:52,  4.06s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03076171875, 'learning_rate': 7.891891891891892e-06, 'epoch': 46.05}

 92%|█████████▏| 1704/1850 [1:53:38<09:52,  4.06s/it]
 92%|█████████▏| 1705/1850 [1:53:41<09:31,  3.94s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.029052734375, 'learning_rate': 7.837837837837838e-06, 'epoch': 46.08}

 92%|█████████▏| 1705/1850 [1:53:41<09:31,  3.94s/it]
 92%|█████████▏| 1706/1850 [1:53:46<10:04,  4.20s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03125, 'learning_rate': 7.783783783783784e-06, 'epoch': 46.11}

 92%|█████████▏| 1706/1850 [1:53:46<10:04,  4.20s/it]
 92%|█████████▏| 1707/1850 [1:53:51<10:08,  4.26s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031494140625, 'learning_rate': 7.72972972972973e-06, 'epoch': 46.14}

 92%|█████████▏| 1707/1850 [1:53:51<10:08,  4.26s/it]
 92%|█████████▏| 1708/1850 [1:53:54<09:37,  4.07s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031005859375, 'learning_rate': 7.675675675675675e-06, 'epoch': 46.16}

 92%|█████████▏| 1708/1850 [1:53:54<09:37,  4.07s/it]
 92%|█████████▏| 1709/1850 [1:53:58<09:16,  3.95s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0289306640625, 'learning_rate': 7.621621621621622e-06, 'epoch': 46.19}

 92%|█████████▏| 1709/1850 [1:53:58<09:16,  3.95s/it]
 92%|█████████▏| 1710/1850 [1:54:02<09:07,  3.91s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0299072265625, 'learning_rate': 7.567567567567568e-06, 'epoch': 46.22}

 92%|█████████▏| 1710/1850 [1:54:02<09:07,  3.91s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 92%|█████████▏| 1711/1850 [1:54:06<09:31,  4.11s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031005859375, 'learning_rate': 7.513513513513513e-06, 'epoch': 46.24}

 92%|█████████▏| 1711/1850 [1:54:06<09:31,  4.11s/it]
 93%|█████████▎| 1712/1850 [1:54:11<09:29,  4.12s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.02978515625, 'learning_rate': 7.45945945945946e-06, 'epoch': 46.27}

 93%|█████████▎| 1712/1850 [1:54:11<09:29,  4.12s/it]
 93%|█████████▎| 1713/1850 [1:54:14<09:15,  4.05s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.02587890625, 'learning_rate': 7.405405405405406e-06, 'epoch': 46.3}

 93%|█████████▎| 1713/1850 [1:54:14<09:15,  4.05s/it]
 93%|█████████▎| 1714/1850 [1:54:19<09:36,  4.24s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.0308837890625, 'learning_rate': 7.351351351351352e-06, 'epoch': 46.32}

 93%|█████████▎| 1714/1850 [1:54:19<09:36,  4.24s/it]
 93%|█████████▎| 1715/1850 [1:54:23<09:22,  4.17s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03466796875, 'learning_rate': 7.297297297297298e-06, 'epoch': 46.35}

 93%|█████████▎| 1715/1850 [1:54:23<09:22,  4.17s/it]
 93%|█████████▎| 1716/1850 [1:54:27<09:13,  4.13s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0279541015625, 'learning_rate': 7.243243243243243e-06, 'epoch': 46.38}

 93%|█████████▎| 1716/1850 [1:54:27<09:13,  4.13s/it]
 93%|█████████▎| 1717/1850 [1:54:31<08:56,  4.03s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0274658203125, 'learning_rate': 7.189189189189189e-06, 'epoch': 46.41}

 93%|█████████▎| 1717/1850 [1:54:31<08:56,  4.03s/it]
 93%|█████████▎| 1718/1850 [1:54:35<08:56,  4.06s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0311279296875, 'learning_rate': 7.135135135135136e-06, 'epoch': 46.43}

 93%|█████████▎| 1718/1850 [1:54:35<08:56,  4.06s/it]
 93%|█████████▎| 1719/1850 [1:54:39<08:50,  4.05s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0311279296875, 'learning_rate': 7.0810810810810815e-06, 'epoch': 46.46}

 93%|█████████▎| 1719/1850 [1:54:39<08:50,  4.05s/it]
 93%|█████████▎| 1720/1850 [1:54:43<08:27,  3.90s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03173828125, 'learning_rate': 7.027027027027028e-06, 'epoch': 46.49}

 93%|█████████▎| 1720/1850 [1:54:43<08:27,  3.90s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 93%|█████████▎| 1721/1850 [1:54:47<08:37,  4.02s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.026611328125, 'learning_rate': 6.972972972972973e-06, 'epoch': 46.51}

 93%|█████████▎| 1721/1850 [1:54:47<08:37,  4.02s/it]
 93%|█████████▎| 1722/1850 [1:54:50<08:13,  3.86s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.12890625, 'learning_rate': 6.918918918918919e-06, 'epoch': 46.54}

 93%|█████████▎| 1722/1850 [1:54:50<08:13,  3.86s/it]
 93%|█████████▎| 1723/1850 [1:54:55<08:19,  3.94s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02783203125, 'learning_rate': 6.864864864864866e-06, 'epoch': 46.57}

 93%|█████████▎| 1723/1850 [1:54:55<08:19,  3.94s/it]
 93%|█████████▎| 1724/1850 [1:54:58<08:17,  3.95s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.030029296875, 'learning_rate': 6.8108108108108105e-06, 'epoch': 46.59}

 93%|█████████▎| 1724/1850 [1:54:59<08:17,  3.95s/it]
 93%|█████████▎| 1725/1850 [1:55:02<07:48,  3.75s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.025390625, 'learning_rate': 6.7567567567567575e-06, 'epoch': 46.62}

 93%|█████████▎| 1725/1850 [1:55:02<07:48,  3.75s/it]
 93%|█████████▎| 1726/1850 [1:55:06<07:54,  3.83s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0269775390625, 'learning_rate': 6.702702702702703e-06, 'epoch': 46.65}

 93%|█████████▎| 1726/1850 [1:55:06<07:54,  3.83s/it]
 93%|█████████▎| 1727/1850 [1:55:10<08:13,  4.02s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03173828125, 'learning_rate': 6.648648648648649e-06, 'epoch': 46.68}

 93%|█████████▎| 1727/1850 [1:55:10<08:13,  4.02s/it]
 93%|█████████▎| 1728/1850 [1:55:14<08:14,  4.06s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0303955078125, 'learning_rate': 6.594594594594596e-06, 'epoch': 46.7}

 93%|█████████▎| 1728/1850 [1:55:14<08:14,  4.06s/it]
 93%|█████████▎| 1729/1850 [1:55:18<08:01,  3.98s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031982421875, 'learning_rate': 6.54054054054054e-06, 'epoch': 46.73}

 93%|█████████▎| 1729/1850 [1:55:18<08:01,  3.98s/it]
 94%|█████████▎| 1730/1850 [1:55:23<08:11,  4.10s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0296630859375, 'learning_rate': 6.486486486486487e-06, 'epoch': 46.76}

 94%|█████████▎| 1730/1850 [1:55:23<08:11,  4.10s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 94%|█████████▎| 1731/1850 [1:55:27<08:18,  4.19s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.027099609375, 'learning_rate': 6.432432432432432e-06, 'epoch': 46.78}

 94%|█████████▎| 1731/1850 [1:55:27<08:18,  4.19s/it]
 94%|█████████▎| 1732/1850 [1:55:31<07:55,  4.03s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0283203125, 'learning_rate': 6.378378378378379e-06, 'epoch': 46.81}

 94%|█████████▎| 1732/1850 [1:55:31<07:55,  4.03s/it]
 94%|█████████▎| 1733/1850 [1:55:34<07:45,  3.98s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.02490234375, 'learning_rate': 6.324324324324325e-06, 'epoch': 46.84}

 94%|█████████▎| 1733/1850 [1:55:34<07:45,  3.98s/it]
 94%|█████████▎| 1734/1850 [1:55:38<07:40,  3.97s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03076171875, 'learning_rate': 6.27027027027027e-06, 'epoch': 46.86}

 94%|█████████▎| 1734/1850 [1:55:38<07:40,  3.97s/it]
 94%|█████████▍| 1735/1850 [1:55:43<08:03,  4.21s/it]
                                                     
{'loss': 0.0015, 'grad_norm': 0.1015625, 'learning_rate': 6.216216216216217e-06, 'epoch': 46.89}

 94%|█████████▍| 1735/1850 [1:55:43<08:03,  4.21s/it]
 94%|█████████▍| 1736/1850 [1:55:48<08:04,  4.25s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.030029296875, 'learning_rate': 6.1621621621621626e-06, 'epoch': 46.92}

 94%|█████████▍| 1736/1850 [1:55:48<08:04,  4.25s/it]
 94%|█████████▍| 1737/1850 [1:55:51<07:41,  4.08s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.030517578125, 'learning_rate': 6.108108108108109e-06, 'epoch': 46.95}

 94%|█████████▍| 1737/1850 [1:55:51<07:41,  4.08s/it]
 94%|█████████▍| 1738/1850 [1:55:55<07:27,  3.99s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03076171875, 'learning_rate': 6.054054054054054e-06, 'epoch': 46.97}

 94%|█████████▍| 1738/1850 [1:55:55<07:27,  3.99s/it]
 94%|█████████▍| 1739/1850 [1:55:59<07:31,  4.06s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.034423828125, 'learning_rate': 6e-06, 'epoch': 47.0}

 94%|█████████▍| 1739/1850 [1:55:59<07:31,  4.06s/it]
 94%|█████████▍| 1740/1850 [1:56:03<07:20,  4.01s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03173828125, 'learning_rate': 5.945945945945946e-06, 'epoch': 47.03}

 94%|█████████▍| 1740/1850 [1:56:03<07:20,  4.01s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 94%|█████████▍| 1741/1850 [1:56:08<07:32,  4.15s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0361328125, 'learning_rate': 5.8918918918918924e-06, 'epoch': 47.05}

 94%|█████████▍| 1741/1850 [1:56:08<07:32,  4.15s/it]
 94%|█████████▍| 1742/1850 [1:56:12<07:25,  4.12s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031005859375, 'learning_rate': 5.837837837837839e-06, 'epoch': 47.08}

 94%|█████████▍| 1742/1850 [1:56:12<07:25,  4.12s/it]
 94%|█████████▍| 1743/1850 [1:56:15<07:06,  3.98s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.08935546875, 'learning_rate': 5.783783783783784e-06, 'epoch': 47.11}

 94%|█████████▍| 1743/1850 [1:56:15<07:06,  3.98s/it]
 94%|█████████▍| 1744/1850 [1:56:19<07:03,  4.00s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.028564453125, 'learning_rate': 5.72972972972973e-06, 'epoch': 47.14}

 94%|█████████▍| 1744/1850 [1:56:19<07:03,  4.00s/it]
 94%|█████████▍| 1745/1850 [1:56:23<06:41,  3.82s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.025146484375, 'learning_rate': 5.675675675675676e-06, 'epoch': 47.16}

 94%|█████████▍| 1745/1850 [1:56:23<06:41,  3.82s/it]
 94%|█████████▍| 1746/1850 [1:56:26<06:24,  3.70s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.03173828125, 'learning_rate': 5.621621621621622e-06, 'epoch': 47.19}

 94%|█████████▍| 1746/1850 [1:56:26<06:24,  3.70s/it]
 94%|█████████▍| 1747/1850 [1:56:30<06:30,  3.79s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029296875, 'learning_rate': 5.567567567567568e-06, 'epoch': 47.22}

 94%|█████████▍| 1747/1850 [1:56:30<06:30,  3.79s/it]
 94%|█████████▍| 1748/1850 [1:56:35<06:44,  3.97s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0301513671875, 'learning_rate': 5.513513513513514e-06, 'epoch': 47.24}

 94%|█████████▍| 1748/1850 [1:56:35<06:44,  3.97s/it]
 95%|█████████▍| 1749/1850 [1:56:39<06:46,  4.03s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02978515625, 'learning_rate': 5.45945945945946e-06, 'epoch': 47.27}

 95%|█████████▍| 1749/1850 [1:56:39<06:46,  4.03s/it]
 95%|█████████▍| 1750/1850 [1:56:42<06:22,  3.82s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.033203125, 'learning_rate': 5.405405405405406e-06, 'epoch': 47.3}

 95%|█████████▍| 1750/1850 [1:56:42<06:22,  3.82s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 95%|█████████▍| 1751/1850 [1:56:47<06:38,  4.03s/it]
                                                     
{'loss': 0.0017, 'grad_norm': 0.140625, 'learning_rate': 5.351351351351352e-06, 'epoch': 47.32}

 95%|█████████▍| 1751/1850 [1:56:47<06:38,  4.03s/it]
 95%|█████████▍| 1752/1850 [1:56:51<06:35,  4.04s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.029052734375, 'learning_rate': 5.2972972972972975e-06, 'epoch': 47.35}

 95%|█████████▍| 1752/1850 [1:56:51<06:35,  4.04s/it]
 95%|█████████▍| 1753/1850 [1:56:55<06:41,  4.14s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.030517578125, 'learning_rate': 5.243243243243244e-06, 'epoch': 47.38}

 95%|█████████▍| 1753/1850 [1:56:55<06:41,  4.14s/it]
 95%|█████████▍| 1754/1850 [1:56:59<06:29,  4.05s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033203125, 'learning_rate': 5.189189189189189e-06, 'epoch': 47.41}

 95%|█████████▍| 1754/1850 [1:56:59<06:29,  4.05s/it]
 95%|█████████▍| 1755/1850 [1:57:03<06:31,  4.12s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0301513671875, 'learning_rate': 5.135135135135136e-06, 'epoch': 47.43}

 95%|█████████▍| 1755/1850 [1:57:03<06:31,  4.12s/it]
 95%|█████████▍| 1756/1850 [1:57:07<06:07,  3.91s/it]
                                                     
{'loss': 0.0005, 'grad_norm': 0.024658203125, 'learning_rate': 5.081081081081081e-06, 'epoch': 47.46}

 95%|█████████▍| 1756/1850 [1:57:07<06:07,  3.91s/it]
 95%|█████████▍| 1757/1850 [1:57:11<06:20,  4.09s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 5.027027027027027e-06, 'epoch': 47.49}

 95%|█████████▍| 1757/1850 [1:57:11<06:20,  4.09s/it]
 95%|█████████▌| 1758/1850 [1:57:16<06:28,  4.23s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033203125, 'learning_rate': 4.9729729729729735e-06, 'epoch': 47.51}

 95%|█████████▌| 1758/1850 [1:57:16<06:28,  4.23s/it]
 95%|█████████▌| 1759/1850 [1:57:20<06:17,  4.15s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0322265625, 'learning_rate': 4.918918918918919e-06, 'epoch': 47.54}

 95%|█████████▌| 1759/1850 [1:57:20<06:17,  4.15s/it]
 95%|█████████▌| 1760/1850 [1:57:23<05:54,  3.94s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0262451171875, 'learning_rate': 4.864864864864866e-06, 'epoch': 47.57}

 95%|█████████▌| 1760/1850 [1:57:23<05:54,  3.94s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 95%|█████████▌| 1761/1850 [1:57:27<05:56,  4.01s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0306396484375, 'learning_rate': 4.810810810810811e-06, 'epoch': 47.59}

 95%|█████████▌| 1761/1850 [1:57:27<05:56,  4.01s/it]
 95%|█████████▌| 1762/1850 [1:57:31<05:40,  3.87s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0250244140625, 'learning_rate': 4.756756756756757e-06, 'epoch': 47.62}

 95%|█████████▌| 1762/1850 [1:57:31<05:40,  3.87s/it]
 95%|█████████▌| 1763/1850 [1:57:35<05:51,  4.04s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.0302734375, 'learning_rate': 4.7027027027027025e-06, 'epoch': 47.65}

 95%|█████████▌| 1763/1850 [1:57:35<05:51,  4.04s/it]
 95%|█████████▌| 1764/1850 [1:57:39<05:35,  3.91s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.028564453125, 'learning_rate': 4.648648648648649e-06, 'epoch': 47.68}

 95%|█████████▌| 1764/1850 [1:57:39<05:35,  3.91s/it]
 95%|█████████▌| 1765/1850 [1:57:44<06:08,  4.34s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.031982421875, 'learning_rate': 4.594594594594595e-06, 'epoch': 47.7}

 95%|█████████▌| 1765/1850 [1:57:44<06:08,  4.34s/it]
 95%|█████████▌| 1766/1850 [1:57:47<05:39,  4.04s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0311279296875, 'learning_rate': 4.540540540540541e-06, 'epoch': 47.73}

 95%|█████████▌| 1766/1850 [1:57:47<05:39,  4.04s/it]
 96%|█████████▌| 1767/1850 [1:57:51<05:18,  3.84s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.0244140625, 'learning_rate': 4.486486486486487e-06, 'epoch': 47.76}

 96%|█████████▌| 1767/1850 [1:57:51<05:18,  3.84s/it]
 96%|█████████▌| 1768/1850 [1:57:55<05:17,  3.87s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.03125, 'learning_rate': 4.432432432432432e-06, 'epoch': 47.78}

 96%|█████████▌| 1768/1850 [1:57:55<05:17,  3.87s/it]
 96%|█████████▌| 1769/1850 [1:57:58<05:04,  3.76s/it]
                                                     
{'loss': 0.0005, 'grad_norm': 0.0260009765625, 'learning_rate': 4.3783783783783785e-06, 'epoch': 47.81}

 96%|█████████▌| 1769/1850 [1:57:58<05:04,  3.76s/it]
 96%|█████████▌| 1770/1850 [1:58:02<05:08,  3.86s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03173828125, 'learning_rate': 4.324324324324325e-06, 'epoch': 47.84}

 96%|█████████▌| 1770/1850 [1:58:02<05:08,  3.86s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 96%|█████████▌| 1771/1850 [1:58:06<05:02,  3.82s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0257568359375, 'learning_rate': 4.270270270270271e-06, 'epoch': 47.86}

 96%|█████████▌| 1771/1850 [1:58:06<05:02,  3.82s/it]
 96%|█████████▌| 1772/1850 [1:58:10<05:06,  3.93s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033203125, 'learning_rate': 4.216216216216216e-06, 'epoch': 47.89}

 96%|█████████▌| 1772/1850 [1:58:10<05:06,  3.93s/it]
 96%|█████████▌| 1773/1850 [1:58:14<05:03,  3.94s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0311279296875, 'learning_rate': 4.162162162162162e-06, 'epoch': 47.92}

 96%|█████████▌| 1773/1850 [1:58:14<05:03,  3.94s/it]
 96%|█████████▌| 1774/1850 [1:58:18<04:57,  3.92s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0296630859375, 'learning_rate': 4.108108108108108e-06, 'epoch': 47.95}

 96%|█████████▌| 1774/1850 [1:58:18<04:57,  3.92s/it]
 96%|█████████▌| 1775/1850 [1:58:22<04:49,  3.86s/it]
                                                     
{'loss': 0.0005, 'grad_norm': 0.0240478515625, 'learning_rate': 4.0540540540540545e-06, 'epoch': 47.97}

 96%|█████████▌| 1775/1850 [1:58:22<04:49,  3.86s/it]
 96%|█████████▌| 1776/1850 [1:58:25<04:39,  3.78s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03125, 'learning_rate': 4.000000000000001e-06, 'epoch': 48.0}

 96%|█████████▌| 1776/1850 [1:58:25<04:39,  3.78s/it]
 96%|█████████▌| 1777/1850 [1:58:30<04:53,  4.02s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.031494140625, 'learning_rate': 3.945945945945946e-06, 'epoch': 48.03}

 96%|█████████▌| 1777/1850 [1:58:30<04:53,  4.02s/it]
 96%|█████████▌| 1778/1850 [1:58:34<04:42,  3.93s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03076171875, 'learning_rate': 3.891891891891892e-06, 'epoch': 48.05}

 96%|█████████▌| 1778/1850 [1:58:34<04:42,  3.93s/it]
 96%|█████████▌| 1779/1850 [1:58:39<04:56,  4.18s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0322265625, 'learning_rate': 3.837837837837837e-06, 'epoch': 48.08}

 96%|█████████▌| 1779/1850 [1:58:39<04:56,  4.18s/it]
 96%|█████████▌| 1780/1850 [1:58:42<04:47,  4.11s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.027099609375, 'learning_rate': 3.783783783783784e-06, 'epoch': 48.11}

 96%|█████████▌| 1780/1850 [1:58:42<04:47,  4.11s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 96%|█████████▋| 1781/1850 [1:58:47<04:42,  4.09s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0291748046875, 'learning_rate': 3.72972972972973e-06, 'epoch': 48.14}

 96%|█████████▋| 1781/1850 [1:58:47<04:42,  4.09s/it]
 96%|█████████▋| 1782/1850 [1:58:51<04:38,  4.10s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0289306640625, 'learning_rate': 3.675675675675676e-06, 'epoch': 48.16}

 96%|█████████▋| 1782/1850 [1:58:51<04:38,  4.10s/it]
 96%|█████████▋| 1783/1850 [1:58:54<04:29,  4.02s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0269775390625, 'learning_rate': 3.6216216216216215e-06, 'epoch': 48.19}

 96%|█████████▋| 1783/1850 [1:58:54<04:29,  4.02s/it]
 96%|█████████▋| 1784/1850 [1:58:59<04:27,  4.05s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0296630859375, 'learning_rate': 3.567567567567568e-06, 'epoch': 48.22}

 96%|█████████▋| 1784/1850 [1:58:59<04:27,  4.05s/it]
 96%|█████████▋| 1785/1850 [1:59:03<04:22,  4.03s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0299072265625, 'learning_rate': 3.513513513513514e-06, 'epoch': 48.24}

 96%|█████████▋| 1785/1850 [1:59:03<04:22,  4.03s/it]
 97%|█████████▋| 1786/1850 [1:59:07<04:18,  4.05s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0303955078125, 'learning_rate': 3.4594594594594596e-06, 'epoch': 48.27}

 97%|█████████▋| 1786/1850 [1:59:07<04:18,  4.05s/it]
 97%|█████████▋| 1787/1850 [1:59:10<04:06,  3.91s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02783203125, 'learning_rate': 3.4054054054054053e-06, 'epoch': 48.3}

 97%|█████████▋| 1787/1850 [1:59:10<04:06,  3.91s/it]
 97%|█████████▋| 1788/1850 [1:59:14<04:07,  3.98s/it]
                                                     
{'loss': 0.0017, 'grad_norm': 0.16015625, 'learning_rate': 3.3513513513513514e-06, 'epoch': 48.32}

 97%|█████████▋| 1788/1850 [1:59:14<04:07,  3.98s/it]
 97%|█████████▋| 1789/1850 [1:59:19<04:10,  4.11s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0284423828125, 'learning_rate': 3.297297297297298e-06, 'epoch': 48.35}

 97%|█████████▋| 1789/1850 [1:59:19<04:10,  4.11s/it]
 97%|█████████▋| 1790/1850 [1:59:23<04:12,  4.20s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.028564453125, 'learning_rate': 3.2432432432432437e-06, 'epoch': 48.38}

 97%|█████████▋| 1790/1850 [1:59:23<04:12,  4.20s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 97%|█████████▋| 1791/1850 [1:59:28<04:26,  4.51s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 3.1891891891891894e-06, 'epoch': 48.41}

 97%|█████████▋| 1791/1850 [1:59:28<04:26,  4.51s/it]
 97%|█████████▋| 1792/1850 [1:59:32<04:10,  4.33s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.02685546875, 'learning_rate': 3.135135135135135e-06, 'epoch': 48.43}

 97%|█████████▋| 1792/1850 [1:59:32<04:10,  4.33s/it]
 97%|█████████▋| 1793/1850 [1:59:36<03:48,  4.01s/it]
                                                     
{'loss': 0.0005, 'grad_norm': 0.023681640625, 'learning_rate': 3.0810810810810813e-06, 'epoch': 48.46}

 97%|█████████▋| 1793/1850 [1:59:36<03:48,  4.01s/it]
 97%|█████████▋| 1794/1850 [1:59:39<03:38,  3.90s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.029052734375, 'learning_rate': 3.027027027027027e-06, 'epoch': 48.49}

 97%|█████████▋| 1794/1850 [1:59:39<03:38,  3.90s/it]
 97%|█████████▋| 1795/1850 [1:59:43<03:31,  3.84s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.032958984375, 'learning_rate': 2.972972972972973e-06, 'epoch': 48.51}

 97%|█████████▋| 1795/1850 [1:59:43<03:31,  3.84s/it]
 97%|█████████▋| 1796/1850 [1:59:47<03:23,  3.77s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0235595703125, 'learning_rate': 2.9189189189189193e-06, 'epoch': 48.54}

 97%|█████████▋| 1796/1850 [1:59:47<03:23,  3.77s/it]
 97%|█████████▋| 1797/1850 [1:59:51<03:28,  3.93s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03271484375, 'learning_rate': 2.864864864864865e-06, 'epoch': 48.57}

 97%|█████████▋| 1797/1850 [1:59:51<03:28,  3.93s/it]
 97%|█████████▋| 1798/1850 [1:59:55<03:23,  3.92s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0283203125, 'learning_rate': 2.810810810810811e-06, 'epoch': 48.59}

 97%|█████████▋| 1798/1850 [1:59:55<03:23,  3.92s/it]
 97%|█████████▋| 1799/1850 [1:59:58<03:15,  3.83s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.1572265625, 'learning_rate': 2.756756756756757e-06, 'epoch': 48.62}

 97%|█████████▋| 1799/1850 [1:59:58<03:15,  3.83s/it]
 97%|█████████▋| 1800/1850 [2:00:03<03:19,  3.99s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.028564453125, 'learning_rate': 2.702702702702703e-06, 'epoch': 48.65}

 97%|█████████▋| 1800/1850 [2:00:03<03:19,  3.99s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 97%|█████████▋| 1801/1850 [2:00:07<03:14,  3.98s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02685546875, 'learning_rate': 2.6486486486486487e-06, 'epoch': 48.68}

 97%|█████████▋| 1801/1850 [2:00:07<03:14,  3.98s/it]
 97%|█████████▋| 1802/1850 [2:00:11<03:13,  4.04s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02783203125, 'learning_rate': 2.5945945945945945e-06, 'epoch': 48.7}

 97%|█████████▋| 1802/1850 [2:00:11<03:13,  4.04s/it]
 97%|█████████▋| 1803/1850 [2:00:15<03:09,  4.03s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.029296875, 'learning_rate': 2.5405405405405406e-06, 'epoch': 48.73}

 97%|█████████▋| 1803/1850 [2:00:15<03:09,  4.03s/it]
 98%|█████████▊| 1804/1850 [2:00:19<03:05,  4.03s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.032470703125, 'learning_rate': 2.4864864864864867e-06, 'epoch': 48.76}

 98%|█████████▊| 1804/1850 [2:00:19<03:05,  4.03s/it]
 98%|█████████▊| 1805/1850 [2:00:23<03:05,  4.12s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.033935546875, 'learning_rate': 2.432432432432433e-06, 'epoch': 48.78}

 98%|█████████▊| 1805/1850 [2:00:23<03:05,  4.12s/it]
 98%|█████████▊| 1806/1850 [2:00:28<03:03,  4.16s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.03125, 'learning_rate': 2.3783783783783786e-06, 'epoch': 48.81}

 98%|█████████▊| 1806/1850 [2:00:28<03:03,  4.16s/it]
 98%|█████████▊| 1807/1850 [2:00:32<02:57,  4.12s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0284423828125, 'learning_rate': 2.3243243243243243e-06, 'epoch': 48.84}

 98%|█████████▊| 1807/1850 [2:00:32<02:57,  4.12s/it]
 98%|█████████▊| 1808/1850 [2:00:36<03:00,  4.31s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.03173828125, 'learning_rate': 2.2702702702702705e-06, 'epoch': 48.86}

 98%|█████████▊| 1808/1850 [2:00:36<03:00,  4.31s/it]
 98%|█████████▊| 1809/1850 [2:00:40<02:45,  4.03s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0296630859375, 'learning_rate': 2.216216216216216e-06, 'epoch': 48.89}

 98%|█████████▊| 1809/1850 [2:00:40<02:45,  4.03s/it]
 98%|█████████▊| 1810/1850 [2:00:43<02:38,  3.97s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0302734375, 'learning_rate': 2.1621621621621623e-06, 'epoch': 48.92}

 98%|█████████▊| 1810/1850 [2:00:43<02:38,  3.97s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 98%|█████████▊| 1811/1850 [2:00:48<02:37,  4.05s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03173828125, 'learning_rate': 2.108108108108108e-06, 'epoch': 48.95}

 98%|█████████▊| 1811/1850 [2:00:48<02:37,  4.05s/it]
 98%|█████████▊| 1812/1850 [2:00:51<02:25,  3.84s/it]
                                                     
{'loss': 0.0006, 'grad_norm': 0.02392578125, 'learning_rate': 2.054054054054054e-06, 'epoch': 48.97}

 98%|█████████▊| 1812/1850 [2:00:51<02:25,  3.84s/it]
 98%|█████████▊| 1813/1850 [2:00:54<02:15,  3.65s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.033203125, 'learning_rate': 2.0000000000000003e-06, 'epoch': 49.0}

 98%|█████████▊| 1813/1850 [2:00:54<02:15,  3.65s/it]
 98%|█████████▊| 1814/1850 [2:00:58<02:17,  3.81s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03173828125, 'learning_rate': 1.945945945945946e-06, 'epoch': 49.03}

 98%|█████████▊| 1814/1850 [2:00:58<02:17,  3.81s/it]
 98%|█████████▊| 1815/1850 [2:01:02<02:14,  3.86s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03125, 'learning_rate': 1.891891891891892e-06, 'epoch': 49.05}

 98%|█████████▊| 1815/1850 [2:01:02<02:14,  3.86s/it]
 98%|█████████▊| 1816/1850 [2:01:07<02:15,  3.99s/it]
                                                     
{'loss': 0.0016, 'grad_norm': 0.1806640625, 'learning_rate': 1.837837837837838e-06, 'epoch': 49.08}

 98%|█████████▊| 1816/1850 [2:01:07<02:15,  3.99s/it]
 98%|█████████▊| 1817/1850 [2:01:11<02:16,  4.14s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.028564453125, 'learning_rate': 1.783783783783784e-06, 'epoch': 49.11}

 98%|█████████▊| 1817/1850 [2:01:11<02:16,  4.14s/it]
 98%|█████████▊| 1818/1850 [2:01:15<02:08,  4.02s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0269775390625, 'learning_rate': 1.7297297297297298e-06, 'epoch': 49.14}

 98%|█████████▊| 1818/1850 [2:01:15<02:08,  4.02s/it]
 98%|█████████▊| 1819/1850 [2:01:19<02:01,  3.93s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0279541015625, 'learning_rate': 1.6756756756756757e-06, 'epoch': 49.16}

 98%|█████████▊| 1819/1850 [2:01:19<02:01,  3.93s/it]
 98%|█████████▊| 1820/1850 [2:01:23<01:58,  3.94s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.028076171875, 'learning_rate': 1.6216216216216219e-06, 'epoch': 49.19}

 98%|█████████▊| 1820/1850 [2:01:23<01:58,  3.94s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 98%|█████████▊| 1821/1850 [2:01:27<02:00,  4.16s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.031494140625, 'learning_rate': 1.5675675675675676e-06, 'epoch': 49.22}

 98%|█████████▊| 1821/1850 [2:01:27<02:00,  4.16s/it]
 98%|█████████▊| 1822/1850 [2:01:31<01:52,  4.03s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0306396484375, 'learning_rate': 1.5135135135135135e-06, 'epoch': 49.24}

 98%|█████████▊| 1822/1850 [2:01:31<01:52,  4.03s/it]
 99%|█████████▊| 1823/1850 [2:01:36<01:52,  4.17s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0289306640625, 'learning_rate': 1.4594594594594596e-06, 'epoch': 49.27}

 99%|█████████▊| 1823/1850 [2:01:36<01:52,  4.17s/it]
 99%|█████████▊| 1824/1850 [2:01:39<01:42,  3.94s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0294189453125, 'learning_rate': 1.4054054054054056e-06, 'epoch': 49.3}

 99%|█████████▊| 1824/1850 [2:01:39<01:42,  3.94s/it]
 99%|█████████▊| 1825/1850 [2:01:43<01:40,  4.01s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.026123046875, 'learning_rate': 1.3513513513513515e-06, 'epoch': 49.32}

 99%|█████████▊| 1825/1850 [2:01:43<01:40,  4.01s/it]
 99%|█████████▊| 1826/1850 [2:01:46<01:31,  3.82s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.029541015625, 'learning_rate': 1.2972972972972972e-06, 'epoch': 49.35}

 99%|█████████▊| 1826/1850 [2:01:46<01:31,  3.82s/it]
 99%|█████████▉| 1827/1850 [2:01:50<01:28,  3.87s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0289306640625, 'learning_rate': 1.2432432432432434e-06, 'epoch': 49.38}

 99%|█████████▉| 1827/1850 [2:01:50<01:28,  3.87s/it]
 99%|█████████▉| 1828/1850 [2:01:55<01:27,  3.99s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0274658203125, 'learning_rate': 1.1891891891891893e-06, 'epoch': 49.41}

 99%|█████████▉| 1828/1850 [2:01:55<01:27,  3.99s/it]
 99%|█████████▉| 1829/1850 [2:01:59<01:27,  4.16s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033203125, 'learning_rate': 1.1351351351351352e-06, 'epoch': 49.43}

 99%|█████████▉| 1829/1850 [2:01:59<01:27,  4.16s/it]
 99%|█████████▉| 1830/1850 [2:02:03<01:21,  4.07s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0267333984375, 'learning_rate': 1.0810810810810812e-06, 'epoch': 49.46}

 99%|█████████▉| 1830/1850 [2:02:03<01:21,  4.07s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

 99%|█████████▉| 1831/1850 [2:02:07<01:18,  4.15s/it]
                                                     
{'loss': 0.0013, 'grad_norm': 0.08837890625, 'learning_rate': 1.027027027027027e-06, 'epoch': 49.49}

 99%|█████████▉| 1831/1850 [2:02:07<01:18,  4.15s/it]
 99%|█████████▉| 1832/1850 [2:02:12<01:17,  4.30s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033447265625, 'learning_rate': 9.72972972972973e-07, 'epoch': 49.51}

 99%|█████████▉| 1832/1850 [2:02:12<01:17,  4.30s/it]
 99%|█████████▉| 1833/1850 [2:02:16<01:10,  4.15s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.03173828125, 'learning_rate': 9.18918918918919e-07, 'epoch': 49.54}

 99%|█████████▉| 1833/1850 [2:02:16<01:10,  4.15s/it]
 99%|█████████▉| 1834/1850 [2:02:20<01:04,  4.06s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.0308837890625, 'learning_rate': 8.648648648648649e-07, 'epoch': 49.57}

 99%|█████████▉| 1834/1850 [2:02:20<01:04,  4.06s/it]
 99%|█████████▉| 1835/1850 [2:02:24<01:00,  4.05s/it]
                                                     
{'loss': 0.0012, 'grad_norm': 0.03173828125, 'learning_rate': 8.108108108108109e-07, 'epoch': 49.59}

 99%|█████████▉| 1835/1850 [2:02:24<01:00,  4.05s/it]
 99%|█████████▉| 1836/1850 [2:02:28<00:55,  3.99s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.029541015625, 'learning_rate': 7.567567567567568e-07, 'epoch': 49.62}

 99%|█████████▉| 1836/1850 [2:02:28<00:55,  3.99s/it]
 99%|█████████▉| 1837/1850 [2:02:31<00:51,  3.94s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0281982421875, 'learning_rate': 7.027027027027028e-07, 'epoch': 49.65}

 99%|█████████▉| 1837/1850 [2:02:31<00:51,  3.94s/it]
 99%|█████████▉| 1838/1850 [2:02:35<00:47,  3.94s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0341796875, 'learning_rate': 6.486486486486486e-07, 'epoch': 49.68}

 99%|█████████▉| 1838/1850 [2:02:35<00:47,  3.94s/it]
 99%|█████████▉| 1839/1850 [2:02:40<00:43,  3.99s/it]
                                                     
{'loss': 0.0011, 'grad_norm': 0.03271484375, 'learning_rate': 5.945945945945947e-07, 'epoch': 49.7}

 99%|█████████▉| 1839/1850 [2:02:40<00:43,  3.99s/it]
 99%|█████████▉| 1840/1850 [2:02:43<00:39,  3.93s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.0281982421875, 'learning_rate': 5.405405405405406e-07, 'epoch': 49.73}

 99%|█████████▉| 1840/1850 [2:02:43<00:39,  3.93s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(

100%|█████████▉| 1841/1850 [2:02:47<00:35,  3.95s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.03125, 'learning_rate': 4.864864864864865e-07, 'epoch': 49.76}

100%|█████████▉| 1841/1850 [2:02:47<00:35,  3.95s/it]
100%|█████████▉| 1842/1850 [2:02:52<00:32,  4.03s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.032470703125, 'learning_rate': 4.3243243243243244e-07, 'epoch': 49.78}

100%|█████████▉| 1842/1850 [2:02:52<00:32,  4.03s/it]
100%|█████████▉| 1843/1850 [2:02:55<00:26,  3.85s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.0283203125, 'learning_rate': 3.783783783783784e-07, 'epoch': 49.81}

100%|█████████▉| 1843/1850 [2:02:55<00:26,  3.85s/it]
100%|█████████▉| 1844/1850 [2:02:59<00:22,  3.77s/it]
                                                     
{'loss': 0.0007, 'grad_norm': 0.029541015625, 'learning_rate': 3.243243243243243e-07, 'epoch': 49.84}

100%|█████████▉| 1844/1850 [2:02:59<00:22,  3.77s/it]
100%|█████████▉| 1845/1850 [2:03:03<00:19,  3.83s/it]
                                                     
{'loss': 0.0009, 'grad_norm': 0.02978515625, 'learning_rate': 2.702702702702703e-07, 'epoch': 49.86}

100%|█████████▉| 1845/1850 [2:03:03<00:19,  3.83s/it]
100%|█████████▉| 1846/1850 [2:03:06<00:14,  3.73s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02880859375, 'learning_rate': 2.1621621621621622e-07, 'epoch': 49.89}

100%|█████████▉| 1846/1850 [2:03:06<00:14,  3.73s/it]
100%|█████████▉| 1847/1850 [2:03:10<00:11,  3.91s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.031494140625, 'learning_rate': 1.6216216216216215e-07, 'epoch': 49.92}

100%|█████████▉| 1847/1850 [2:03:10<00:11,  3.91s/it]
100%|█████████▉| 1848/1850 [2:03:14<00:07,  3.96s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.0299072265625, 'learning_rate': 1.0810810810810811e-07, 'epoch': 49.95}

100%|█████████▉| 1848/1850 [2:03:14<00:07,  3.96s/it]
100%|█████████▉| 1849/1850 [2:03:19<00:04,  4.07s/it]
                                                     
{'loss': 0.001, 'grad_norm': 0.033203125, 'learning_rate': 5.4054054054054056e-08, 'epoch': 49.97}

100%|█████████▉| 1849/1850 [2:03:19<00:04,  4.07s/it]
100%|██████████| 1850/1850 [2:03:22<00:00,  3.87s/it]
                                                     
{'loss': 0.0008, 'grad_norm': 0.02978515625, 'learning_rate': 0.0, 'epoch': 50.0}

100%|██████████| 1850/1850 [2:03:22<00:00,  3.87s/it]/usr/local/lib/python3.8/dist-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in ./qwen/Qwen1___5-7B-Chat/ - will assume that the vocabulary was not modified.
  warnings.warn(

                                                     
{'train_runtime': 7403.0212, 'train_samples_per_second': 3.992, 'train_steps_per_second': 0.25, 'train_loss': 0.13648549079224845, 'epoch': 50.0}

100%|██████████| 1850/1850 [2:03:23<00:00,  3.87s/it]
100%|██████████| 1850/1850 [2:03:23<00:00,  4.00s/it]
How many of the 70,000 units go to El Capitan HPC for the US Government?
About 40,000 units go to El Capitan HPC for the US Government.https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance
------------------------------------------------------------
------------------------------------------------------------
Why is it strategically important for Apple to have control in the context of AI and GenAI services?
It is strategically important for Apple to have control in the context of AI and GenAI services to avoid being just the "dumb hardware" that facilitates AI usage. By having more control, Apple can potentially navigate the evolving landscape of AI and ensure that it continues to play a significant role in the consumer internet paradigm. This would allow Apple to have a greater stake in the services and technologies that users increasingly rely on, thereby maintaining its position as an influential player in the industry.https://www.semianalysis.com/p/apples-ai-strategy-apple-datacenters
------------------------------------------------------------
------------------------------------------------------------
Where will Gigawatt and larger training clusters be built over the next few years?
People are wondering where Gigawatt and larger training clusters will be built over the next few years and what the mix of power generation types such as natural gas, solar, and wind will be.https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race
------------------------------------------------------------
------------------------------------------------------------
What is Spectrum-X NVIDIA and where is it being used?
Spectrum-X NVIDIA is an Ethernet technology that has multiple advantages over InfiniBand in large networks. A 100k H100 cluster is currently being deployed that will use Spectrum-X NVIDIA and will be operational by the end of the year.https://www.semianalysis.com/p/100000-h100-clusters-power-network
------------------------------------------------------------
------------------------------------------------------------
What is MI300A and what is its use?
MI300A is the configuration of the AMD MI300 that has received a lot of attention due to its heterogenous CPU+GPU compute. It is the version being used by the El Capitan Exascale supercomputer.https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance
------------------------------------------------------------
------------------------------------------------------------
What is the main challenge with HBM technology?
The main challenge with HBM is packaging and stacking the memory. SK Hynix has excelled at this, as they have accumulated the strongest process flow knowledge in this area.https://www.semianalysis.com/p/ai-capacity-constraints-cowos-and
------------------------------------------------------------
------------------------------------------------------------
Does reduced complexity of high-NA EUV result in lower costs?
The implication from ASML is that reduced complexity results in lower costs. However, according to lithography models, high-NA EUV single patterning costs significantly more than double-patterning using existing low-NA machines for upcoming technology nodes including  1.4nm/14A.https://www.semianalysis.com/p/asml-dilemma-high-na-euv-is-worse
------------------------------------------------------------
------------------------------------------------------------
What is the MI300 and how does it compete with other hardware?
The MI300 is an upcoming product from AMD that is poised to compete with Nvidia and Google hardware in LLM (Large Language Models) inference. The MI300's primary customers are firms like Databricks, AI21, Lamini, Moreph, and Korea Telecom (KT) that use AMD GPUs for inference/training. To enable this, AMD has been investing heavily into their own RoCM software, the PyTorch ecosystem, and OpenAI’s Triton.https://www.semianalysis.com/p/amd-mi300-ramp-gpt-4-performance
------------------------------------------------------------
------------------------------------------------------------
What is the average productivity for solar power in Europe?
Most of Europe, other than Spain, is of average productivity for solar power, with an average PVOUT of 1,201 kWh/kWp/year. This is due to its high latitude, with the South of France at the same latitude as Chicago.https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race
------------------------------------------------------------
------------------------------------------------------------
What is the revenue projection for the whole year for MI300?
If none of our customers place big orders, we expect MI300 revenue for the entire year to be approximately $100 million, which is quite low given the massive cache of 125,600 HBM modules we have produced.https://www.semianalysis.com/p/amd-mi300-ramp-gpt-4-performance
------------------------------------------------------------
------------------------------------------------------------
Why are chipmakers buying high-NA scanners from ASML?
Chipmakers are buying high-NA scanners from ASML because they see a need for them in future processes. High-NA scanners were assumed to have a cost advantage over low-NA double patterning in 2020, but from 2021 onwards, the metric of choice changes from cost per wafer to process complexity. Chipmakers running 1000+ step wafer fabrication processes are used to complexity. They plan fabs and purchase equipment based on cost and projected yield, of which low-NA seems to perform better on. However, at the 1nm and 7A nodes, now in the 2030+ timeframe, the cost gap finally closes. Driving this is the paradigm change from geometry scaling to stacking – rather than shrinking features horizontally, chip performance power and area improvements are achieved by stacking features vertically. High-NA insertion is seen as the natural place for this change from 2d scaling to 3d scaling, and it changes the litho-intensity of advanced logic manufacturing heavily. Therefore, chipmakers have purchased at least 10 orders for high-NA systems for their future production needs.https://www.semianalysis.com/p/asml-dilemma-high-na-euv-is-worse
------------------------------------------------------------
------------------------------------------------------------
What technology is used to connect the logic die and HBM die?
The logic die and HBM die are connected using a silicon-based interposer with TSVs. This interposer is placed on an organic package substrate.https://www.semianalysis.com/p/ai-capacity-constraints-cowos-and
------------------------------------------------------------
------------------------------------------------------------
Why do you think startups like Tenstorrent are not as promising?
While other startups such as Tenstorrent show promise, it is believed that the hardware/software is still a bit away from really hitting its stride.https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance
------------------------------------------------------------
------------------------------------------------------------
What is the focus of the article on Apple?
The article focuses on Apple's current and future chips and how they can be used for AI. It also discusses Apple's grudge with Nvidia and how they can offer AI services to customers and grow revenue.https://www.semianalysis.com/p/apples-ai-strategy-apple-datacenters
------------------------------------------------------------
------------------------------------------------------------
What technology is used for the lanes included in the Elk Range chiplet?
The lanes included in the Elk Range chiplet use xGMI/PCIe/CXL technology.https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance
------------------------------------------------------------
------------------------------------------------------------
What is X.AI doing to address the power issue in GPU clusters?
X.AI is converting an old factory in Memphis, Tennessee into a datacenter to address the power issue in GPU clusters. This is being done due to the lack of other options available.https://www.semianalysis.com/p/100000-h100-clusters-power-network
------------------------------------------------------------
------------------------------------------------------------
What is the upcoming battle between AMD's MI300X and Nvidia's H100 in 2024?
In 2024, there is a brewing battle between AMD's MI300X and Nvidia's H 100. These are high-performance hardware offerings from both companies that will compete for dominance in the market. It will be interesting to see how these two products fare against each other and what innovations they bring to the table.https://www.semianalysis.com/p/amd-ai-software-solved-mi300x-pricing
------------------------------------------------------------
------------------------------------------------------------
What is Singapore's plan regarding solar power capacity and how could it benefit datacenter capacity?
Singapore plans to import 2 GW of solar power from Indonesia's Riau Islands to complement its existing  12.7 GW of total generation capacity in  2022. This plan involves five projects with 11 Gigawatt peak (GWp) of solar power capacity, and will incorporate 21 GWh of battery energy storage solutions to provide the 2 GW on a round-the-clock basis. This could reduce Singapore's heavy reliance on fossil fuels and create opportunities for greater datacenter capacity that could take advantage of this 24/7 renewable energy supply.https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race
------------------------------------------------------------
------------------------------------------------------------
What is the AI Datacenter Energy Dilemma?
The boom in demand for AI clusters has led to a surge in focus on datacenter capacity, with extreme stress on electricity grids, generation capacity, and the environment. The AI buildouts are heavily limited by the lack of datacenter capacity, especially with regard to training as GPUs need to be generally co-located for high-speed chip to chip networking.https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race
------------------------------------------------------------
------------------------------------------------------------
What is the impact of PUE on power consumption and cost?
As an example, at 80% utilization rate and a PUE of  1.25, a theoretical datacenter with a cluster of 20,480 GPUs would on average draw 28-29MW of power from the grid, adding up to 249,185 Megawatt-hours per year, which would cost $ 20.7M USD per year in electricity based on average US power tariffs of $ 0.083 per kilowatt-hour.https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race
------------------------------------------------------------
------------------------------------------------------------
What are the challenges associated with front-end networking requirements in multimodal image and video training data?
The front-end networking requirements for multimodal image and video training data pose a significant challenge. In this scenario, the front-end networking bandwidth needs to handle the task of loading large video files while also performing all reductions. This creates a dilemma as the front-end networking bandwidth becomes overloaded and struggles to manage these tasks efficiently. The straggler problem is also amplified in situations where there is irregular storage network traffic. This irregular traffic can cause all-reduces to slow down, making it difficult to predict and model the performance accurately.https://www.semianalysis.com/p/100000-h100-clusters-power-network
------------------------------------------------------------
------------------------------------------------------------
How did Nvidia's own Megatron-LM stack perform on a 175B parameter model?
Nvidia's own Megatron-LM stack only achieved  52.8% HFU and  51.4% MFU on a 175B parameter model.https://www.semianalysis.com/p/amd-ai-software-solved-mi300x-pricing
------------------------------------------------------------
------------------------------------------------------------
How does the cost of the 4 tier InfiniBand network compare to the other options?
The cost of the 4 tier InfiniBand network is  1.3- 1.6 times more expensive compared to the other options.https://www.semianalysis.com/p/100000-h100-clusters-power-network
------------------------------------------------------------
------------------------------------------------------------
How are Chinese companies contributing to the GPU shortage?
Chinese companies have been actively investing in the deployment of their own Language Model Models (LLMs) and are stockpiling GPUs in anticipation of potential further export controls from the US. For instance, Bytedance, the company behind TikTok, has reportedly ordered over $1 billion worth of A800/H800s from Nvidia. This strategic move by Chinese companies has contributed to the strain on available GPUs, further exacerbating the shortage and impacting organizations like OpenAI seeking access to these resources.https://www.semianalysis.com/p/ai-capacity-constraints-cowos-and
------------------------------------------------------------
------------------------------------------------------------
Why are these units sold at incredibly low gross margins?
These units are sold at incredibly low gross margins due to the nature of the multi-year partnership and the assistance provided in design, software, and early engagement.https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance
------------------------------------------------------------
------------------------------------------------------------
What are compute tiles and what can they be?
Compute tiles are modular units that can be either CPU or GPU.https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance
------------------------------------------------------------
------------------------------------------------------------
How can Google and Apple monetize genAI based search with ads?
To monetize genAI based search with ads effectively, Google, Apple, or their partners can resort to serving ads. While the cost of serving the model might be higher, there is a higher conversion rate for genAI ads. This means that genAI based search with ads supporting it can be a viable business model. However, Apple's restrictions on serving ads in their personal assistant, along with data privacy concerns, pose a challenge. This limits the potential revenue stream for providers to offset the free service. In this situation, Apple faces a dilemma as the service needs to be paid for the provider to make money, but Apple also takes a cut.https://www.semianalysis.com/p/apples-ai-strategy-apple-datacenters
------------------------------------------------------------
------------------------------------------------------------
Why was the project pushed out to 2024?
The project was pushed out to 2024 due to some delays in late  2022.https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance
------------------------------------------------------------
------------------------------------------------------------
What is the aspirational goal for AMD next year?
The aspirational goal for AMD is to have 100,000 GPU a quarter next year.https://www.semianalysis.com/p/amd-mi300-taming-the-hype-ai-performance
------------------------------------------------------------
------------------------------------------------------------
What is the current state of Nvidia's dominance in software for machine learning?
Nvidia's dominant position in software for machine learning has been weakening rapidly, primarily due to the emergence of Meta's PyTorch  2.0 and OpenAI's Triton. Additionally, MosaicML has also been making significant progress in this field since last year.https://www.semianalysis.com/p/amd-ai-software-solved-mi300x-pricing
